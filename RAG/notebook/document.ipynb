{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7403dc",
   "metadata": {},
   "source": [
    "## doc ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92803ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56e0ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.pdf', 'page': 1, 'other_info': 'additional metadata'}, page_content='this is main text conetent used for RAG')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is main text conetent used for RAG\", \n",
    "    metadata={\n",
    "        \"source\": \"example.pdf\", \n",
    "        \"page\": 1, \n",
    "        \"other_info\": \"additional metadata\"})\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1fa01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaac9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b95419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Introduction to Python\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability. It emphasizes clear syntax, which makes it beginner-friendly yet powerful for advanced programming. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\\n\\nKey Features\\n\\nEasy to Read and Write: Simple syntax similar to English.\\n\\nInterpreted: Executes code line by line, making debugging easier.\\n\\nCross-Platform: Runs on Windows, macOS, Linux, and more.\\n\\nExtensive Libraries: Offers libraries like NumPy, Pandas, Matplotlib, TensorFlow, and Flask.\\n\\nOpen Source: Free to use with a large developer community.\\n\\nCommon Uses\\n\\nWeb Development: Using frameworks like Django and Flask.\\n\\nData Science: For data analysis, visualization, and machine learning.\\n\\nAutomation: Writing scripts to automate repetitive tasks.\\n\\nSoftware Development: Building desktop apps, games, and APIs.\\n\\nArtificial Intelligence: Implementing AI and ML models efficiently.\\n\\na = 5\\nb = 3\\nsum = a + b\\nprint(\"Sum:\", sum)')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fee3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Introduction to Machine Learning\\n\\nMachine learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn from data and improve performance on a task without being explicitly programmed. Instead of following hard-coded rules, machine learning algorithms detect patterns in data and make predictions or decisions based on them.\\n\\nTypes of Machine Learning\\n\\nSupervised Learning\\nIn supervised learning, the algorithm is trained on labeled data, meaning the input comes with the correct output. The goal is for the model to learn the mapping between inputs and outputs to predict outcomes for new data.\\n\\nExamples: Predicting house prices, classifying emails as spam or not, forecasting sales.\\n\\nCommon Algorithms: Linear regression, decision trees, support vector machines, neural networks.\\n\\nUnsupervised Learning\\nUnsupervised learning deals with unlabeled data. The algorithm identifies patterns, clusters, or structures within the data.\\n\\nExamples: Customer segmentation, anomaly detection, market basket analysis.\\n\\nCommon Algorithms: K-means clustering, hierarchical clustering, principal component analysis (PCA).\\n\\nReinforcement Learning\\nReinforcement learning is about learning through trial and error. An agent interacts with an environment, receives feedback in the form of rewards or penalties, and learns to maximize cumulative reward.\\n\\nExamples: Game-playing AI, robotics, autonomous vehicles.\\n\\nCommon Algorithms: Q-learning, deep Q-networks, policy gradients.\\n\\nSemi-Supervised Learning\\nSemi-supervised learning combines labeled and unlabeled data. It is useful when labeling data is expensive or time-consuming.\\n\\nExamples: Speech recognition, image classification.\\n\\nKey Components of Machine Learning\\n\\nData: Raw information used for training models. Quality and quantity of data significantly impact performance.\\n\\nFeatures: Measurable properties or attributes of the data that the model uses to make predictions.\\n\\nModel: The mathematical representation or algorithm that learns patterns from data.\\n\\nTraining: The process of teaching the model using historical data.\\n\\nEvaluation: Assessing the model’s performance using metrics like accuracy, precision, recall, or F1-score.\\n\\nPrediction: Using the trained model to make decisions on new, unseen data.\\n\\nApplications of Machine Learning\\n\\nHealthcare: Disease prediction, medical imaging analysis, drug discovery.\\n\\nFinance: Fraud detection, algorithmic trading, risk assessment.\\n\\nRetail: Recommendation systems, demand forecasting, inventory optimization.\\n\\nTransportation: Autonomous vehicles, route optimization, predictive maintenance.\\n\\nNatural Language Processing: Language translation, sentiment analysis, chatbots.\\n\\nChallenges in Machine Learning\\n\\nData Quality: Poor or biased data leads to inaccurate models.\\n\\nOverfitting: The model performs well on training data but poorly on new data.\\n\\nInterpretability: Complex models, like deep neural networks, are often hard to interpret.\\n\\nScalability: Handling massive datasets efficiently can be challenging.\\n\\nEthics and Bias: Ensuring fairness and avoiding discriminatory outcomes.\\n\\nMachine learning continues to transform industries by automating tasks, enhancing decision-making, and uncovering insights from large amounts of data. As models and algorithms advance, the scope of ML applications grows rapidly.'), Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Introduction to Python\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability. It emphasizes clear syntax, which makes it beginner-friendly yet powerful for advanced programming. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\\n\\nKey Features\\n\\nEasy to Read and Write: Simple syntax similar to English.\\n\\nInterpreted: Executes code line by line, making debugging easier.\\n\\nCross-Platform: Runs on Windows, macOS, Linux, and more.\\n\\nExtensive Libraries: Offers libraries like NumPy, Pandas, Matplotlib, TensorFlow, and Flask.\\n\\nOpen Source: Free to use with a large developer community.\\n\\nCommon Uses\\n\\nWeb Development: Using frameworks like Django and Flask.\\n\\nData Science: For data analysis, visualization, and machine learning.\\n\\nAutomation: Writing scripts to automate repetitive tasks.\\n\\nSoftware Development: Building desktop apps, games, and APIs.\\n\\nArtificial Intelligence: Implementing AI and ML models efficiently.\\n\\na = 5\\nb = 3\\nsum = a + b\\nprint(\"Sum:\", sum)')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "\n",
    "text_documents=dir_loader.load()\n",
    "print(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a6639",
   "metadata": {},
   "source": [
    "## load all pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c16bb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 0, 'page_label': '1'}, page_content='DM I: Block ’Classification’\\nUnit ’Decision Trees’\\nMyra Spiliopoulou'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 1, 'page_label': '2'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nMaterials\\n▶ Algorithms and equations: Chapter 3 of the course book\\n▶ Pictures: Chapter 4 of the 1st edition, but with pointers to the course\\nbook\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 2/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 2, 'page_label': '3'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 3/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 3, 'page_label': '4'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms\\nDT for verterbrate classification Tan et al., Ch.4 (2006) a\\naIn the book of the course, this is Figure 3.4\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 4/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 4, 'page_label': '5'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms\\nUsing the DT for verterbrate classification Tan et al., Ch.4 (2006) a\\naIn the book of the course, this is Figure 3.5\\nto classify a flamingo:\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 5/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 5, 'page_label': '6'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms\\nLearning decision trees:\\n▶ A very old, simple tree induction algorithm: Hunt’s algorithm\\n▶ Split criteria for decision tree learners\\n▶ Learning bushy classifiers\\n▶ Learning binary classifiers\\non the example of the patient responses’ dataset:\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes better no r no yes\\n#2 m M no better no b yes no\\n#3 m M no worse no b no no\\n#4 f VH yes worse no b no yes\\n#5 m L no no effect no l no no\\n#6 m M no better no l no no\\n#7 f VH yes better yes l yes yes\\n#8 f H no better yes r no no\\n#9 f H yes better no l no yes\\n#10 m M yes worse no b no no\\n#11 m M no no effect no l no no\\n#12 f H no no effect no r no no\\n#13 f L yes better no l yes yes\\n#14 m M no worse no b no no\\n#15 m L no no effect no l yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 6/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 6, 'page_label': '7'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Hunt’s algorithm\\nHunt’s algorithm based on [Ch 3, Section 3.3.1]\\nINPUT: Training setD, Labelset L = {y1, . . . ,yk}\\nAt the current node v, invoke\\nhunt(v,L)\\nIF exists y ∈ L such that ∀x ∈ v : label(x) = y\\nTHEN do\\n1. set y as the label of the whole v\\n2. return\\nELSE do\\n1. compute children(v) by invoking split(v,L)\\n2. for each u ∈ children(v)\\nhunt(u,L)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 7/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 7, 'page_label': '8'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 8/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 8, 'page_label': '9'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nWhich split is better ?\\nTypical objectives for the split function\\n▶ Minimize the impurity with respect to the target variable\\n▶ Minimize the misclassification rate\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 9/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 9, 'page_label': '9'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nWhich split is better ?\\nTypical objectives for the split function\\n▶ Minimize the impurity with respect to the target variable\\n▶ Minimize the misclassification rate\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 9/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 10, 'page_label': '9'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nWhich split is better ?\\nTypical objectives for the split function\\n▶ Minimize the impurity with respect to the target variable\\n▶ Minimize the misclassification rate\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 9/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 11, 'page_label': '10'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels. 1\\nExample split functions\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\n1cf. Equations 3.4, 3.5, 3.6 – notice the differences in notation\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 10/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 12, 'page_label': '10'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels. 1\\nExample split functions\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\n1cf. Equations 3.4, 3.5, 3.6 – notice the differences in notation\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 10/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 13, 'page_label': '10'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels. 1\\nExample split functions\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\n1cf. Equations 3.4, 3.5, 3.6 – notice the differences in notation\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 10/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 14, 'page_label': '11'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels.\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(t) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\nHow do these functions differ in their behaviour?\\n▶ Compute the values of the split functions for these nodes a:\\nv1 : 0 members with label Y , 6 members with label N\\nv2 : 1 member with label Y , 5 members with label N\\nv3 : 3 members with label Y , 3 members with label N\\n▶ Compute the values of the split functions for the 7 attributes in the\\n’patients responses’ dataset\\naExamples from Tan et al., Ch.4 (2006)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 11/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 15, 'page_label': '11'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels.\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(t) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\nHow do these functions differ in their behaviour?\\n▶ Compute the values of the split functions for these nodes a:\\nv1 : 0 members with label Y , 6 members with label N\\nv2 : 1 member with label Y , 5 members with label N\\nv3 : 3 members with label Y , 3 members with label N\\n▶ Compute the values of the split functions for the 7 attributes in the\\n’patients responses’ dataset\\naExamples from Tan et al., Ch.4 (2006)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 11/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 16, 'page_label': '12'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nBehaviour of the split functions for binary classification a\\naIn the book of the course, this is Figure 3.11\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 12/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 17, 'page_label': '13'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 13/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 18, 'page_label': '14'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nMore on splits 1/2\\nRECALL example:\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nTwo ways of splitting an attribute that takes<< 2 values\\n⋆ multi-split: one child node per attribute value\\n⋆ binary split: pick one value zz and split into two child nodes, one for zz\\nand one for ’notzz’\\nTwo types of classifiers – bushy & binary\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 14/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 19, 'page_label': '14'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nMore on splits 1/2\\nRECALL example:\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nTwo ways of splitting an attribute that takes<< 2 values\\n⋆ multi-split: one child node per attribute value\\n⋆ binary split: pick one value zz and split into two child nodes, one for zz\\nand one for ’notzz’\\nTwo types of classifiers – bushy & binary\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 14/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 20, 'page_label': '15'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nMore on splits 2/2\\nHow to split a node v on a continuous attribute a?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 15/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 21, 'page_label': '16'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Dealing with non-categorical attributes\\nOrder preserving n-split of a node v on an attribute a that takes continuous\\nvalues:\\n▶ Greedy way:\\nIteratively consider candidate positions within the valuerange of a, e.g.\\n· by sampling n values randomly for some n (input parameter)\\n▶ Discretization:\\n· in an unsupervised way, e.g. by\\nbuilding n homogeneous and well-separated clusters, or\\nby building a histogramm of equisized bins\\n· in a supervised way, e.g. by partitioning the data so as to minimize the\\nimpurity measure\\nacquiring n groups of instances 2.\\nThis results in a candidate split of of node v to n children, i.e. to a set\\nchildren(v,a), for which we can then compute the gain on inpurity.\\n2The number of groups n may be an input parameter or may be derived.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 16/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 22, 'page_label': '17'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 17/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 23, 'page_label': '18'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithm - Quinlan’s ID3 (simplified)\\nLearning a bushy classifier with ID3\\nINPUT: Training setD, Labelset L = {y1, . . . ,yk}, set of attributes A\\nAt the current node v, invoke\\nID3(v,L,A)\\nIF ∃y ∈ L such that for most of the x ∈ v : label(x) = y THEN do\\nset y as the label of the whole v and return\\nELSE IF A = / 0THEN do\\n1. identify the majority class label of v, y\\n2. set y as the label of the whole v and return\\nELSE do\\n1. set abest as the arg maxa∈A{InfGain(v,L,a)}\\n2. compute children(v,abest) by splitting v on the values of abest\\n3. for each u ∈ children(v,abest), invoke ID3(u,L,A \\\\ {abest})\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 18/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 24, 'page_label': '19'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Entropy and Information Gain\\nGain on impurity\\nFor a node v split on the values of attribute a ∈ A into children(v,a), the gain\\nof this split is:\\n∆(v,a) = I(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| I(u)\\nwhere I(·) is an impurity measure 3.\\nIf we set\\nI(v) := entropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nthen ∆(v,a) ≡ InfGain(v,a). 4\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\n3See description of gain around Equations 3.7, 3.8 (different notation)\\n4NOTE: L is fixed, so InfGain(v,a) ≡ InfGain(v,L,a).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 19/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 25, 'page_label': '19'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Entropy and Information Gain\\nGain on impurity\\nFor a node v split on the values of attribute a ∈ A into children(v,a), the gain\\nof this split is:\\n∆(v,a) = I(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| I(u)\\nwhere I(·) is an impurity measure 3. If we set\\nI(v) := entropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nthen ∆(v,a) ≡ InfGain(v,a). 4\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\n3See description of gain around Equations 3.7, 3.8 (different notation)\\n4NOTE: L is fixed, so InfGain(v,a) ≡ InfGain(v,L,a).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 19/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 26, 'page_label': '20'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Information Gain and Gain Ratio\\nFor a node v split on the values of attribute a ∈ A into children(v,a) . . .\\nInformation Gain\\nInfGain(v,a) = entropy(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| entropy(u)\\nIntrinsic Information from Piatesky-Shapiro\\nIntrinsicInformation(v,a) = − ∑\\nu∈children(v,a)\\n|u|\\n|v| log |u|\\n|v|\\nGain Ratio from (Quinlan, 1986)\\nThe gain ratio achieved when splitting v on a is\\nGainRatio(v,a) = InfGain(v,a)\\nIntrinsicInformation(v,a)\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 20/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 27, 'page_label': '20'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Information Gain and Gain Ratio\\nFor a node v split on the values of attribute a ∈ A into children(v,a) . . .\\nInformation Gain\\nInfGain(v,a) = entropy(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| entropy(u)\\nIntrinsic Information from Piatesky-Shapiro\\nIntrinsicInformation(v,a) = − ∑\\nu∈children(v,a)\\n|u|\\n|v| log |u|\\n|v|\\nGain Ratio from (Quinlan, 1986)\\nThe gain ratio achieved when splitting v on a is\\nGainRatio(v,a) = InfGain(v,a)\\nIntrinsicInformation(v,a)\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 20/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 28, 'page_label': '20'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Information Gain and Gain Ratio\\nFor a node v split on the values of attribute a ∈ A into children(v,a) . . .\\nInformation Gain\\nInfGain(v,a) = entropy(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| entropy(u)\\nIntrinsic Information from Piatesky-Shapiro\\nIntrinsicInformation(v,a) = − ∑\\nu∈children(v,a)\\n|u|\\n|v| log |u|\\n|v|\\nGain Ratio from (Quinlan, 1986)\\nThe gain ratio achieved when splitting v on a is\\nGainRatio(v,a) = InfGain(v,a)\\nIntrinsicInformation(v,a)\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 20/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 29, 'page_label': '20'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Information Gain and Gain Ratio\\nFor a node v split on the values of attribute a ∈ A into children(v,a) . . .\\nInformation Gain\\nInfGain(v,a) = entropy(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| entropy(u)\\nIntrinsic Information from Piatesky-Shapiro\\nIntrinsicInformation(v,a) = − ∑\\nu∈children(v,a)\\n|u|\\n|v| log |u|\\n|v|\\nGain Ratio from (Quinlan, 1986)\\nThe gain ratio achieved when splitting v on a is\\nGainRatio(v,a) = InfGain(v,a)\\nIntrinsicInformation(v,a)\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 20/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 30, 'page_label': '21'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 21/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 31, 'page_label': '22'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Binary Classifiers\\nLearning a binary classifier\\nINPUT: Training setD, Labelset L = {y1, . . . ,yk}, set of (attribute,value)-pairs\\nAV(A) ≡ AV for the set of attributes A\\nAt the current node v, invoke\\nbinCore(v,L,AV)\\nIF ∃y ∈ L such that for most of the x ∈ v : label(x) = y THEN do\\nset y as the label of the whole v and return\\nELSE IF AV = / 0THEN do\\n1. identify the majority class label of v, y\\n2. set y as the label of the whole v and return\\nELSE do\\n1. Choose the best binary split of v into v1,v2\\n2. Remove from AV the pair that caused the best binary split\\n3. For each u ∈ {v1,v2} invoke binCore(u,L,AV)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 22/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 32, 'page_label': '23'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 23/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 33, 'page_label': '24'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nProgress and outlook\\nWe have seen:\\n√ How to build a decision tree by recursively splitting the dataset into more\\nand more homogeneous nodes – where homogeneity refers to the\\ntarget variable\\n√ Split functions that implement different definitions of ’homogeneity’\\n√ Different types of decision tree, depending on whether a node has\\nexactly two children or as many children as the values of the splitting\\nattribute\\nEach type of decision tree and each type of split function lead to a different\\nclassifier:\\n▶ How to figure out how good a classifier is?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 24/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 34, 'page_label': '25'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nThank you very much! Questions?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 25/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1'}, page_content='DM I: Block ’Classification’\\nUnit ’Naive Bayes’\\nMyra Spiliopoulou'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nEXAMPLE: ’patient responses’ dataset\\nSimplified\\nversion:\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes better no r no yes\\n#2 m M no better no b yes no\\n#3 m M no worse no b no no\\n#4 f VH yes worse no b no yes\\n#5 m L no no effect no l no no\\n#6 m M no better no l no no\\n#7 f VH yes better yes l yes yes\\n#8 f H no better yes r no no\\n#9 f H yes better no l no yes\\n#10 m M yes worse no b no no\\n#11 m M no no effect no l no no\\n#12 f H no no effect no r no no\\n#13 f L yes better no l yes yes\\n#14 m M no worse no b no no\\n#15 m L no no effect no l yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 2/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nEXAMPLE: ’patient responses’ dataset\\nand version with\\nboth categorical\\nand numerical\\nattributes:\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes 1 no r no yes\\n#2 m M no 2 no b yes no\\n#3 m M no 6 no b no no\\n#4 f VH yes 7 no b no yes\\n#5 m L no 3 no l no no\\n#6 m M no 1 no l no no\\n#7 f VH yes 1 yes l yes yes\\n#8 f H no 2 yes r no no\\n#9 f H yes 2 no l no yes\\n#10 m M yes 6 no b no no\\n#11 m M no 3 no l no no\\n#12 f H no 4 no r no no\\n#13 f L yes 4 no l yes yes\\n#14 m M no 7 no b no no\\n#15 m L no 5 no l yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 3/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\n1 The Law of Bayes for Model Learning\\n2 The Zero-Frequency Problem\\n3 NB on numerical attributes\\n4 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 4/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes from Witten & Eibe\\nLaw of Bayes\\nThe probability of an event H given evidence E is: p(H|E) = p(E|H)p(H)\\np(E)\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nAppeared in:\\n“Essay towards solving a problem in the doctrine of chances” (1763)\\nby Thomas Bayes (born: 1702, London; died: 1761, Tunbridge Wells, Kent)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 5/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 5, 'page_label': '5'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes from Witten & Eibe\\nLaw of Bayes\\nThe probability of an event H given evidence E is: p(H|E) = p(E|H)p(H)\\np(E)\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nAppeared in:\\n“Essay towards solving a problem in the doctrine of chances” (1763)\\nby Thomas Bayes (born: 1702, London; died: 1761, Tunbridge Wells, Kent)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 5/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 6, 'page_label': '6'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes Witten & Eibe\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nIn classification, an event is the observation of a class H,\\nwhile E is a record, composed of attributes values E1, . . . ,En in an\\nn-dimensional feature space A = {a1, . . . ,an}.\\nThe independence assumption means that:\\n▶ All attributes contribute equally to the class prediction.\\n▶ Within a given class, the value of an attribute does not influence the\\nvalues of other attributes.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 6/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 7, 'page_label': '6'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes Witten & Eibe\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nIn classification, an event is the observation of a class H,\\nwhile E is a record, composed of attributes values E1, . . . ,En in an\\nn-dimensional feature space A = {a1, . . . ,an}.\\nThe independence assumption means that:\\n▶ All attributes contribute equally to the class prediction.\\n▶ Within a given class, the value of an attribute does not influence the\\nvalues of other attributes.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 6/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 8, 'page_label': '6'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes Witten & Eibe\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nIn classification, an event is the observation of a class H,\\nwhile E is a record, composed of attributes values E1, . . . ,En in an\\nn-dimensional feature space A = {a1, . . . ,an}.\\nThe independence assumption means that:\\n▶ All attributes contribute equally to the class prediction.\\n▶ Within a given class, the value of an attribute does not influence the\\nvalues of other attributes.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 6/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 9, 'page_label': '7'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes\\nLearning phase of Naive Bayes\\nFor each (attribute, value)-pair (a, z) for an attribute a ∈ A:\\n▶ For each label y ∈ L: compute p((a, z)|y)\\nApplication phase with Naive Bayes\\nFor an instance x with unknown label:\\n▶ For each label y ∈ L: compute p(y|x) using the computations of the\\nlearning phase.\\n▶ Assign to x the label with the highest probability, i.e.\\nlabel(x) = arg maxy∈L p(y|x).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 7/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 10, 'page_label': '7'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes\\nLearning phase of Naive Bayes\\nFor each (attribute, value)-pair (a, z) for an attribute a ∈ A:\\n▶ For each label y ∈ L: compute p((a, z)|y)\\nApplication phase with Naive Bayes\\nFor an instance x with unknown label:\\n▶ For each label y ∈ L: compute p(y|x) using the computations of the\\nlearning phase.\\n▶ Assign to x the label with the highest probability, i.e.\\nlabel(x) = arg maxy∈L p(y|x).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 7/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 11, 'page_label': '8'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nEXAMPLE for the ’patient responses’ dataset (categorical attributes only)\\nPriors of the target variable: p(yes) = 5/15, p(no) = 10/15.\\n1. Count the occurrences of each class given the value of each attribute:\\nI2: yes no\\nf 5 2\\nm 0 8\\nI15: yes no\\nVH 3 0\\nH 1 2\\nM 0 6\\nL 1 2\\nI30: yes no\\nyes 5 1\\nno 0 9\\nI22: yes no\\nbetter 4 3\\nworse 1 3\\nno effect 0 4\\nI31: yes no\\nyes 1 1\\nno 4 9\\nI9: yes no\\nr 1 2\\nl 3 4\\nb 1 4\\nI26: yes no\\nyes 2 2\\nno 3 8\\n2. Compute the NB model\\n3.Apply the NB model\\nTo which class should we assign x =<#85,f,VH, yes, no effect, no, l, no> ?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 8/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 12, 'page_label': '9'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\n1 The Law of Bayes for Model Learning\\n2 The Zero-Frequency Problem\\n3 NB on numerical attributes\\n4 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 9/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 13, 'page_label': '10'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes – The Zero-Frequency Problem\\nFor a training set D from a population D, a Labelset L = {y1, . . . ,yk},\\nan attribute a ∈ A and a y ∈ L, let\\nDy = {x ∈ D|label(x) = y}, D(a,zi) = {x ∈ D|x.a = zi}\\nIf there is a value zi that a can take and a label y ∈ L so that p((a, zi)|y) = 0,\\nthen\\nfor all x ∈ D with x.a = zi it holds that: p(y|x) = 0.\\nLaplace Estimator\\nInstead of computing p((a, zi)|y) as\\n|D(a,zi)∩Dy|\\n|Dy| , we set:\\np((a, zi)|y) =\\n|D(a,zi) ∩ Dy| + w\\nna\\n|Dy| + w\\nwhere w is some small weight > 0 and na is the number of distinct values\\nthat attribute a can take.\\nThis is a generalization of the original Laplace estimator, where w = na.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 10/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 14, 'page_label': '10'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes – The Zero-Frequency Problem\\nFor a training set D from a population D, a Labelset L = {y1, . . . ,yk},\\nan attribute a ∈ A and a y ∈ L, let\\nDy = {x ∈ D|label(x) = y}, D(a,zi) = {x ∈ D|x.a = zi}\\nIf there is a value zi that a can take and a label y ∈ L so that p((a, zi)|y) = 0,\\nthen\\nfor all x ∈ D with x.a = zi it holds that: p(y|x) = 0.\\nLaplace Estimator\\nInstead of computing p((a, zi)|y) as\\n|D(a,zi)∩Dy|\\n|Dy| , we set:\\np((a, zi)|y) =\\n|D(a,zi) ∩ Dy| + w\\nna\\n|Dy| + w\\nwhere w is some small weight > 0 and na is the number of distinct values\\nthat attribute a can take.\\nThis is a generalization of the original Laplace estimator, where w = na.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 10/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 15, 'page_label': '10'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes – The Zero-Frequency Problem\\nFor a training set D from a population D, a Labelset L = {y1, . . . ,yk},\\nan attribute a ∈ A and a y ∈ L, let\\nDy = {x ∈ D|label(x) = y}, D(a,zi) = {x ∈ D|x.a = zi}\\nIf there is a value zi that a can take and a label y ∈ L so that p((a, zi)|y) = 0,\\nthen\\nfor all x ∈ D with x.a = zi it holds that: p(y|x) = 0.\\nLaplace Estimator\\nInstead of computing p((a, zi)|y) as\\n|D(a,zi)∩Dy|\\n|Dy| , we set:\\np((a, zi)|y) =\\n|D(a,zi) ∩ Dy| + w\\nna\\n|Dy| + w\\nwhere w is some small weight > 0 and na is the number of distinct values\\nthat attribute a can take.\\nThis is a generalization of the original Laplace estimator, where w = na.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 10/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 16, 'page_label': '11'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes – more on missing values\\nLet x ∈ D be an instance of unknown label, so that the value of x for some\\nattribute a is missing.\\nHow do we deduce the most likely class of x?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 11/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 17, 'page_label': '12'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\n1 The Law of Bayes for Model Learning\\n2 The Zero-Frequency Problem\\n3 NB on numerical attributes\\n4 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 12/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 18, 'page_label': '13'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nNaive Bayes for attributes with continuous valueranges\\nFor a training set D from a population D, a Labelset L = {y1, . . . ,yk},\\nlet a ∈ A be an attribute with a continuous valuerange, and let Da ⊆ D be the\\nset of instances from D, that have a non-NULL value for attribute a.\\nUsing densities in the NB calculations – from Witten & Eibe\\nAssumption: The values of a follow a Gaussian distribution a (also: normal\\ndistribution), with density function:\\nf (x) = 1√\\n2πσ a\\ne\\n− (x−µa)2\\n2σ 2a\\nwhere µa is the sample mean: µa = 1\\n|Da| ∑x∈Da x.a\\nσa is the sample standard deviation, and\\nσ 2\\na the sample variance: σ 2\\na = 1\\n|Da|−1 ∑x∈Da(x.a − µa)2\\naNamed after the German mathematician Karl Friedrich Gauss (1777-1855), who laid the\\nfoundations of number theory.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 13/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 19, 'page_label': '14'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nEXAMPLE for the ’patient responses’ dataset (mixed attributes)\\nPriors of the target variable are p(yes) = 5/15, p(no) = 10/15; the\\ncomputations for all attributes except I22 remain the same.\\nI22: yes no\\n1, 2, 4, 7 1, 2, 3, 4, 5, 6, 7\\nµ =?, σ =? µ =?, σ =?\\nTo which class should we assign x =<#86,f,VH, yes, 4, no, l, no> ?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 14/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 20, 'page_label': '15'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\n1 The Law of Bayes for Model Learning\\n2 The Zero-Frequency Problem\\n3 NB on numerical attributes\\n4 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 15/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 21, 'page_label': '16'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nProgress and outlook\\nWe have seen:\\n√ How to train a classification model with help of the Law of Bayes\\n√ How to deal with values that may occur in the test set and did not occur\\nin the training set\\n√ How to extend the algorithm for learning on mixed (categorical and\\nnumerical) attributes\\nNB classifiers perform usually well, despite the naive assumption,\\nunless\\n→ there are many redundant attributes, or\\n→ the valueranges do not follow a Gaussian distribution\\n⇓\\n▶ How to figure out how good a classifier is?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 16/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 22, 'page_label': '17'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nThank you very much! Questions?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 17/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='DM I: Block ’Classification’\\nUnit ’Underpinnings’\\nMyra Spiliopoulou'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='Phases of Classification Example datasets Closing\\n1 Phases of Classification\\n2 Example datasets\\n3 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 2/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different\\nfrom the instances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 3/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 3, 'page_label': '3'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different\\nfrom the instances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 3/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 4, 'page_label': '3'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different\\nfrom the instances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 3/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 5, 'page_label': '3'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different\\nfrom the instances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 3/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 6, 'page_label': '4'}, page_content='Phases of Classification Example datasets Closing\\nClassification – Learning Phase: Model induction &\\ndeduction\\nInducing and deducing a model on data of known class membership\\nbefore applying the model on data of unknown class membership.\\nExample from Tan, Steinbach & Kumar, Ch.4 (2006)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 4/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 7, 'page_label': '5'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nWe split D into a training set to be used for model induction\\nand a test set to be used for model deduction (includes: evaluation).\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different from the\\ninstances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 5/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 8, 'page_label': '5'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nWe split D into a training set to be used for model induction\\nand a test set to be used for model deduction (includes: evaluation).\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different from the\\ninstances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 5/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 9, 'page_label': '5'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nWe split D into a training set to be used for model induction\\nand a test set to be used for model deduction (includes: evaluation).\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different from the\\ninstances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 5/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 10, 'page_label': '5'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nWe split D into a training set to be used for model induction\\nand a test set to be used for model deduction (includes: evaluation).\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different from the\\ninstances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 5/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 11, 'page_label': '6'}, page_content='Phases of Classification Example datasets Closing\\n1 Phases of Classification\\n2 Example datasets\\n3 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 6/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 12, 'page_label': '7'}, page_content='Phases of Classification Example datasets Closing\\nClassification – Example on verterbrate classification\\nOriginal dataset – Tan, Steinbach & Kumar, Ch.4 (2006)\\n▶ Classes: amphibian, bird, fish, mammal, reptile\\n▶ Feature space: Body Temperature, Skin Cover, Gives Birth, Aquatic\\nCreature, Aerial Creature, Has Legs, Hibernates\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 7/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 13, 'page_label': '8'}, page_content='Phases of Classification Example datasets Closing\\nClassification Example Dataset 1 – Verterbrates binary\\nModified dataset, based on Tan, Steinbach & Kumar, Ch.4 (2006): two\\nclasses ’Mammal yes/no’, 6 variables + Id variable\\nId Body\\nTemperature\\n(1)\\nSkin\\nCover\\nGives\\nBirth\\n(2)\\nAquatic\\n(3)\\nAerial\\n(4)\\nLegs\\n(5)\\nHiber-\\nnates\\n(6)\\nMammal\\nhuman warm-blooded hair yes no no two no yes\\npython cold-blooded scales no no no zero yes no\\nsalmon cold-blooded scales no yes no zero no no\\nwhale warm-blooded hair yes yes no zero no yes\\nfrog cold-blooded none no semi no four no no\\nkomodo\\ndragon\\ncold-blooded scales no no no four no no\\nbat warm-blooded hair yes no yes four yes yes\\npigeon warm-blooded feathers no no yes two no no\\ncat warm-blooded fur yes no no four no yes\\nleopard\\nshark\\ncold-blooded scales yes yes no zero no no\\nturtle cold-blooded scales no semi no four no no\\npenguin warm-blooded feathers no semi no two no no\\nporcupine warm-blooded quills yes no no four yes yes\\neel cold-blooded scales no yes no zero no no\\nsalamander cold-blooded none no semi no four yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 8/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 14, 'page_label': '9'}, page_content='Phases of Classification Example datasets Closing\\nClassification – Another verterbrate dataset example\\nVerterbrate classification on\\n▶ Classes: mammal, non-mammal\\n▶ Feature space: Body Temperature, Gives Birth, Four-legged, Hibernates\\nfrom Tan, Steinbach & Kumar, Ch.4 (2006)\\nTraining Set (cf. Table 4.3, with modifications in two instances!)\\nName Body Temperature Gives\\nBirth\\nFour-\\nlegged\\nHiber-\\nnates\\nClass label\\nporcupine warm-blooded yes yes yes mammal\\ncat warm-blooded yes yes no mammal\\nbat warm-blooded yes no yes mammal\\nwhale warm-blooded yes no no mammal\\nsalamander cold-blooded no yes yes non-mammal\\nkomodo dragon cold-blooded no yes no non-mammal\\npython cold-blooded no no yes non-mammal\\nsalmon cold-blooded no no no non-mammal\\neagle warm-blooded no no no non-mammal\\nguppy cold-blooded yes no no non-mammal\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 9/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 15, 'page_label': '10'}, page_content='Phases of Classification Example datasets Closing\\nClassification – Another verterbrate dataset example\\nVerterbrate classification on\\n▶ Classes: mammal, non-mammal\\n▶ Feature space: Body Temperature, Gives Birth, Four-legged, Hibernates\\nfrom Tan, Steinbach & Kumar, Ch.4 (2006)\\nTest Set (cf. Table 4.4 – one instance modified!)\\nName Body temperature Gives\\nBirth\\nfour\\nlegged\\nHiber-\\nnates\\nClass label\\nhuman warm-blooded yes no no mammal\\npigeon warm-blooded no no no non-mammal\\nelephant warm-blooded yes no no mammal\\nleopard shark cold-blooded yes no no non-mammal\\nturtle cold-blooded no no no non-mammal\\npenguin warm-blooded no no no non-mammal\\neel cold-blooded no no no non-mammal\\ndolphin warm-blooded yes no no mammal\\nspiny anteater warm-blooded no yes yes mammal\\ngila monster cold-blooded no yes yes non-mammal\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 10/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 16, 'page_label': '11'}, page_content='Phases of Classification Example datasets Closing\\nClassification Example Dataset 2 – Weather for playing golf\\nPublic domain dataset ”Golf” – Witten & Eibe, Book on Mining with Java\\n▶ Classes: Y es, No\\n▶ Feature space: Outlook, Temperature 1, Humidity, Windy\\nGolf dataset as running example\\nOutlook Temperature Humidity Windy Play\\nSunny Hot High False No\\nSunny Hot High True No\\nOvercast Hot High False Y es\\nRainy Mild High False Y es\\nRainy Cool Normal False Y es\\nRainy Cool Normal True No\\nOvercast Cool Normal True Y es\\nSunny Mild High False No\\nSunny Cool Normal False Y es\\nRainy Mild Normal False Y es\\nSunny Mild Normal True Y es\\nOvercast Mild High True Y es\\nOvercast Hot Normal False Y es\\nRainy Mild High True No\\n1also: ”Temp”\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 11/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 17, 'page_label': '12'}, page_content='Phases of Classification Example datasets Closing\\n. . . and the fictive dataset on patient responses to treatment\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes better no r no yes\\n#2 m M no better no b yes no\\n#3 m M no worse no b no no\\n#4 f VH yes worse no b no yes\\n#5 m L no no effect no l no no\\n#6 m M no better no l no no\\n#7 f VH yes better yes l yes yes\\n#8 f H no better yes r no no\\n#9 f H yes better no l no yes\\n#10 m M yes worse no b no no\\n#11 m M no no effect no l no no\\n#12 f H no no effect no r no no\\n#13 f L yes better no l yes yes\\n#14 m M no worse no b no no\\n#15 m L no no effect no l yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 12/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 18, 'page_label': '13'}, page_content='Phases of Classification Example datasets Closing\\n1 Phases of Classification\\n2 Example datasets\\n3 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 13/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 19, 'page_label': '14'}, page_content='Phases of Classification Example datasets Closing\\nPhases, Data and . . .\\n▶ How to build a classifier? → Classification algorithms\\n▶ How to assess the quality of a classifier? → Model evaluation\\n▶ After building many classifiers:\\nHow to figure out which one is best? → Model comparison\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 14/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 20, 'page_label': '15'}, page_content='Phases of Classification Example datasets Closing\\nThank you very much! Questions?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 15/15'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 0, 'page_label': '1'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nData Mining Block “Clustering”\\nMyra Spiliopoulou 1\\n1 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 1, 'page_label': '2'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\n1 Introduction\\n2 K-Means family\\n3 Similarity functions\\n4 Hierarchical clustering\\n5 Density-based clustering\\n6 Evaluation in Clustering\\n2 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 2, 'page_label': '3'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering\\nis a family of mining algorithms,\\ndesigned to help you organize your data into groups of similar objects\\n3 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 3, 'page_label': '4'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering\\nis a family of mining algorithms,\\ndesigned to help you organize your data into groups of similar objects\\nA clustering algorithm groups data points in such a way that the objects\\ninside each cluster are more similar to each other than the objects outside\\nthe cluster.\\n3 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 4, 'page_label': '5'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering\\nis a family of mining algorithms,\\ndesigned to help you organize your data into groups of similar objects\\nA clustering algorithm groups data points in such a way that the objects\\ninside each cluster are more similar to each other than the objects outside\\nthe cluster.\\nFor some clustering algorithms, this corresponds to\\n� minimizing the intra-cluster distance\\n� maximizing the inter-cluster distance\\n3 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 5, 'page_label': '6'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDifferent types of clusters T an, Steinbach & Kumar, Ch.8 (2006)\\n4 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 6, 'page_label': '7'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nGoals of the Block\\n� Make yourself familiar with instruments that you can use to cluster\\ndata\\n� Learn how to choose the right instrument for your data\\n� Learn to recognize bad clusters\\n5 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 7, 'page_label': '8'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n� K-Means\\n� Hierarchical clustering\\n� Density-based clustering: DBSCAN\\n6 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 8, 'page_label': '9'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means\\nK-means is good for a quick-and-dirty attempt toﬁnd groups in the data.\\n7 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 9, 'page_label': '10'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means\\nINPUT: a set of data points D in feature space F; the number of clusters K\\nK-Means algorithm\\n1. Select K initial centroids\\n2. Repeat\\n· Assign each point to the centroid closest to it\\n· Recompute the positions of the K centroids\\nuntil the positions of the centroids do not change anymore.\\n8 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 10, 'page_label': '11'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means\\nINPUT: a set of data points D in feature space F; the number of clusters K\\nK-Means algorithm\\n1. Select K initial centroids\\n2. Repeat\\n· Assign each point to the centroid closest to it\\n· Recompute the positions of the K centroids\\nuntil the positions of the centroids do not change anymore.\\nOptimization criterion for K-Means\\nSSE=\\nK\\n∑\\ni=1\\n∑\\nx∈Ci\\ndist(x,c i)2\\nwhere ci is the centroid of cluster Ci with cij =\\n∑x∈Ci xj\\n|Ci | ,j=1. . .|F|\\n8 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 11, 'page_label': '12'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means\\nINPUT: a set of data points D in feature space F; the number of clusters K\\nK-Means algorithm\\n1. Select K initial centroids\\n2. Repeat\\n· Assign each point to the centroid closest to it\\n· Recompute the positions of the K centroids\\nuntil the positions of the centroids do not change anymore.\\nOptimization criterion for K-Means\\nSSE=\\nK\\n∑\\ni=1\\n∑\\nx∈Ci\\ndist(x,c i)2\\nwhere ci is the centroid of cluster Ci with cij =\\n∑x∈Ci xj\\n|Ci | ,j=1. . .|F|\\nComplexity of K-Means\\nO(K· |F| · |D| ·I) for feature space F , dataset D and number of iterations I\\n8 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 12, 'page_label': '13'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means - Example\\nBuilding K=3 clusters T an, Steinbach & Kumar, Ch.8 (2006)\\n9 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 13, 'page_label': '14'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nExample: same data, different initialization of centroids\\nBuilding K=3 clusters T an, Steinbach & Kumar, Ch.8 (2006)\\n10 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 14, 'page_label': '15'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe K in K-Means\\nHow to determine K ?\\nHow to choose ”good” initial positions for the K centroids?\\n11 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 15, 'page_label': '16'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDisadvantages of K-Means\\n� K-Means favours spherical clusters\\n� K-Means favours clusters of the same size\\n� K-Means favours clusters of the same density\\n� K-Means is very sensitive to outlier objects\\n12 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 16, 'page_label': '17'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means variant: Bisecting K-Means\\nBisecting K-Means algorithm\\nStarting with a single root cluster that contains all objects in dataset D\\nREPEA T\\n1. Apply 2-Means on the cluster\\n2. Choose the most inhomogenenous of the leaf clusters\\n3. GOTO 1\\nUNTIL K leaf clusters have been built.\\n13 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 17, 'page_label': '18'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means variant: Bisecting K-Means\\nBisecting K-Means algorithm\\nStarting with a single root cluster that contains all objects in dataset D\\nREPEA T\\n1. Apply 2-Means on the cluster\\n2. Choose the most inhomogenenous of the leaf clusters\\n3. GOTO 1\\nUNTIL K leaf clusters have been built.\\nWhat advantage(s) brings Bisecting K-Means over K-Means?\\n13 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 18, 'page_label': '19'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n� Hierarchical clustering\\n� Density-based clustering: DBSCAN\\n14 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 19, 'page_label': '20'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n� Hierarchical clustering\\n� Density-based clustering: DBSCAN\\n� A small excursion on similarity functions and matrices\\n14 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 20, 'page_label': '21'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity functions\\nProperties of a similarity function s()\\nFor any two data points x,y∈D , it holds that:\\n� s(x,y)≤1 and s(x,y) =1↔x=y\\n� s(x,y) =s(y,x)\\n15 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 21, 'page_label': '22'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity functions\\nProperties of a similarity function s()\\nFor any two data points x,y∈D , it holds that:\\n� s(x,y)≤1 and s(x,y) =1↔x=y\\n� s(x,y) =s(y,x)\\nProperties of a distance function d()\\nFor any two data points x,y∈D , it holds that:\\n� d(x,y)≥0 and d(x,y) =0↔x=y\\n� d(x,y) =d(y,x)\\n� For each z∈D:d(x,z)≤d(x,y) +d(y,z)\\n15 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 22, 'page_label': '23'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity functions\\nProperties of a similarity function s()\\nFor any two data points x,y∈D , it holds that:\\n� s(x,y)≤1 and s(x,y) =1↔x=y\\n� s(x,y) =s(y,x)\\nProperties of a distance function d()\\nFor any two data points x,y∈D , it holds that:\\n� d(x,y)≥0 and d(x,y) =0↔x=y\\n� d(x,y) =d(y,x)\\n� For each z∈D:d(x,z)≤d(x,y) +d(y,z)\\nVery often, we use the complement of a distance function as a similarity\\nfunction.\\n15 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 23, 'page_label': '24'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity and Distance functions\\nGiven are two data points x,y∈D over an n-dimensional feature space F.\\n� f1(x,y) =\\n�\\n∑n\\ni=1(xi −y i)2\\n� f2(x,y) = ∑n\\ni=1 |xi −y i|\\n� f3(x,y) = ∑n\\ni=1 xiyi√\\n∑n\\ni=1 x2\\ni\\n√\\n∑n\\ni=1 y2\\ni\\n16 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 24, 'page_label': '25'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 25, 'page_label': '26'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 26, 'page_label': '27'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n� disagree10(x,y) = ∑n\\ni=1 xi(1−y i)\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 27, 'page_label': '28'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n� disagree10(x,y) = ∑n\\ni=1 xi(1−y i)\\n� disagree01(x,y) = ∑n\\ni=1(1−x i)yi\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 28, 'page_label': '29'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n� disagree10(x,y) = ∑n\\ni=1 xi(1−y i)\\n� disagree01(x,y) = ∑n\\ni=1(1−x i)yi\\nRandIndex(x,y) = agree11(x,y) +agree 00(x,y)\\nagree11(x,y) +agree 00(x,y) +disagree 10(x,y) +disagree 01(x,y)\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 29, 'page_label': '30'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n� disagree10(x,y) = ∑n\\ni=1 xi(1−y i)\\n� disagree01(x,y) = ∑n\\ni=1(1−x i)yi\\nRandIndex(x,y) = agree11(x,y) +agree 00(x,y)\\nagree11(x,y) +agree 00(x,y) +disagree 10(x,y) +disagree 01(x,y)\\nJaccardCoefﬁcient(x,y) = agree11(x,y)\\nagree11(x,y) +disagree 10(x,y) +disagree 01(x,y)\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 30, 'page_label': '31'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity Matrix or Distance Matrix?\\nExample from T an, Steinbach & Kumar, Ch.8 (2006)\\n18 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 31, 'page_label': '32'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n√\\nSimilarity functions\\n� Hierarchical clustering\\n� Density-based clustering: DBSCAN\\n19 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 32, 'page_label': '33'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nHierarchical clustering\\nThis is a family of methods that processes your data to return atree of\\nclusters.\\nY ou run a horizontal cut on this tree to get a set of clusters.\\n20 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 33, 'page_label': '34'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nHierarchical clustering\\nThis is a family of methods that processes your data to return atree of\\nclusters.\\nY ou run a horizontal cut on this tree to get a set of clusters.\\nY ou use algorithms of this family because they are more robust, and you\\ncan easily switch one against the other, toﬁt the properties of your data\\nbetter.\\n20 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 34, 'page_label': '35'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nHierarchical clustering\\nThis is a family of methods that processes your data to return atree of\\nclusters- a dendrogramm.\\nExample of a dendrogramm T an, Steinbach & Kumar, Ch.8 (2006)\\nY ou run a horizontal cut on this tree to get a set of clusters.\\n21 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 35, 'page_label': '36'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nBottom-up and top-down hierarchical clustering algorithms\\nGiven is a set of data points D.\\nBottom-up: Agglomerative clustering T an, Steinbach & Kumar, Ch.8 (2006)\\nStarting with the bottom layer, where each data point is a cluster of its own:\\n22 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 36, 'page_label': '37'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nBottom-up and top-down hierarchical clustering algorithms\\nGiven is a set of data points D.\\nBottom-up: Agglomerative clustering T an, Steinbach & Kumar, Ch.8 (2006)\\nStarting with the bottom layer, where each data point is a cluster of its own:\\nT op-down: Divisive clustering T an, Steinbach & Kumar, Ch.8 (2006)\\nStarting with a single cluster that contains all data points:\\nrepeat\\nSplit one cluster into two\\nuntilk clusters remain\\nwhere k may be a user-deﬁned number or k =|D| .\\n22 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 37, 'page_label': '38'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms\\nClustering a small dataset T an, Steinbach & Kumar, Ch.8 (2006)\\nWhich data points constitute theﬁrst cluster? 23 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 38, 'page_label': '39'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms\\nAgglomerative clustering algorithms differ in their deﬁnition of ”proximity\\nbetween two clusters”.\\n� MIN - single link\\n� MAX - complete link\\n� Group average\\n� Distance between centroids\\n� Ward’s method - squared error\\nComputing proximity/distance T an, Steinbach & Kumar, Ch.8 (2006)\\n24 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 39, 'page_label': '40'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms:MIN\\nDistance between clusters X and Y\\nfor each x∈X\\nfor each y∈Y\\ncompute dist(x,y)\\nsort the distance values into a list LX,Y\\n25 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 40, 'page_label': '41'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms:MIN\\nDistance between clusters X and Y\\nfor each x∈X\\nfor each y∈Y\\ncompute dist(x,y)\\nsort the distance values into a list LX,Y\\nMIN – single link\\nThe distance between X,Y is the minimum value in LX,Y .\\nd(X,Y) =minL X,Y\\n25 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 41, 'page_label': '42'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: MIN\\nApplying MIN on the dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n26 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 42, 'page_label': '43'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: MAX\\nDistance between clusters X and Y\\nfor each x∈X\\nfor each y∈Y\\ncompute dist(x,y)\\nsort the distance values into a list LX,Y\\nMAX – complete link\\nThe distance between X,Y is the maximum value in LX,Y :\\nd(X,Y) =maxL X,Y\\n27 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 43, 'page_label': '44'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: MAX\\nApplying MAX on the dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n28 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 44, 'page_label': '45'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Group Average\\nDistance between clusters X and Y\\nfor each x∈X\\nfor each y∈Y\\ncompute dist(x,y)\\nsort the distance values into a list LX,Y\\nGroup Average\\nThe distance between X,Y is the average value in LX,Y , i.e.\\nd(X,Y) = ∑x∈X (∑y∈Y dist(x,y))\\n|X| · |Y|\\n29 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 45, 'page_label': '46'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Group Average\\nApplying Group Average on the example dataset T an, Steinbach & Kumar,\\nCh.8 (2006)\\n30 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 46, 'page_label': '47'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Ward’s method\\nWard’s method\\nThe distance between two clusters is the increase in the sum of squared\\nerrors that incurs when merging them.\\nApplying Ward’s method on the example dataset T an, Steinbach & Kumar,\\nCh.8 (2006)\\n31 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 47, 'page_label': '48'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Juxtaposing the methods\\nAdvantages of MIN T an, Steinbach & Kumar, Ch.8 (2006)\\nMIN can discover non-spherical clusters\\nSee also Fig. 8.2(c), contiguity-based clusters.\\nDisadvantages of MIN T an, Steinbach & Kumar, Ch.8 (2006)\\nMIN is sensitive to outliers and noise.\\n32 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 48, 'page_label': '49'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Juxtaposing the methods\\nAdvantages of MAX T an, Steinbach & Kumar, Ch.8 (2006)\\nMAX is not inﬂuenced by outliers and noise.\\nDisadvantages of MAX T an, Steinbach & Kumar, Ch.8 (2006)\\nMAX tends to split large clusters and favours spherical clusters.\\n33 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 49, 'page_label': '50'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms\\nAdvantages:\\n+ Number of clusters needs not be speciﬁed.\\n+ Simple statistics and visualizations help deciding the cut position.\\nDisadvantages\\n− Complexity is O(N3) (N=|D| ) (for some variants: O(N2 logN) ).\\n− The order of merging affects the quality of subsequent iterations.\\nDealing with outliers– quoting from the 2019 edition, p. 564-565:\\nOutliers pose the most serious problems for Ward’s method and\\ncentroid-based hierarchical clustering approaches because they\\nincrease SSE and distort centroids. . . . As hierarchical clustering\\nproceeds for these algorithms, outliers or small groups of outliers tend\\nto form singleton or small clusters that do not merge with any other\\nclusters until much later in the merging process. By discarding\\nsingleton or small clusters that are not merging with other clusters,\\noutliers can be removed.\\n34 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 50, 'page_label': '51'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n√\\nSimilarity functions\\n√\\nHierarchical clustering\\n� Density-based clustering: DBSCAN\\n35 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 51, 'page_label': '52'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering - DBSCAN\\nThis is a family of methods that deﬁnes a cluster as a set of overlapping\\nneighbourhoods.\\nDBSCAN 1 is the family progenitor.\\n1M. Ester, H.-P . Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering\\nclusters in large spatial databases with noise.In Proc. of the 2nd Int. Conf. on Knowledge\\nDiscovery and Data Mining (KDD’96), pages 226–231, 1996.\\n36 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 52, 'page_label': '53'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering - DBSCAN\\nThis is a family of methods that deﬁnes a cluster as a set of overlapping\\nneighbourhoods.\\nDBSCAN 1 is the family progenitor.\\nY ou use algorithms of this family when you know that your data have some\\ndense areas surrounded by noise.\\n1M. Ester, H.-P . Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering\\nclusters in large spatial databases with noise.In Proc. of the 2nd Int. Conf. on Knowledge\\nDiscovery and Data Mining (KDD’96), pages 226–231, 1996.\\n36 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 53, 'page_label': '54'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering - DBSCAN\\nThis is a family of methods that deﬁnes a cluster as a set of overlapping\\nneighbourhoods.\\nDBSCAN 1 is the family progenitor.\\nY ou use algorithms of this family when you know that your data have some\\ndense areas surrounded by noise.\\nThese algorithms are best for geographical data, but there are further\\napplication areas.\\n1M. Ester, H.-P . Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering\\nclusters in large spatial databases with noise.In Proc. of the 2nd Int. Conf. on Knowledge\\nDiscovery and Data Mining (KDD’96), pages 226–231, 1996.\\n36 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 54, 'page_label': '55'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering with DBSCAN\\nThe core concept of DBSCAN is that of a neighbourhood around a data\\npoint.\\nDensely populated neighbourhoods form the basis for a cluster.\\nA cluster is a maximal set of overlapping, densely populated\\nneighbourhoods.\\n37 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 55, 'page_label': '56'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering with DBSCAN\\nThe core concept of DBSCAN is that of a neighbourhood around a data\\npoint.\\nDensely populated neighbourhoods form the basis for a cluster.\\nA cluster is a maximal set of overlapping, densely populated\\nneighbourhoods.\\nDBSCAN does not place all data points to clusters. Around each cluster\\nthere are sparsely populated areas, which are not part of any cluster.\\n37 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 56, 'page_label': '57'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nDBSCAN basic concepts T an, Steinbach & Kumar, Ch.8 (2006)\\nA point x is a core point, given eps and minPts, iff there are at least minPts\\npoints within radius eps around x. A point y is a border point if there is a\\ncore point x, such that dist(x,y)≤eps . All other points are noise points.\\n38 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 57, 'page_label': '58'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nDBSCAN parameters\\n� eps: radius of the hypersphere a around a data point, i.e. the\\nneighbourhoodof the data point\\n� minPts: neighbourhood size – can be deﬁned as\\n· number of neighbours of the data point in the center of the\\nhypersphere\\n· number of data points inside the hypersphere\\naThe hypersphere is deﬁned in the feature space F.\\n39 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 58, 'page_label': '59'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofreachabilityin DBSCAN\\nGiven are two data points x,y in the dataset D over feature space F, and\\nﬁxed parameter values for eps,minPts :\\nDirectly density-reachable data point\\ny isdirectly-density reachablefrom x iff\\n� x is a core point AND\\n� y is in the neighbourhood of x\\n40 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 59, 'page_label': '60'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofreachabilityin DBSCAN\\nGiven are two data points x,y in the dataset D over feature space F, and\\nﬁxed parameter values for eps,minPts :\\nDirectly density-reachable data point\\ny isdirectly-density reachablefrom x iff\\n� x is a core point AND\\n� y is in the neighbourhood of x\\nDensity-reachable data point\\ny isdensity reachablefrom x iff there are x1, . . . ,xn ∈D such that:\\n� x=x 1 and y=x n AND\\n� for each i=2, . . . ,n it holds that xi is directly density-reachable from xi−1\\n40 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 60, 'page_label': '61'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofreachabilityin DBSCAN\\nGiven are two data points x,y in the dataset D over feature space F, and\\nﬁxed parameter values for eps,minPts :\\nDirectly density-reachable data point\\ny isdirectly-density reachablefrom x iff\\n� x is a core point AND\\n� y is in the neighbourhood of x\\nDensity-reachable data point\\ny isdensity reachablefrom x iff there are x1, . . . ,xn ∈D such that:\\n� x=x 1 and y=x n AND\\n� for each i=2, . . . ,n it holds that xi is directly density-reachable from xi−1\\nDensity-connected data points\\nx,y are density-connected iff there is a data point z∈D such that:\\n� x is density-reachable from z AND\\n� y is density-reachable from z\\n40 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 61, 'page_label': '62'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofclusterin DBSCAN\\nGiven is a dataset D over feature space F, andﬁxed parameter values for\\neps,minPts .\\nCluster in DBSCAN\\nA cluster C⊆D is a non-empty subset of D that has following properties:\\n� Connectivity:for each x,y∈C it holds that x,y are\\ndensity-connected.\\n� Maximality:for each x,y∈D such that x∈C and x,y are\\ndensity-connected, it holds that y∈C .\\n41 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 62, 'page_label': '63'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofclusterin DBSCAN\\nGiven is a dataset D over feature space F, andﬁxed parameter values for\\neps,minPts .\\nCluster in DBSCAN\\nA cluster C⊆D is a non-empty subset of D that has following properties:\\n� Connectivity:for each x,y∈C it holds that x,y are\\ndensity-connected.\\n� Maximality:for each x,y∈D such that x∈C and x,y are\\ndensity-connected, it holds that y∈C .\\nThe noise areas\\nLet C1, . . . ,Ck be all the clusters found by DBSCAN for D.\\nnoise(D) =D\\\\∪ k\\ni=1Ci\\n41 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 63, 'page_label': '64'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nDBSCAN - simpliﬁed T an, Steinbach & Kumar, Ch.8 (2006)\\n42 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 64, 'page_label': '65'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nExample (1 of 3): the dataset\\n43 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 65, 'page_label': '66'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nExample (2 of 3) for eps=1 , minPts=5 : core points and\\nneighbourhoods\\n44 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 66, 'page_label': '67'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nExample (3 of 3) for eps=1 , minPts=5 : clusters and noise areas 2\\n2Beware of a mistake: the blue cluster has two core points, not one.\\n45 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 67, 'page_label': '68'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nAdvantages of DBSCAN\\n� DBSCAN canﬁnd clusters of arbitrary geometrical form\\n� DBSCAN is insensitive to noise\\n46 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 68, 'page_label': '69'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nAdvantages of DBSCAN\\n� DBSCAN canﬁnd clusters of arbitrary geometrical form\\n� DBSCAN is insensitive to noise\\nDisadvantages of DBSCAN\\n� DBSCAN runs into difﬁculties when there are dense areas with\\ndifferent density\\n46 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 69, 'page_label': '70'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN: Disadvantage\\nExample dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n47 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 70, 'page_label': '71'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN: Disadvantage\\nExample dataset T an, Steinbach & Kumar, Ch.8 (2006)\\nT wo runs forMinPts=4 T an, Steinbach & Kumar, Ch.8 (2006)\\neps=9.75\\n eps=9.92 47 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 71, 'page_label': '72'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n√\\nSimilarity functions\\n√\\nHierarchical clustering\\n√\\nDensity-based clustering: DBSCAN\\n48 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 72, 'page_label': '73'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n√\\nSimilarity functions\\n√\\nHierarchical clustering\\n√\\nDensity-based clustering: DBSCAN\\nHow toﬁnd out whether the clustering algorithm has built good clusters?\\n48 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 73, 'page_label': '74'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nWhat to evaluate when you do clustering?\\n49 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 74, 'page_label': '75'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nWhat to evaluate when you do clustering?\\n� How good are the clusters?\\n� How good is the algorithm?\\n50 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 75, 'page_label': '76'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation in Clustering\\n� Evaluation of cluster quality 1 − →Internal Indices\\n� Evaluation of cluster quality 2 − →Models of randomness\\n� Evaluation of algorithm performance − →External Indices\\n51 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 76, 'page_label': '77'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\n� Sum of Squared Errors\\n� Cohesion and Separation\\n� Silhouette Coefﬁcient\\n52 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 77, 'page_label': '78'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Sum of Squared Errors\\nQuality on the basis of distance of data points to ”their” centroid\\n· ∀X∈ζ,x∈X:pointSSE(x) =d(x,center(X)) 2\\n53 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 78, 'page_label': '79'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Sum of Squared Errors\\nQuality on the basis of distance of data points to ”their” centroid\\n· ∀X∈ζ,x∈X:pointSSE(x) =d(x,center(X)) 2\\n· ∀X∈ζ:cluSSE(X) = 1\\n|X| ∑x∈X pointSSE(x)\\n53 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 79, 'page_label': '80'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Sum of Squared Errors\\nQuality on the basis of distance of data points to ”their” centroid\\n· ∀X∈ζ,x∈X:pointSSE(x) =d(x,center(X)) 2\\n· ∀X∈ζ:cluSSE(X) = 1\\n|X| ∑x∈X pointSSE(x)\\n· SSE(ζ) = 1\\n|ζ| ∑X∈ζ cluSSE(X) = 1\\n|ζ| ∑X∈ζ\\n�\\n1\\n|X| ∑x∈X d(x,center(X)) 2\\n�\\n53 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 80, 'page_label': '81'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Cohesion and Separation\\nQuality as in-cluster homogeneity and between-clusters gap\\n1. Centroid-based indices: ∀X∈ζ\\n· cohesion(X) =1− 1\\n|X| ∑x∈X d(x,center(X))\\n· separation(X) = 1\\n|ζ|−1 ∑Y∈ζ,Y�=X d(center(X),center(Y))\\nwhere values closer to 1 are better.\\nExample from T an, Steinbach & Kumar, Ch.8 (2006)\\n54 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 81, 'page_label': '82'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Cohesion and Separation\\nQuality as in-cluster homogeneity and between-clusters gap\\n2. Graph-based indices: ∀X∈ζ\\n· cohesion(X) =1− 1\\n|X|(|X|−1) ∑x,y∈X;x�=y d(x,y)\\n· separation(X) = 1\\n|ζ|−1 ∑Y∈ζ\\\\X\\n1\\n|X|·|Y|\\n�\\n∑x∈X,y∈Y d(x,y)\\n�\\nwhere values closer to 1 are better.\\nExample from T an, Steinbach & Kumar, Ch.8 (2006)\\n55 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 82, 'page_label': '83'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality\\nInternal indices of cluster quality:\\n√\\nSum of Square Errors\\n√\\nCohesion and Separation: center-based and graph-based\\n√\\nSilhouette Coefﬁcient\\nand a visualization aid: Similarity matrix sorted on clusterID\\n57 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 83, 'page_label': '84'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality\\nInternal indices of cluster quality:√\\nSum of Square Errors√\\nCohesion and Separation: center-based and graph-based√\\nSilhouette Coefﬁcient\\nand a visualization aid: Similarity matrix sorted on clusterID\\nA clustered dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n58 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 84, 'page_label': '85'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality and the problem of random data\\nAnother clustered dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n59 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 85, 'page_label': '86'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality and the problem of random data\\nExample continued T an, Steinbach & Kumar, Ch.8 (2006)\\n60 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 86, 'page_label': '87'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality and the problem of random data\\nThe data we cluster may actually have NO clustering structure.\\nA clustering algorithm run on those data will in any case return some\\nclusters.\\n? When is the value of the internal index good enough ?\\n? How to recognize whether the boxes in the similarity matrix are\\nindeed clusters ?\\n61 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 87, 'page_label': '88'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation in Clustering\\n√\\nEvaluation of cluster quality 1 − →Internal Indices\\n� Evaluation of cluster quality 2 − →Models of randomness\\n62 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 88, 'page_label': '89'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality with help ofModels of Randomness\\nModels of Randomness - Approach 1 from T an, Steinbach & Kumar, Ch.8\\n(2006)\\nGiven is a dataset D with feature space F, and a set of clusters ξ learned\\nwith algorithm A .\\nFOR i=1. . .N\\n� generate a random dataset Di, so that |Di|=|D| and Fi =F\\n� derive a model ζi for Di using A with the same parameter settings as\\nused for ζ .\\n� compute the quality of ζi with an internal index q()\\nDO compute the histogramm of the quality values of the N models\\nDO compare q(ξ) with the values on the histogramm\\nDO DISCARD ξ if it is in the wrong area of the plot ???\\n63 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 89, 'page_label': '90'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality with help ofModels of Randomness\\nApproach 1 - Example from T an, Steinbach & Kumar, Ch.8 (2006)\\nWhen is the SSE value of the model ξ good? 64 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 90, 'page_label': '91'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality with help ofModels of Randomness\\nModels of Randomness - Approach 2 Uli Niemann (Master thesis) 2\\nGiven is a dataset D with feature space F, and a set of clusters ξ learned\\nwith algorithm A .\\nA model-of-randomness over D,F , i.e. MofR(D,F) is a set of clusters ζ ,\\nsuch that |ζ|=|ξ| and the assignment of points to clusters in ζ is\\nrandom.\\nFOR i=1. . .N , generate a MofR(D,F) , ζi\\nDO compute the histogramm of the quality values of the N models\\nDO compare q(ξ) with the values on the histogramm\\nDO DISCARD ξ if it is in the wrong area of the plot\\n3\\n3Niemann, U., Spiliopoulou, M., V ¨olzke, H., and K ¨uhn, J.-P . (2014). Subpopulation\\nDiscovery in Epidemiological Data with Subspace Clustering.Foundations of Computing\\nand Decision Sciences(FCDS),39(4):271–300. 65 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 91, 'page_label': '92'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation in Clustering\\n√\\nEvaluation of cluster quality 1 − →Internal Indices\\n√\\nEvaluation of cluster quality 2 − →Models of randomness\\n� Evaluation of algorithm performance − →External Indices\\n66 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 92, 'page_label': '93'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nExternal indices\\nT wo groups of external indices:\\nGroup 1: How well did the clustering algorithm guess the true classes of the\\ndata?\\n� Entropy\\n� Purity\\n� Precision & Recall BLOCK: Classiﬁcation\\n� F-measure BLOCK: Classiﬁcation\\nGroup 2: T o what extend do the clusters agree with the classes?√\\nRand Index√\\nJaccard Coefﬁcient\\nwhere higher values are better.\\n67 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 93, 'page_label': '94'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nExternal indices group 1 - class-oriented indices\\nGiven is the dataset D, the true assignment of the data points in D to\\nclasses ϕ={C 1, . . . ,CL} and a set of clusters ξ={X 1, . . . ,XK } over D.\\nEntropy\\nentropy(ξ,ϕ) =\\nK\\n∑\\ni=1\\n|Xi|\\n|D| clusEntropy(Xi,ϕ)\\nwhere clusEntropy(Xi,ϕ) =− ∑L\\nj=1 pij log2(pij) with pij = |Xi∩Cj|\\n|Xi| .\\n68 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 94, 'page_label': '95'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nExternal indices group 1 - class-oriented indices\\nGiven is the dataset D, the true assignment of the data points in D to\\nclasses ϕ={C 1, . . . ,CL} and a set of clusters ξ={X 1, . . . ,XK } over D.\\nEntropy\\nentropy(ξ,ϕ) =\\nK\\n∑\\ni=1\\n|Xi|\\n|D| clusEntropy(Xi,ϕ)\\nwhere clusEntropy(Xi,ϕ) =− ∑L\\nj=1 pij log2(pij) with pij = |Xi∩Cj|\\n|Xi| .\\nPurity\\npurity(ξ,ϕ) =\\nK\\n∑\\ni=1\\n|Xi|\\n|D| clusPurity(Xi,ϕ)\\nwhere clusPurity(Xi,ϕ) =max j=1...L{pij},\\ni.e. each cluster Xi is assigned to the class, to which most of its members\\nbelong.\\n68 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 95, 'page_label': '96'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClosing\\n√\\nBasic clustering algorithms: K-Means, DBSCAN, hierarchical\\nclustering methods\\n√\\nSimilarity/distance functions for clustering\\n√\\nEvaluation of internal cluster quality\\n√\\nEvaluation of algorithm performance towards a ground truth\\n69 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 96, 'page_label': '97'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClosing\\nClustering encompasses much more:\\n� More basic algorithms like K-Medoids\\n� Algorithms that perceive the data as a graph: Graph clustering\\nalgorithms\\n� Algorithms that build clusters in subsets of the feature space:\\n� Subspace clustering\\n� Projected clustering\\n� Algorithms that exploit knowledge of the expert:\\n� Constraint-based clustering algorithms\\n� Semi-supervised clustering algorithms\\n� Algorithms that assign the data probabilistically to groups\\n� Probabilistic clustering algorithms\\n� Fuzzy clustering algorithms, like C-Means\\n� T opic models (generative models over the data space)\\n70 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 97, 'page_label': '98'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThank you very much!\\nQuestions?\\n71 / 71'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 0, 'page_label': '1'}, page_content='BLOCK Data Engineering - Unit 3 on Feature\\nSelection\\nMyra Spiliopoulou 1\\n1Faculty of Computer Science, Otto-von-Guericke University Magdeburg'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 1, 'page_label': '2'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nLiterature of the unit\\nBOOK: Salvador Garcia, Julian Luengo, Francisco Herrera (2015) Data\\nPreprocessing in Data Mining , SPRINGER International Publishing\\nSwitzerland\\n2 Chapter 3 ’Data Preparation Basic Models’\\n1 Chapter 4 ’Dealing with Missing Values’\\n3 Chapter 7 ’Feature Selection’\\nand some words on\\n3.2.1 Data Integration –’Finding Redundant Attributes’ → [Unit 3]\\n3.5 Data Transformation → [Unit 3]\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 2/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 2, 'page_label': '3'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 3/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 3, 'page_label': '4'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 4/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 4, 'page_label': '5'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nRECALL: Running example on patient response to treatment\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes better no r no yes\\n#2 m M no better no b yes no\\n#3 m M no worse no b no no\\n#4 f VH yes worse no b no yes\\n#5 m L no no effect no l no no\\n#6 m M no better no l no no\\n#7 f ?? yes better yes l yes yes\\n#8 f H no better yes r no no\\n#9 f H yes better no l no yes\\n#10 m M yes worse no b no no\\n#11 m M no no effect no l no no\\n#12 f H no no effect no r no no\\n#13 ?? ?? yes ?? no l yes yes\\n#14 m M no worse no b no no\\n#15 m L no no effect no l yes no\\nWe use the data to train models and acquire insights. Therefore we must:√ deal with missing values,√ clean away the errors in the data,√ normalize the data, remove duplicates and\\n→ eliminate redundant variables, because\\nlearning algorithms do not cope well with high-dimensional spaces and\\nbecause the more variables we seek to record, the more likely it is that we will\\nhave missing values in them.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 5/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 5, 'page_label': '6'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe real counterpart of our tiny dataset [Schleicher et al., 2024]\\n• Original dataset from Charit ´e Universit ¨atsmedizin Berlin: patients with tinnitus\\nduration of at least 3 months, an age of at least 18 years and sufficient knowledge\\nof the German language, in the period January 2011 until October 2015, treated\\n[. . . ], exclusion criteria [. . . ] N=3971\\n• Further exclusion criteria: treatment finished, no intermediate visits, only one\\ntreatment sequence, no longer than 15 days N=1450\\n• Constraint: no missing values in the 9 questionnaires of the study N=1287\\n• Random sample of 500 patients: N=500 (f:240, m: 260)\\nTable 1: lists the questionnaires and the number of items in each one\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 6/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 6, 'page_label': '7'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 7/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 7, 'page_label': '8'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection: What and Why\\nDefinition of FSel [Section 7.1]\\n’Feature selection is a process that chooses the optimal subset of features\\naccording to a certain criterion.’\\nOptimality / Goodness ⇒ Purpose of the feature selection process a\\naFrom Section 4.1 and from citation [48]:Sayes Y ., Inza I, Larranaga P (2007) A review of\\nfeature selection techniques in bioinformatics. Bioinformatics 23(19), 2507-2517\\n⋆ discard redundant features\\n⋆ reduce the cost of data acquisition\\nand when applying FSel in preparation of an already known learning task:\\n◦ discard irrelevant features, and thus also irrelevant data\\n◦ increase the accuracy/quality of the learned models\\n◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 8/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 8, 'page_label': '8'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection: What and Why\\nDefinition of FSel [Section 7.1]\\n’Feature selection is a process that chooses the optimal subset of features\\naccording to a certain criterion.’\\nOptimality / Goodness ⇒ Purpose of the feature selection process a\\naFrom Section 4.1 and from citation [48]:Sayes Y ., Inza I, Larranaga P (2007) A review of\\nfeature selection techniques in bioinformatics. Bioinformatics 23(19), 2507-2517\\n⋆ discard redundant features\\n⋆ reduce the cost of data acquisition\\nand when applying FSel in preparation of an already known learning task:\\n◦ discard irrelevant features, and thus also irrelevant data\\n◦ increase the accuracy/quality of the learned models\\n◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 8/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 9, 'page_label': '8'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection: What and Why\\nDefinition of FSel [Section 7.1]\\n’Feature selection is a process that chooses the optimal subset of features\\naccording to a certain criterion.’\\nOptimality / Goodness ⇒ Purpose of the feature selection process a\\naFrom Section 4.1 and from citation [48]:Sayes Y ., Inza I, Larranaga P (2007) A review of\\nfeature selection techniques in bioinformatics. Bioinformatics 23(19), 2507-2517\\n⋆ discard redundant features\\n⋆ reduce the cost of data acquisition\\nand when applying FSel in preparation of an already known learning task:\\n◦ discard irrelevant features, and thus also irrelevant data\\n◦ increase the accuracy/quality of the learned models\\n◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 8/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 10, 'page_label': '9'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection Tasks in our Example\\nFrom Table 1 in [Schleicher et al., 2024]:\\nPos Name # Items Outcome\\nof interest\\n1 SOZK 25 -\\n2 TQ 52 YES\\n3 PSQ 30 YES\\n4 SF8 8 –\\n5 SWOP 9 –\\n6 TLQ 8 –\\n7 BSF 30 –\\n8 BI 56 –\\n9 ADSL 20 YES\\n238 YES: 102\\nFSel tasks with known outcomes of\\ninterest:\\n⋆ remove redundant features\\n⇒ reduce cost / burden\\n◦ remove irrelevant features\\n⇒ reduce cost / burden\\n◦ reduce model complexity\\n⇒ make it more actionable for DS\\nexcept that we cannot remove individual\\nfeatures, only complete questionnaires.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 9/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 11, 'page_label': '9'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection Tasks in our Example\\nFrom Table 1 in [Schleicher et al., 2024]:\\nPos Name # Items Outcome\\nof interest\\n1 SOZK 25 -\\n2 TQ 52 YES\\n3 PSQ 30 YES\\n4 SF8 8 –\\n5 SWOP 9 –\\n6 TLQ 8 –\\n7 BSF 30 –\\n8 BI 56 –\\n9 ADSL 20 YES\\n238 YES: 102\\nFSel tasks with known outcomes of\\ninterest:\\n⋆ remove redundant features\\n⇒ reduce cost / burden\\n◦ remove irrelevant features\\n⇒ reduce cost / burden\\n◦ reduce model complexity\\n⇒ make it more actionable for DS\\nexcept that we cannot remove individual\\nfeatures, only complete questionnaires.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 9/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 12, 'page_label': '10'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest for the Good Features (Section 7.2.1.1, with modifications)\\nGiven a feature space F and a goodness measure U: Build the subset of the\\ngood features S from scratch or rather by discarding features from F ?\\nSequential forward and backward feature set generation algorithms\\nSFG Sequential forward\\nfeature set generation\\nfunction SFG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextBest(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S satisfies U or F = / 0\\nreturn S\\nend function\\nSBG Sequential backward\\nfeature set generation\\nfunction SBG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextWorst(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S does not satisfy U or F = / 0\\nreturn F ∪ {f } // add the last feature again\\nend function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).\\nHence, each newly selected feature f is best/worst given the previous\\nselections.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 10/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 13, 'page_label': '10'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest for the Good Features (Section 7.2.1.1, with modifications)\\nGiven a feature space F and a goodness measure U: Build the subset of the\\ngood features S from scratch or rather by discarding features from F ?\\nSequential forward and backward feature set generation algorithms\\nSFG Sequential forward\\nfeature set generation\\nfunction SFG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextBest(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S satisfies U or F = / 0\\nreturn S\\nend function\\nSBG Sequential backward\\nfeature set generation\\nfunction SBG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextWorst(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S does not satisfy U or F = / 0\\nreturn F ∪ {f } // add the last feature again\\nend function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).\\nHence, each newly selected feature f is best/worst given the previous\\nselections.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 10/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 14, 'page_label': '10'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest for the Good Features (Section 7.2.1.1, with modifications)\\nGiven a feature space F and a goodness measure U: Build the subset of the\\ngood features S from scratch or rather by discarding features from F ?\\nSequential forward and backward feature set generation algorithms\\nSFG Sequential forward\\nfeature set generation\\nfunction SFG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextBest(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S satisfies U or F = / 0\\nreturn S\\nend function\\nSBG Sequential backward\\nfeature set generation\\nfunction SBG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextWorst(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S does not satisfy U or F = / 0\\nreturn F ∪ {f } // add the last feature again\\nend function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).\\nHence, each newly selected feature f is best/worst given the previous\\nselections.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 10/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 15, 'page_label': '11'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest cntd (Algorithms 3 & 4, with modifications)\\nCombining SFG and SBG into: Bidirectional feature set generation algorithm\\nBG Bidirectional feature set generation\\nfunction BG(F, U)\\ninitialize Sg = / 0; Sb = / 0; Fg = F ; Fb = F\\nrepeat\\nfg = FindNextBest(Fg, U) ; Sg = Sg ∪ {fg} ; Fg = Fg \\\\ {fg}\\nfb = FindNextWorst(Fb, U) ; Sb = Sb ∪ {fb} ; Fb = Fb \\\\ {fb}\\nuntil (a)Sf satisfies U or Ff = / 0 or (b) Sb does not satisfy U or Fb = / 0\\nif (a) holds then return Sf else return Fb ∪ {fb} endif\\nend function\\nGenerating subsets of F at random:\\nRG Random feature set generation\\nfunction RG(F, U)\\ninitialize S = Sbest / 0; cbest = |F|\\nrepeat\\nS = RandGen(F) // pick features from F at random\\nif |S| ≤ cbest and S satisfies U then Sbest = S ; cbest = |S| endif\\nuntil some stopping criterion is satisfied\\nreturn Sbest\\nend function\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 11/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 16, 'page_label': '11'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest cntd (Algorithms 3 & 4, with modifications)\\nCombining SFG and SBG into: Bidirectional feature set generation algorithm\\nBG Bidirectional feature set generation\\nfunction BG(F, U)\\ninitialize Sg = / 0; Sb = / 0; Fg = F ; Fb = F\\nrepeat\\nfg = FindNextBest(Fg, U) ; Sg = Sg ∪ {fg} ; Fg = Fg \\\\ {fg}\\nfb = FindNextWorst(Fb, U) ; Sb = Sb ∪ {fb} ; Fb = Fb \\\\ {fb}\\nuntil (a)Sf satisfies U or Ff = / 0 or (b) Sb does not satisfy U or Fb = / 0\\nif (a) holds then return Sf else return Fb ∪ {fb} endif\\nend function\\nGenerating subsets of F at random:\\nRG Random feature set generation\\nfunction RG(F, U)\\ninitialize S = Sbest / 0; cbest = |F|\\nrepeat\\nS = RandGen(F) // pick features from F at random\\nif |S| ≤ cbest and S satisfies U then Sbest = S ; cbest = |S| endif\\nuntil some stopping criterion is satisfied\\nreturn Sbest\\nend functionMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 11/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 17, 'page_label': '12'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nHow long to search? (Section 7.2.1.2, with modifications)\\n▶ Exhaustive search:\\nGenerate all non-empty subsets of F, i.e. P(F) \\\\ {/ 0}, and choose the\\nbest one\\n· ok if F is small\\n· nok if F is large ⇒ set a threshold m as the minimum number of features to be\\nselected or removed\\n▶ Heuristic search:\\nTraverse the search space of solutions by adhering to some heuristic,\\nwhich also incorporates a termination criterion\\n▶ Non-deterministic search:\\nGenerate subsets of F at random (cf. Algorithm RG) without an explicit\\ntermination criterion ⇒ at each time point, RG() returns the best subset\\nfound thus far 1\\nTo which category does each of SFG, SBG and BG belong?\\n1In ’stream mining’, best effort algorithms of this kind are calledanytime algorithms.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 12/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 18, 'page_label': '12'}, page_content='Running\\nexample Feature Selection Process Goodness\\nCriteria Filter\\ns & Wrappers Closing\\nHo\\nw long to search? (Section 7.2.1.2, with modifications)\\n▶ Exhaustiv\\ne search:\\nGenerate all non-empty subsets of F, i.e. P(F) \\\\ {/ 0}, and choose the\\nbest one\\n· ok if F is small\\n· nok if F is large ⇒ set a threshold m as the minimum number of features to be\\nselected or removed\\n▶ Heuristic search:\\nTraverse the search space of solutions by adhering to some heuristic,\\nwhich also incorporates a termination criterion\\n▶ Non-deterministic search:\\nGenerate subsets of F at random (cf. Algorithm RG) without an explicit\\ntermination criterion ⇒ at each time point, RG() returns the best subset\\nfound thus far 1\\nT\\no which category does each of SFG, SBG and BG belong?\\n1In\\n’stream mining’, best effort algorithms of this kind are calledanytime algorithms.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 12/28\\nHeuristic search'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 19, 'page_label': '13'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen what feature selection means, and what reasons\\nare there to perform a feature selection process.\\nWe have seen some ways of building a ’good’ subset of the original set of\\nfeatures, assuming a criterion of ’goodness’, such as redundancy (negative\\ncriterion) or good predictive power towards a target (positive criterion)\\nY our turn:Y ou must be able to explain the four feature set construction\\nalgorithms, name their termination criteria (if any), and say whether each of\\nthem is exhaustive, heuristic or non-deterministic – and explain why.\\nWhat comes next: ’Goodness’ quantified\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 13/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 20, 'page_label': '13'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen what feature selection means, and what reasons\\nare there to perform a feature selection process.\\nWe have seen some ways of building a ’good’ subset of the original set of\\nfeatures, assuming a criterion of ’goodness’, such as redundancy (negative\\ncriterion) or good predictive power towards a target (positive criterion)\\nY our turn:Y ou must be able to explain the four feature set construction\\nalgorithms, name their termination criteria (if any), and say whether each of\\nthem is exhaustive, heuristic or non-deterministic – and explain why.\\nWhat comes next: ’Goodness’ quantified\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 13/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 21, 'page_label': '13'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen what feature selection means, and what reasons\\nare there to perform a feature selection process.\\nWe have seen some ways of building a ’good’ subset of the original set of\\nfeatures, assuming a criterion of ’goodness’, such as redundancy (negative\\ncriterion) or good predictive power towards a target (positive criterion)\\nY our turn:Y ou must be able to explain the four feature set construction\\nalgorithms, name their termination criteria (if any), and say whether each of\\nthem is exhaustive, heuristic or non-deterministic – and explain why.\\nWhat comes next: ’Goodness’ quantified\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 13/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 22, 'page_label': '14'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 14/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 23, 'page_label': '15'}, page_content='Running\\nexample Feature Selection Process Goodness\\nCriteria Filter\\ns & Wrappers Closing\\nGoodness\\nas ’redundancy’: Correlations between features (1a)\\nχ 2 f\\nor two categorical attributes (Section 3.2.1.1)\\nLet A, B be\\ntwo nominal (categorical) attributes with c, respectively r distinct\\nvalues, in a dataset with m instances. We compute\\nχ 2 =\\nc\\n∑\\ni=1\\nr\\n∑\\nj=1\\n(oij − eij)2\\neij\\nwhere oij is\\nthe observed frequency of the joint event (Ai, Bj), and\\neij = count(A=ai)×count(B=bj)\\nm is\\nthe expected frequency.\\nWe test against the hypothesis that A, B are independent (H 0)\\nwith\\n(c − 1) × (r − 1) degrees of freedom. a\\naT\\no perform a statistical test like χ 2 we use a table or a software that can provide the values\\nfor the likelihood of the independence hypothesis.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 15/28\\nhow often the feature are independent, and how often its is observed together\\nif both are dependent on each other we might discard any one of them.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 24, 'page_label': '16'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness as ’redundancy’: Correlations between features (1b)\\nPearson product moment coeff for two numerical attributes (Section 3.2.1.2)\\nA, B with means A, B, standard deviations σA, σB:\\nrA,B = ∑m\\ni=1(ai − A)(bi − B)\\nmσAσB\\n∈ [−1, +1]\\nwhere (ai, bi) the values of A, B at the ith instance in the dataset, i = 1 . . .m.\\nPearson correlation coeff (Section 7.2.2.3)\\nmeasures the degree of linear correlation between two variables X, Y with\\nmeasurements {xi} and {yi} and means x, y:\\nρ(X, Y) = ∑i()xi − x)(yi − y)p\\n( ∑i(xi − x)2 ∑i(yi − y)2)\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 16/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 25, 'page_label': '16'}, page_content='Running\\nexample Feature Selection Process Goodness\\nCriteria Filter\\ns & Wrappers Closing\\nGoodness\\nas ’redundancy’: Correlations between features (1b)\\nP\\nearson product moment coeff for two numerical attributes (Section 3.2.1.2)\\nA, B with\\nmeans A, B,\\nstandard deviations σA, σB:\\nrA,B = ∑m\\ni=1(ai − A)\\n(bi − B)\\nmσAσB\\n∈ [−1, +1]\\nwhere (ai, bi) the\\nvalues of A, B at the ith instance in the dataset, i = 1 . . .m.\\nP\\nearson correlation coeff (Section 7.2.2.3)\\nmeasures\\nthe degree of linear correlation between two variables X, Y with\\nmeasurements {xi} and {yi} and means x, y:\\nρ(X, Y)\\n= ∑i()xi − x)\\n(yi − y)p\\n( ∑i(xi − x)2 ∑i(yi − y)2)\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 16/28\\nvalue closer to one will decribe it the two feature values are strongly corellated or no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 26, 'page_label': '17'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness as ’redundancy’: Correlations between features (1c)\\nCovariance between two numerical attributes (Section 3.2.1.2)\\nCov(A, B) = E((A − A)(B − B)) = 1\\nm\\nm\\n∑\\ni=1\\n(ai − A)(bi − B)\\n· If A, B are independent, then E(A · B) = E(A) · E(B), and thus\\nCov(A, B) = 0\\n· If A, B vary similarly, then whenever A > A we also expect that\\nB > B, and thus the covariance will be positive.\\n· If A, B vary in opposite directions, then whenever A > A we\\nexpect that B < B, and thus the covariance will be negative.\\nPearson product moment coeff and the covariance matrix\\nrA,B = ∑m\\ni=1(ai−A)(bi−B)\\nm · 1\\nσAσB\\n= Cov(A,B)\\nσAσB\\n(Eq. 3.5)\\n’\\n 2\\n2Covariance trends.svg.png from Cmglee, CC BY -SA 4.0\\nhttps://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 17/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 27, 'page_label': '17'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness as ’redundancy’: Correlations between features (1c)\\nCovariance between two numerical attributes (Section 3.2.1.2)\\nCov(A, B) = E((A − A)(B − B)) = 1\\nm\\nm\\n∑\\ni=1\\n(ai − A)(bi − B)\\n· If A, B are independent, then E(A · B) = E(A) · E(B), and thus\\nCov(A, B) = 0\\n· If A, B vary similarly, then whenever A > A we also expect that\\nB > B, and thus the covariance will be positive.\\n· If A, B vary in opposite directions, then whenever A > A we\\nexpect that B < B, and thus the covariance will be negative.\\nPearson product moment coeff and the covariance matrix\\nrA,B = ∑m\\ni=1(ai−A)(bi−B)\\nm · 1\\nσAσB\\n= Cov(A,B)\\nσAσB\\n(Eq. 3.5)\\n’\\n 2\\n2Covariance trends.svg.png from Cmglee, CC BY -SA 4.0\\nhttps://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 17/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 28, 'page_label': '17'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness as ’redundancy’: Correlations between features (1c)\\nCovariance between two numerical attributes (Section 3.2.1.2)\\nCov(A, B) = E((A − A)(B − B)) = 1\\nm\\nm\\n∑\\ni=1\\n(ai − A)(bi − B)\\n· If A, B are independent, then E(A · B) = E(A) · E(B), and thus\\nCov(A, B) = 0\\n· If A, B vary similarly, then whenever A > A we also expect that\\nB > B, and thus the covariance will be positive.\\n· If A, B vary in opposite directions, then whenever A > A we\\nexpect that B < B, and thus the covariance will be negative.\\nPearson product moment coeff and the covariance matrix\\nrA,B = ∑m\\ni=1(ai−A)(bi−B)\\nm · 1\\nσAσB\\n= Cov(A,B)\\nσAσB\\n(Eq. 3.5)\\n’\\n 2\\n2Covariance trends.svg.png from Cmglee, CC BY -SA 4.0\\nhttps://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 17/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 29, 'page_label': '18'}, page_content='Running\\nexample Feature Selection Process Goodness\\nCriteria Filter\\ns & Wrappers Closing\\nGoodness\\nas ’redundancy’: Correlations between features (1d)\\nSpear\\nman correlation coeff\\nis\\nthe Pearson correlation coeff for the ranks of the\\nvariables, i.e.\\nrs = ρ(R(X), R(Y)) = Cov(R(X), R(Y))\\nσR(X)σR(Y)\\nwhere X, Y are\\nnumerical variables and R(X), R(Y) are their\\nranks.\\nSpear\\nman correlation coeff can be used for ordinal vari-\\nables, and, iff all ranks are distinct integers, then it holds that\\n[Fieller et al., 1957] : rs = 1 − 6 ∑m\\ni=1(xi−yi)2\\nm3−m\\n3\\n3Upper\\nfig: upload.wikimedia.org/wikipedia/commons/4/4e/Spearman_fig1.svg\\nLower fig: upload.wikimedia.org/wikipedia/commons/6/67/Spearman_fig3.svg\\nSkbkekas, CC BY -SA 3.0https://creativecommons.org/licenses/by-sa/3.0, via\\nWikimedia Commons\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 18/28\\nfor categorical data- usage of chi square is preffered\\nbelow one depends on numerical data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 30, 'page_label': '19'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nRECALL [Unit 2] Example of a correlation analysis [Niemann et al., 2020] 4\\nCorrelation analysis (with Spearman correlation coefficient) on a dataset of 4,117\\ntinnitus patients who had been treated at the Tinnitus Center of Charit´e\\nUniversitaetsmedizin Berlin between January 2011 and October 2015.\\nFigure 3\\nFeature-feature correlation & feature correlation\\nwith respect to TQ tinnitus-related distress score\\nin T0 and T1.\\n(A) Correlation heatmap for all pairs of features\\n(T0). Features are ordered by agglomerative\\nhierarchical clustering with complete linkage. (B)\\nCorrelation of each feature with TQ\\ntinnitus-related distress score, in T0 (x-axis) and\\nin T1 (y-axis). The diamond symbol represents a\\nquestionnaire’s median. (C) Top-20 features with\\nhighest correlation to TQ tinnitus-related distress\\nscore (T0). (D) Top-20 features with highest\\ncorrelation to TQ tinnitus-related distress score\\n(T1). (E) Top-10 features whose correlational\\neffects with TQ tinnitus-related distress score\\ndiffer in T0 vs. T1. Correlation values before and\\nafter treatment are shown as light blue and dark\\nblue bars, respectively. Differences in correlation\\nare represented as black bars centered in\\nbetween.\\n4Article at DOI: 10.1371/journal.pone.0228037, figure at DOI: 10.1371/journal.pone.0228037Myra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 19/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 31, 'page_label': '20'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness towards a target variable (2)\\nQuantifying goodness as uncertainty U\\nGiven a set of k classes/labels C with prior class probability P(ci) for each i = 1, . . . ,k\\nand a feature space F:\\n◦ Information measures: What is the information gain achieved when we consider a\\nfeature A ∈ F (i) in comparison to P(ck) or (ii) in comparison to considering another\\nfeature B ∈ F ? (Section 7.2.2.1)\\n◦ Accuracy-like measures: How does accuracy change when (i) we consider A ∈ F\\nadditionally to previous selected features, (ii) we skip A ∈ F ? (Section 7.2.2.5)\\nExample: information gain as uncertainty reduction (Section 7.2.2.1)\\nThe ’Information Gain’ achieved by considering a featureA is the difference between\\nthe prior uncertainty ∑k\\ni=1 U(P(ci)) and the expected posterior uncertainty using A:\\nIG(A) =\\nk\\n∑\\ni=1\\nU(P(ci)) − E\\n \\nk\\n∑\\ni=1\\nU(P(ci|A))\\n!\\nand as U() we use Shannon’s entropy (or a derived function), so that:\\n∑k\\ni=1 U(P(ci)) = − ∑k\\ni=1 P(ci) log2 P(ci)\\nand accordingly for the expected posterior uncertainty given A.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 20/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 32, 'page_label': '20'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness towards a target variable (2)\\nQuantifying goodness as uncertainty U\\nGiven a set of k classes/labels C with prior class probability P(ci) for each i = 1, . . . ,k\\nand a feature space F:\\n◦ Information measures: What is the information gain achieved when we consider a\\nfeature A ∈ F (i) in comparison to P(ck) or (ii) in comparison to considering another\\nfeature B ∈ F ? (Section 7.2.2.1)\\n◦ Accuracy-like measures: How does accuracy change when (i) we consider A ∈ F\\nadditionally to previous selected features, (ii) we skip A ∈ F ? (Section 7.2.2.5)\\nExample: information gain as uncertainty reduction (Section 7.2.2.1)\\nThe ’Information Gain’ achieved by considering a featureA is the difference between\\nthe prior uncertainty ∑k\\ni=1 U(P(ci)) and the expected posterior uncertainty using A:\\nIG(A) =\\nk\\n∑\\ni=1\\nU(P(ci)) − E\\n \\nk\\n∑\\ni=1\\nU(P(ci|A))\\n!\\nand as U() we use Shannon’s entropy (or a derived function), so that:\\n∑k\\ni=1 U(P(ci)) = − ∑k\\ni=1 P(ci) log2 P(ci)\\nand accordingly for the expected posterior uncertainty given A.Myra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 20/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 33, 'page_label': '21'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen ways of quantifying goodness of features.\\nWe focused on correlations between two features, and we have seen some of\\nthe many many functions that can be used to compute correlations. We have\\nalso re-visited a visualization instrument for showing all pairs of correlation\\ncoefficient values in a heatmap matrix.\\nWe have also seen examples of functions that compute the goodness of a\\nfeature for class separation.\\nY our turn:Y ou must be able to compute correlation coefficients for example\\ncases, and to explain correlation results. Y ou must be also able to compute\\ninformation gain with Shannon’s entropy.\\nWhat comes next: Closing the workflow of feature selection\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 21/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 34, 'page_label': '21'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen ways of quantifying goodness of features.\\nWe focused on correlations between two features, and we have seen some of\\nthe many many functions that can be used to compute correlations. We have\\nalso re-visited a visualization instrument for showing all pairs of correlation\\ncoefficient values in a heatmap matrix.\\nWe have also seen examples of functions that compute the goodness of a\\nfeature for class separation.\\nY our turn:Y ou must be able to compute correlation coefficients for example\\ncases, and to explain correlation results. Y ou must be also able to compute\\ninformation gain with Shannon’s entropy.\\nWhat comes next: Closing the workflow of feature selection\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 21/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 35, 'page_label': '21'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen ways of quantifying goodness of features.\\nWe focused on correlations between two features, and we have seen some of\\nthe many many functions that can be used to compute correlations. We have\\nalso re-visited a visualization instrument for showing all pairs of correlation\\ncoefficient values in a heatmap matrix.\\nWe have also seen examples of functions that compute the goodness of a\\nfeature for class separation.\\nY our turn:Y ou must be able to compute correlation coefficients for example\\ncases, and to explain correlation results. Y ou must be also able to compute\\ninformation gain with Shannon’s entropy.\\nWhat comes next: Closing the workflow of feature selection\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 21/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 36, 'page_label': '22'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 22/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 37, 'page_label': '23'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nWhat is missing from the feature selection workflow?\\nWe already have√ functions that help us decide whether a feature is good to keep or rather to discard√ procedures to traverse the search space, i.e. all combinations of features, and\\ncreate feature subsets√ stopping criteria\\nbut how good is our constructed subset of features ? for a learning algorithm ?\\n↙\\nTraining and testing\\nGiven a sample of the data D that is representative\\nof the population under study:\\n· We split D into a training set Dtrain and a test set\\nDtest so that D = Dtrain ∪ Dtest and\\nDtrain ∩ Dtest = / 0.\\n· We use Dtrain to train the learning algorithm and\\ninduce a model M\\n· We use Dtest to test the behaviour of M on\\npreviously unseen data\\n→\\n? How to test the impact of\\nthe feature selection\\nprocess on the model M\\n? And WHEN?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 23/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 38, 'page_label': '23'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nWhat is missing from the feature selection workflow?\\nWe already have√ functions that help us decide whether a feature is good to keep or rather to discard√ procedures to traverse the search space, i.e. all combinations of features, and\\ncreate feature subsets√ stopping criteria\\nbut how good is our constructed subset of features ? for a learning algorithm ?\\n↙\\nTraining and testing\\nGiven a sample of the data D that is representative\\nof the population under study:\\n· We split D into a training set Dtrain and a test set\\nDtest so that D = Dtrain ∪ Dtest and\\nDtrain ∩ Dtest = / 0.\\n· We use Dtrain to train the learning algorithm and\\ninduce a model M\\n· We use Dtest to test the behaviour of M on\\npreviously unseen data\\n→\\n? How to test the impact of\\nthe feature selection\\nprocess on the model M\\n? And WHEN?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 23/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 39, 'page_label': '23'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nWhat is missing from the feature selection workflow?\\nWe already have√ functions that help us decide whether a feature is good to keep or rather to discard√ procedures to traverse the search space, i.e. all combinations of features, and\\ncreate feature subsets√ stopping criteria\\nbut how good is our constructed subset of features ? for a learning algorithm ?\\n↙\\nTraining and testing\\nGiven a sample of the data D that is representative\\nof the population under study:\\n· We split D into a training set Dtrain and a test set\\nDtest so that D = Dtrain ∪ Dtest and\\nDtrain ∩ Dtest = / 0.\\n· We use Dtrain to train the learning algorithm and\\ninduce a model M\\n· We use Dtest to test the behaviour of M on\\npreviously unseen data\\n→\\n? How to test the impact of\\nthe feature selection\\nprocess on the model M\\n? And WHEN?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 23/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 40, 'page_label': '23'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nWhat is missing from the feature selection workflow?\\nWe already have√ functions that help us decide whether a feature is good to keep or rather to discard√ procedures to traverse the search space, i.e. all combinations of features, and\\ncreate feature subsets√ stopping criteria\\nbut how good is our constructed subset of features ? for a learning algorithm ?\\n↙\\nTraining and testing\\nGiven a sample of the data D that is representative\\nof the population under study:\\n· We split D into a training set Dtrain and a test set\\nDtest so that D = Dtrain ∪ Dtest and\\nDtrain ∩ Dtest = / 0.\\n· We use Dtrain to train the learning algorithm and\\ninduce a model M\\n· We use Dtest to test the behaviour of M on\\npreviously unseen data\\n→\\n? How to test the impact of\\nthe feature selection\\nprocess on the model M\\n? And WHEN?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 23/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 41, 'page_label': '24'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFilters (Section 7.2.3.1) & Wrappers (Section 7.2.3.2)\\nFilters\\nfilter the good from the no-good features before learning.\\nStage 1: Feature subset selection with a goodness criterion and a feature set\\ngeneration algorithm – it delivers its best subset of features\\nStage 2: The learning algorithm uses the data of this subset of features only, first for\\ntraining, then for testing\\nSubcategory Rankers: Stage 1 delivers all features, each one ranked on goodness;\\nat stage 2, the algorithm applies a threshold to pick the top-ranked features\\nWrappers\\nengage a classifier to decide whether a feature should be kept or discarded,\\ndepending on its impact on classification quality.\\nStage 1: Feature subset selection, where the goodness function is in the blackbox of\\nthe classification algorithm – it delivers the best subset of features, from the\\nviewpoint of the classifier\\nStage 2: as for Filters, but\\n! a model must be induced on the best subset of features\\n! and tested on data that have not been seen before\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 24/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 42, 'page_label': '24'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFilters (Section 7.2.3.1) & Wrappers (Section 7.2.3.2)\\nFilters\\nfilter the good from the no-good features before learning.\\nStage 1: Feature subset selection with a goodness criterion and a feature set\\ngeneration algorithm – it delivers its best subset of features\\nStage 2: The learning algorithm uses the data of this subset of features only, first for\\ntraining, then for testing\\nSubcategory Rankers: Stage 1 delivers all features, each one ranked on goodness;\\nat stage 2, the algorithm applies a threshold to pick the top-ranked features\\nWrappers\\nengage a classifier to decide whether a feature should be kept or discarded,\\ndepending on its impact on classification quality.\\nStage 1: Feature subset selection, where the goodness function is in the blackbox of\\nthe classification algorithm – it delivers the best subset of features, from the\\nviewpoint of the classifier\\nStage 2: as for Filters, but\\n! a model must be induced on the best subset of features\\n! and tested on data that have not been seen before\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 24/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 43, 'page_label': '25'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 25/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 44, 'page_label': '26'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nSummary and Outlook\\nWe have seen\\na workflow for the feature selection process:√ Ways of building up a feature subset by adding or discarding features√ Functions that quantify the goodness of features√ Two types of link to the learning algorithm: filter the feature space before\\nlearning, or wrap the learning algorithm into the feature selection workflow\\nInstruments that we skipped:\\n· Methods for feature construction in a projected space, e.g. ’Principal\\nComponent Analysis’\\n· Methods quantifying the discriminative power of a feature towards a target,\\ne.g. SHAP\\nWhat comes next: Shifting from static data to time series\\n4 What is a time series\\n4 What to learn on one time series, and what to learn on many\\n4 Imputing missing values − →filling gaps\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 26/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 45, 'page_label': '27'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThank you very much!\\nQuestions?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 27/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 46, 'page_label': '28'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nBibliography I\\nFieller, E. C., Hartley, H. O., and Pearson, E. S. (1957). Tests for rank\\ncorrelation coefficients. i. Biometrika, 44(3/4):470–481.\\nNiemann, U., Boecking, B., Brueggemann, P ., Mebus, W., Mazurek, B.,\\nand Spiliopoulou, M. (2020). Tinnitus-related distress after multimodal\\ntreatment can be characterized using a key subset of baseline variables.\\nPLOS ONE, 15(1):1–18.\\nSchleicher, M., Br¨uggemann, P ., B¨ocking, B., Niemann, U., Mazurek, B.,\\nand Spiliopoulou, M. (2024). Parsimonious predictors for medical\\ndecision support: Minimizing the set of questionnaires used for tinnitus\\noutcome prediction. Expert Systems with Applications , 239:122336.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 28/28')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    # loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c42a78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df623de2",
   "metadata": {},
   "source": [
    "## Split into chucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92312ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, # Each chunk: ~1000 characters\n",
    "        chunk_overlap=chunk_overlap, # 200 chars overlap for context\n",
    "        length_function=len, # How to measure length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Split hierarchy\n",
    "    )\n",
    "    # Actually split the documents\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show what a chunk looks like\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd66ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 224 documents into 239 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: DM I: Block ’Classification’\n",
      "Unit ’Decision Trees’\n",
      "Myra Spiliopoulou...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "chunks= split_documents(pdf_documents,chunk_size=1000,chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5493c0f",
   "metadata": {},
   "source": [
    "## embedding and vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774e47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer   \n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fe17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\python.exe\n",
      "Imports OK: sentence_transformers and chromadb available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "print(\"Imports OK: sentence_transformers and chromadb available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd0808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'all-MiniLM-L6-v2'...\n",
      "Model 'all-MiniLM-L6-v2' loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1cb117ebf10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.model =None\n",
    "        self.load_model()#load teh model during initialization\n",
    "    \n",
    "    def load_model(self):       \n",
    "        try:\n",
    "            print(f\"Loading model '{self.model_name}'...\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model '{self.model_name}' loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "            #convert text to dimension 384\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model '{self.model_name}': {e}\")\n",
    "            raise e\n",
    "    def generate_embedding(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        print(f\"Generating embedding for text of length {len(texts)}...\")\n",
    "        embedding = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embedding of shape {embedding.shape}.\")\n",
    "        return embedding\n",
    "   \n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b90e81",
   "metadata": {},
   "source": [
    "## Vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb10462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized at '../data/vector_store' with collection 'pdf_documents'.\n",
      "Current number of vectors in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1cb11bb2cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collecion_name: str = \"pdf_documents\",persist_directory: str=\"../data/vector_store\"):\n",
    "    \n",
    "        self.collecion_name = collecion_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "        \n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            #create directory if not exists\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            \n",
    "            #where to store the vectors inisde vector db like in firestore\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collecion_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "                )\n",
    "            print(f\"Vector store initialized at '{self.persist_directory}' with collection '{self.collecion_name}'.\")\n",
    "            print(f\"Current number of vectors in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        \n",
    "        ids=[]\n",
    "        metadatas=[]\n",
    "        documents_texts=[]\n",
    "        embeddings_list=[]\n",
    "        \n",
    "        for i, (doc , embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            metadata=dict(doc.metadata)\n",
    "            metadata[\"doc_index\"]=i\n",
    "            metadata[\"context_length\"]=len(doc.page_content)\n",
    "            metadatas.append(doc.metadata)\n",
    "            \n",
    "            \n",
    "            documents_texts.append(doc.page_content)\n",
    "            \n",
    "            \n",
    "            embeddings_list.append(embedding.tolist())\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_texts,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Added {len(documents)} documents to vector store.\")\n",
    "            print(f\"New number of vectors in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise e\n",
    "        \n",
    "vector_store = VectorStore()\n",
    "vector_store\n",
    "            \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67b386e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 0, 'page_label': '1'}, page_content='DM I: Block ’Classification’\\nUnit ’Decision Trees’\\nMyra Spiliopoulou'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 1, 'page_label': '2'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nMaterials\\n▶ Algorithms and equations: Chapter 3 of the course book\\n▶ Pictures: Chapter 4 of the 1st edition, but with pointers to the course\\nbook\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 2/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 2, 'page_label': '3'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 3/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 3, 'page_label': '4'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms\\nDT for verterbrate classification Tan et al., Ch.4 (2006) a\\naIn the book of the course, this is Figure 3.4\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 4/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 4, 'page_label': '5'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms\\nUsing the DT for verterbrate classification Tan et al., Ch.4 (2006) a\\naIn the book of the course, this is Figure 3.5\\nto classify a flamingo:\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 5/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 5, 'page_label': '6'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms\\nLearning decision trees:\\n▶ A very old, simple tree induction algorithm: Hunt’s algorithm\\n▶ Split criteria for decision tree learners\\n▶ Learning bushy classifiers\\n▶ Learning binary classifiers\\non the example of the patient responses’ dataset:\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes better no r no yes\\n#2 m M no better no b yes no\\n#3 m M no worse no b no no\\n#4 f VH yes worse no b no yes\\n#5 m L no no effect no l no no\\n#6 m M no better no l no no\\n#7 f VH yes better yes l yes yes\\n#8 f H no better yes r no no\\n#9 f H yes better no l no yes\\n#10 m M yes worse no b no no\\n#11 m M no no effect no l no no\\n#12 f H no no effect no r no no\\n#13 f L yes better no l yes yes\\n#14 m M no worse no b no no\\n#15 m L no no effect no l yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 6/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 6, 'page_label': '7'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Hunt’s algorithm\\nHunt’s algorithm based on [Ch 3, Section 3.3.1]\\nINPUT: Training setD, Labelset L = {y1, . . . ,yk}\\nAt the current node v, invoke\\nhunt(v,L)\\nIF exists y ∈ L such that ∀x ∈ v : label(x) = y\\nTHEN do\\n1. set y as the label of the whole v\\n2. return\\nELSE do\\n1. compute children(v) by invoking split(v,L)\\n2. for each u ∈ children(v)\\nhunt(u,L)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 7/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 7, 'page_label': '8'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 8/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 8, 'page_label': '9'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nWhich split is better ?\\nTypical objectives for the split function\\n▶ Minimize the impurity with respect to the target variable\\n▶ Minimize the misclassification rate\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 9/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 9, 'page_label': '9'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nWhich split is better ?\\nTypical objectives for the split function\\n▶ Minimize the impurity with respect to the target variable\\n▶ Minimize the misclassification rate\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 9/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 10, 'page_label': '9'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nWhich split is better ?\\nTypical objectives for the split function\\n▶ Minimize the impurity with respect to the target variable\\n▶ Minimize the misclassification rate\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 9/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 11, 'page_label': '10'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels. 1\\nExample split functions\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\n1cf. Equations 3.4, 3.5, 3.6 – notice the differences in notation\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 10/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 12, 'page_label': '10'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels. 1\\nExample split functions\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\n1cf. Equations 3.4, 3.5, 3.6 – notice the differences in notation\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 10/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 13, 'page_label': '10'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels. 1\\nExample split functions\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\n1cf. Equations 3.4, 3.5, 3.6 – notice the differences in notation\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 10/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 14, 'page_label': '11'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels.\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(t) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\nHow do these functions differ in their behaviour?\\n▶ Compute the values of the split functions for these nodes a:\\nv1 : 0 members with label Y , 6 members with label N\\nv2 : 1 member with label Y , 5 members with label N\\nv3 : 3 members with label Y , 3 members with label N\\n▶ Compute the values of the split functions for the 7 attributes in the\\n’patients responses’ dataset\\naExamples from Tan et al., Ch.4 (2006)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 11/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 15, 'page_label': '11'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nLet D be the training set, v ⊆ D be a tree node, and L = {y1, . . . ,yk} be the\\nset of labels.\\nMisclassificationRate(v) = 1 − max\\ny∈L\\np(y|v)\\nGini(v) = 1 − ∑\\ny∈L\\np(y|v)2\\nentropy(t) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nwhere 0 log 0is defined to be zero.\\nHow do these functions differ in their behaviour?\\n▶ Compute the values of the split functions for these nodes a:\\nv1 : 0 members with label Y , 6 members with label N\\nv2 : 1 member with label Y , 5 members with label N\\nv3 : 3 members with label Y , 3 members with label N\\n▶ Compute the values of the split functions for the 7 attributes in the\\n’patients responses’ dataset\\naExamples from Tan et al., Ch.4 (2006)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 11/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 16, 'page_label': '12'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Split functions\\nBehaviour of the split functions for binary classification a\\naIn the book of the course, this is Figure 3.11\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 12/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 17, 'page_label': '13'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 13/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 18, 'page_label': '14'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nMore on splits 1/2\\nRECALL example:\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nTwo ways of splitting an attribute that takes<< 2 values\\n⋆ multi-split: one child node per attribute value\\n⋆ binary split: pick one value zz and split into two child nodes, one for zz\\nand one for ’notzz’\\nTwo types of classifiers – bushy & binary\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 14/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 19, 'page_label': '14'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nMore on splits 1/2\\nRECALL example:\\nEXAMPLE: Candidate splits on the ’patient responses’ dataset\\n▶ Split on I2:\\n▶ I2=f [yes:(#1,#4,#7,#9,#13), no:(#8,#12)]\\n▶ I2=m [yes: (), no: (#2,#3,#5,#6,#10, #11, #14, #15)]\\n▶ Split on I22:\\n▶ I22=better [yes: (#1,#7, #9, #13), no: (#2,#6,#8)]\\n▶ I22=worse [yes: (#4), no: (#3,#10, #14)]\\n▶ I22=no effect [yes: (), no: (#5, #11, #12, #15)]\\nTwo ways of splitting an attribute that takes<< 2 values\\n⋆ multi-split: one child node per attribute value\\n⋆ binary split: pick one value zz and split into two child nodes, one for zz\\nand one for ’notzz’\\nTwo types of classifiers – bushy & binary\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 14/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 20, 'page_label': '15'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nMore on splits 2/2\\nHow to split a node v on a continuous attribute a?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 15/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 21, 'page_label': '16'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Dealing with non-categorical attributes\\nOrder preserving n-split of a node v on an attribute a that takes continuous\\nvalues:\\n▶ Greedy way:\\nIteratively consider candidate positions within the valuerange of a, e.g.\\n· by sampling n values randomly for some n (input parameter)\\n▶ Discretization:\\n· in an unsupervised way, e.g. by\\nbuilding n homogeneous and well-separated clusters, or\\nby building a histogramm of equisized bins\\n· in a supervised way, e.g. by partitioning the data so as to minimize the\\nimpurity measure\\nacquiring n groups of instances 2.\\nThis results in a candidate split of of node v to n children, i.e. to a set\\nchildren(v,a), for which we can then compute the gain on inpurity.\\n2The number of groups n may be an input parameter or may be derived.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 21, 'page_label': '16'}, page_content='children(v,a), for which we can then compute the gain on inpurity.\\n2The number of groups n may be an input parameter or may be derived.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 16/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 22, 'page_label': '17'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 17/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 23, 'page_label': '18'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithm - Quinlan’s ID3 (simplified)\\nLearning a bushy classifier with ID3\\nINPUT: Training setD, Labelset L = {y1, . . . ,yk}, set of attributes A\\nAt the current node v, invoke\\nID3(v,L,A)\\nIF ∃y ∈ L such that for most of the x ∈ v : label(x) = y THEN do\\nset y as the label of the whole v and return\\nELSE IF A = / 0THEN do\\n1. identify the majority class label of v, y\\n2. set y as the label of the whole v and return\\nELSE do\\n1. set abest as the arg maxa∈A{InfGain(v,L,a)}\\n2. compute children(v,abest) by splitting v on the values of abest\\n3. for each u ∈ children(v,abest), invoke ID3(u,L,A \\\\ {abest})\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 18/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 24, 'page_label': '19'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Entropy and Information Gain\\nGain on impurity\\nFor a node v split on the values of attribute a ∈ A into children(v,a), the gain\\nof this split is:\\n∆(v,a) = I(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| I(u)\\nwhere I(·) is an impurity measure 3.\\nIf we set\\nI(v) := entropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nthen ∆(v,a) ≡ InfGain(v,a). 4\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\n3See description of gain around Equations 3.7, 3.8 (different notation)\\n4NOTE: L is fixed, so InfGain(v,a) ≡ InfGain(v,L,a).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 19/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 25, 'page_label': '19'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Entropy and Information Gain\\nGain on impurity\\nFor a node v split on the values of attribute a ∈ A into children(v,a), the gain\\nof this split is:\\n∆(v,a) = I(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| I(u)\\nwhere I(·) is an impurity measure 3. If we set\\nI(v) := entropy(v) = − ∑\\ny∈L\\np(y|v) logp(y|v)\\nthen ∆(v,a) ≡ InfGain(v,a). 4\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\n3See description of gain around Equations 3.7, 3.8 (different notation)\\n4NOTE: L is fixed, so InfGain(v,a) ≡ InfGain(v,L,a).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 19/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 26, 'page_label': '20'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Information Gain and Gain Ratio\\nFor a node v split on the values of attribute a ∈ A into children(v,a) . . .\\nInformation Gain\\nInfGain(v,a) = entropy(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| entropy(u)\\nIntrinsic Information from Piatesky-Shapiro\\nIntrinsicInformation(v,a) = − ∑\\nu∈children(v,a)\\n|u|\\n|v| log |u|\\n|v|\\nGain Ratio from (Quinlan, 1986)\\nThe gain ratio achieved when splitting v on a is\\nGainRatio(v,a) = InfGain(v,a)\\nIntrinsicInformation(v,a)\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 20/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 27, 'page_label': '20'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Information Gain and Gain Ratio\\nFor a node v split on the values of attribute a ∈ A into children(v,a) . . .\\nInformation Gain\\nInfGain(v,a) = entropy(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| entropy(u)\\nIntrinsic Information from Piatesky-Shapiro\\nIntrinsicInformation(v,a) = − ∑\\nu∈children(v,a)\\n|u|\\n|v| log |u|\\n|v|\\nGain Ratio from (Quinlan, 1986)\\nThe gain ratio achieved when splitting v on a is\\nGainRatio(v,a) = InfGain(v,a)\\nIntrinsicInformation(v,a)\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 20/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 28, 'page_label': '20'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Information Gain and Gain Ratio\\nFor a node v split on the values of attribute a ∈ A into children(v,a) . . .\\nInformation Gain\\nInfGain(v,a) = entropy(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| entropy(u)\\nIntrinsic Information from Piatesky-Shapiro\\nIntrinsicInformation(v,a) = − ∑\\nu∈children(v,a)\\n|u|\\n|v| log |u|\\n|v|\\nGain Ratio from (Quinlan, 1986)\\nThe gain ratio achieved when splitting v on a is\\nGainRatio(v,a) = InfGain(v,a)\\nIntrinsicInformation(v,a)\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 20/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 29, 'page_label': '20'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Information Gain and Gain Ratio\\nFor a node v split on the values of attribute a ∈ A into children(v,a) . . .\\nInformation Gain\\nInfGain(v,a) = entropy(v) − ∑\\nu∈children(v,a)\\n|u|\\n|v| entropy(u)\\nIntrinsic Information from Piatesky-Shapiro\\nIntrinsicInformation(v,a) = − ∑\\nu∈children(v,a)\\n|u|\\n|v| log |u|\\n|v|\\nGain Ratio from (Quinlan, 1986)\\nThe gain ratio achieved when splitting v on a is\\nGainRatio(v,a) = InfGain(v,a)\\nIntrinsicInformation(v,a)\\nWhich attribute gives the best root split in the ’patient responses’ dataset?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 20/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 30, 'page_label': '21'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 21/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 31, 'page_label': '22'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nTree Induction Algorithms - Binary Classifiers\\nLearning a binary classifier\\nINPUT: Training setD, Labelset L = {y1, . . . ,yk}, set of (attribute,value)-pairs\\nAV(A) ≡ AV for the set of attributes A\\nAt the current node v, invoke\\nbinCore(v,L,AV)\\nIF ∃y ∈ L such that for most of the x ∈ v : label(x) = y THEN do\\nset y as the label of the whole v and return\\nELSE IF AV = / 0THEN do\\n1. identify the majority class label of v, y\\n2. set y as the label of the whole v and return\\nELSE do\\n1. Choose the best binary split of v into v1,v2\\n2. Remove from AV the pair that caused the best binary split\\n3. For each u ∈ {v1,v2} invoke binCore(u,L,AV)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 22/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 32, 'page_label': '23'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\n1 Basics on tree induction\\n2 Functions for node splitting\\n3 More on splits\\n4 Bushy DTs – ’multi-splits’\\n5 Binary DTs – ’binary splits’\\n6 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 23/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 33, 'page_label': '24'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nProgress and outlook\\nWe have seen:\\n√ How to build a decision tree by recursively splitting the dataset into more\\nand more homogeneous nodes – where homogeneity refers to the\\ntarget variable\\n√ Split functions that implement different definitions of ’homogeneity’\\n√ Different types of decision tree, depending on whether a node has\\nexactly two children or as many children as the values of the splitting\\nattribute\\nEach type of decision tree and each type of split function lead to a different\\nclassifier:\\n▶ How to figure out how good a classifier is?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 24/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-14T23:13:47+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Decision Trees'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-14T23:13:47+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_DTs.pdf', 'total_pages': 35, 'page': 34, 'page_label': '25'}, page_content='Basics on tree induction Functions for node splitting More on splits Bushy DTs – ’multi-splits’ Binary DTs – ’binary splits’ Closing\\nThank you very much! Questions?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Decision Trees’ · · · 25/25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1'}, page_content='DM I: Block ’Classification’\\nUnit ’Naive Bayes’\\nMyra Spiliopoulou'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nEXAMPLE: ’patient responses’ dataset\\nSimplified\\nversion:\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes better no r no yes\\n#2 m M no better no b yes no\\n#3 m M no worse no b no no\\n#4 f VH yes worse no b no yes\\n#5 m L no no effect no l no no\\n#6 m M no better no l no no\\n#7 f VH yes better yes l yes yes\\n#8 f H no better yes r no no\\n#9 f H yes better no l no yes\\n#10 m M yes worse no b no no\\n#11 m M no no effect no l no no\\n#12 f H no no effect no r no no\\n#13 f L yes better no l yes yes\\n#14 m M no worse no b no no\\n#15 m L no no effect no l yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 2/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nEXAMPLE: ’patient responses’ dataset\\nand version with\\nboth categorical\\nand numerical\\nattributes:\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes 1 no r no yes\\n#2 m M no 2 no b yes no\\n#3 m M no 6 no b no no\\n#4 f VH yes 7 no b no yes\\n#5 m L no 3 no l no no\\n#6 m M no 1 no l no no\\n#7 f VH yes 1 yes l yes yes\\n#8 f H no 2 yes r no no\\n#9 f H yes 2 no l no yes\\n#10 m M yes 6 no b no no\\n#11 m M no 3 no l no no\\n#12 f H no 4 no r no no\\n#13 f L yes 4 no l yes yes\\n#14 m M no 7 no b no no\\n#15 m L no 5 no l yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 3/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\n1 The Law of Bayes for Model Learning\\n2 The Zero-Frequency Problem\\n3 NB on numerical attributes\\n4 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 4/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes from Witten & Eibe\\nLaw of Bayes\\nThe probability of an event H given evidence E is: p(H|E) = p(E|H)p(H)\\np(E)\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nAppeared in:\\n“Essay towards solving a problem in the doctrine of chances” (1763)\\nby Thomas Bayes (born: 1702, London; died: 1761, Tunbridge Wells, Kent)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 5/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 5, 'page_label': '5'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes from Witten & Eibe\\nLaw of Bayes\\nThe probability of an event H given evidence E is: p(H|E) = p(E|H)p(H)\\np(E)\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nAppeared in:\\n“Essay towards solving a problem in the doctrine of chances” (1763)\\nby Thomas Bayes (born: 1702, London; died: 1761, Tunbridge Wells, Kent)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 5/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 6, 'page_label': '6'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes Witten & Eibe\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nIn classification, an event is the observation of a class H,\\nwhile E is a record, composed of attributes values E1, . . . ,En in an\\nn-dimensional feature space A = {a1, . . . ,an}.\\nThe independence assumption means that:\\n▶ All attributes contribute equally to the class prediction.\\n▶ Within a given class, the value of an attribute does not influence the\\nvalues of other attributes.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 6/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 7, 'page_label': '6'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes Witten & Eibe\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nIn classification, an event is the observation of a class H,\\nwhile E is a record, composed of attributes values E1, . . . ,En in an\\nn-dimensional feature space A = {a1, . . . ,an}.\\nThe independence assumption means that:\\n▶ All attributes contribute equally to the class prediction.\\n▶ Within a given class, the value of an attribute does not influence the\\nvalues of other attributes.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 6/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 8, 'page_label': '6'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes Witten & Eibe\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nIn classification, an event is the observation of a class H,\\nwhile E is a record, composed of attributes values E1, . . . ,En in an\\nn-dimensional feature space A = {a1, . . . ,an}.\\nThe independence assumption means that:\\n▶ All attributes contribute equally to the class prediction.\\n▶ Within a given class, the value of an attribute does not influence the\\nvalues of other attributes.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 6/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 9, 'page_label': '7'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes\\nLearning phase of Naive Bayes\\nFor each (attribute, value)-pair (a, z) for an attribute a ∈ A:\\n▶ For each label y ∈ L: compute p((a, z)|y)\\nApplication phase with Naive Bayes\\nFor an instance x with unknown label:\\n▶ For each label y ∈ L: compute p(y|x) using the computations of the\\nlearning phase.\\n▶ Assign to x the label with the highest probability, i.e.\\nlabel(x) = arg maxy∈L p(y|x).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 7/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 10, 'page_label': '7'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes\\nLearning phase of Naive Bayes\\nFor each (attribute, value)-pair (a, z) for an attribute a ∈ A:\\n▶ For each label y ∈ L: compute p((a, z)|y)\\nApplication phase with Naive Bayes\\nFor an instance x with unknown label:\\n▶ For each label y ∈ L: compute p(y|x) using the computations of the\\nlearning phase.\\n▶ Assign to x the label with the highest probability, i.e.\\nlabel(x) = arg maxy∈L p(y|x).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 7/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 11, 'page_label': '8'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nEXAMPLE for the ’patient responses’ dataset (categorical attributes only)\\nPriors of the target variable: p(yes) = 5/15, p(no) = 10/15.\\n1. Count the occurrences of each class given the value of each attribute:\\nI2: yes no\\nf 5 2\\nm 0 8\\nI15: yes no\\nVH 3 0\\nH 1 2\\nM 0 6\\nL 1 2\\nI30: yes no\\nyes 5 1\\nno 0 9\\nI22: yes no\\nbetter 4 3\\nworse 1 3\\nno effect 0 4\\nI31: yes no\\nyes 1 1\\nno 4 9\\nI9: yes no\\nr 1 2\\nl 3 4\\nb 1 4\\nI26: yes no\\nyes 2 2\\nno 3 8\\n2. Compute the NB model\\n3.Apply the NB model\\nTo which class should we assign x =<#85,f,VH, yes, no effect, no, l, no> ?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 8/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 12, 'page_label': '9'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\n1 The Law of Bayes for Model Learning\\n2 The Zero-Frequency Problem\\n3 NB on numerical attributes\\n4 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 9/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 13, 'page_label': '10'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes – The Zero-Frequency Problem\\nFor a training set D from a population D, a Labelset L = {y1, . . . ,yk},\\nan attribute a ∈ A and a y ∈ L, let\\nDy = {x ∈ D|label(x) = y}, D(a,zi) = {x ∈ D|x.a = zi}\\nIf there is a value zi that a can take and a label y ∈ L so that p((a, zi)|y) = 0,\\nthen\\nfor all x ∈ D with x.a = zi it holds that: p(y|x) = 0.\\nLaplace Estimator\\nInstead of computing p((a, zi)|y) as\\n|D(a,zi)∩Dy|\\n|Dy| , we set:\\np((a, zi)|y) =\\n|D(a,zi) ∩ Dy| + w\\nna\\n|Dy| + w\\nwhere w is some small weight > 0 and na is the number of distinct values\\nthat attribute a can take.\\nThis is a generalization of the original Laplace estimator, where w = na.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 10/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 14, 'page_label': '10'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes – The Zero-Frequency Problem\\nFor a training set D from a population D, a Labelset L = {y1, . . . ,yk},\\nan attribute a ∈ A and a y ∈ L, let\\nDy = {x ∈ D|label(x) = y}, D(a,zi) = {x ∈ D|x.a = zi}\\nIf there is a value zi that a can take and a label y ∈ L so that p((a, zi)|y) = 0,\\nthen\\nfor all x ∈ D with x.a = zi it holds that: p(y|x) = 0.\\nLaplace Estimator\\nInstead of computing p((a, zi)|y) as\\n|D(a,zi)∩Dy|\\n|Dy| , we set:\\np((a, zi)|y) =\\n|D(a,zi) ∩ Dy| + w\\nna\\n|Dy| + w\\nwhere w is some small weight > 0 and na is the number of distinct values\\nthat attribute a can take.\\nThis is a generalization of the original Laplace estimator, where w = na.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 10/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 15, 'page_label': '10'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes – The Zero-Frequency Problem\\nFor a training set D from a population D, a Labelset L = {y1, . . . ,yk},\\nan attribute a ∈ A and a y ∈ L, let\\nDy = {x ∈ D|label(x) = y}, D(a,zi) = {x ∈ D|x.a = zi}\\nIf there is a value zi that a can take and a label y ∈ L so that p((a, zi)|y) = 0,\\nthen\\nfor all x ∈ D with x.a = zi it holds that: p(y|x) = 0.\\nLaplace Estimator\\nInstead of computing p((a, zi)|y) as\\n|D(a,zi)∩Dy|\\n|Dy| , we set:\\np((a, zi)|y) =\\n|D(a,zi) ∩ Dy| + w\\nna\\n|Dy| + w\\nwhere w is some small weight > 0 and na is the number of distinct values\\nthat attribute a can take.\\nThis is a generalization of the original Laplace estimator, where w = na.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 10/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 16, 'page_label': '11'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes – more on missing values\\nLet x ∈ D be an instance of unknown label, so that the value of x for some\\nattribute a is missing.\\nHow do we deduce the most likely class of x?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 11/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 17, 'page_label': '12'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\n1 The Law of Bayes for Model Learning\\n2 The Zero-Frequency Problem\\n3 NB on numerical attributes\\n4 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 12/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 18, 'page_label': '13'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nNaive Bayes for attributes with continuous valueranges\\nFor a training set D from a population D, a Labelset L = {y1, . . . ,yk},\\nlet a ∈ A be an attribute with a continuous valuerange, and let Da ⊆ D be the\\nset of instances from D, that have a non-NULL value for attribute a.\\nUsing densities in the NB calculations – from Witten & Eibe\\nAssumption: The values of a follow a Gaussian distribution a (also: normal\\ndistribution), with density function:\\nf (x) = 1√\\n2πσ a\\ne\\n− (x−µa)2\\n2σ 2a\\nwhere µa is the sample mean: µa = 1\\n|Da| ∑x∈Da x.a\\nσa is the sample standard deviation, and\\nσ 2\\na the sample variance: σ 2\\na = 1\\n|Da|−1 ∑x∈Da(x.a − µa)2\\naNamed after the German mathematician Karl Friedrich Gauss (1777-1855), who laid the\\nfoundations of number theory.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 13/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 19, 'page_label': '14'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nEXAMPLE for the ’patient responses’ dataset (mixed attributes)\\nPriors of the target variable are p(yes) = 5/15, p(no) = 10/15; the\\ncomputations for all attributes except I22 remain the same.\\nI22: yes no\\n1, 2, 4, 7 1, 2, 3, 4, 5, 6, 7\\nµ =?, σ =? µ =?, σ =?\\nTo which class should we assign x =<#86,f,VH, yes, 4, no, l, no> ?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 14/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 20, 'page_label': '15'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\n1 The Law of Bayes for Model Learning\\n2 The Zero-Frequency Problem\\n3 NB on numerical attributes\\n4 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 15/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 21, 'page_label': '16'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nProgress and outlook\\nWe have seen:\\n√ How to train a classification model with help of the Law of Bayes\\n√ How to deal with values that may occur in the test set and did not occur\\nin the training set\\n√ How to extend the algorithm for learning on mixed (categorical and\\nnumerical) attributes\\nNB classifiers perform usually well, despite the naive assumption,\\nunless\\n→ there are many redundant attributes, or\\n→ the valueranges do not follow a Gaussian distribution\\n⇓\\n▶ How to figure out how good a classifier is?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 16/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-10-07T20:24:48+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\", 'subject': '', 'keywords': '', 'moddate': '2023-10-07T20:24:48+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf', 'total_pages': 23, 'page': 22, 'page_label': '17'}, page_content='The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nThank you very much! Questions?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 17/17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='DM I: Block ’Classification’\\nUnit ’Underpinnings’\\nMyra Spiliopoulou'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='Phases of Classification Example datasets Closing\\n1 Phases of Classification\\n2 Example datasets\\n3 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 2/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different\\nfrom the instances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 3/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 3, 'page_label': '3'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different\\nfrom the instances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 3/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 4, 'page_label': '3'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different\\nfrom the instances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 3/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 5, 'page_label': '3'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different\\nfrom the instances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 3/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 6, 'page_label': '4'}, page_content='Phases of Classification Example datasets Closing\\nClassification – Learning Phase: Model induction &\\ndeduction\\nInducing and deducing a model on data of known class membership\\nbefore applying the model on data of unknown class membership.\\nExample from Tan, Steinbach & Kumar, Ch.4 (2006)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 4/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 7, 'page_label': '5'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nWe split D into a training set to be used for model induction\\nand a test set to be used for model deduction (includes: evaluation).\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different from the\\ninstances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 5/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 8, 'page_label': '5'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nWe split D into a training set to be used for model induction\\nand a test set to be used for model deduction (includes: evaluation).\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different from the\\ninstances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 5/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 9, 'page_label': '5'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nWe split D into a training set to be used for model induction\\nand a test set to be used for model deduction (includes: evaluation).\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different from the\\ninstances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 5/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 10, 'page_label': '5'}, page_content='Phases of Classification Example datasets Closing\\nThe two phases of classification\\nClassification – Learning Phase\\nA classification algorithm takes as input a set of mutually exclusive classes\\nC = {C1, . . . ,Ck}\\nand, for each Ci ∈ C , i = 1 . . .k, a set of instances Di belonging to Ci.\\nThe set D = ∪k\\ni=1Di is used for learning, i.e. for training.\\nD must be representative of the population D under study.\\nWe split D into a training set to be used for model induction\\nand a test set to be used for model deduction (includes: evaluation).\\nThe classification algorithm builds a classifier ξ ,\\ni.e. a model that reflects what makes the instances in each class different from the\\ninstances in the other classes.\\nClassification – Querying Phase\\nThe classifier ξ is used as an ”oracle”.\\nFor each instance x ∈ D, it decides to which class from C does x belong.\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 5/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 11, 'page_label': '6'}, page_content='Phases of Classification Example datasets Closing\\n1 Phases of Classification\\n2 Example datasets\\n3 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 6/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 12, 'page_label': '7'}, page_content='Phases of Classification Example datasets Closing\\nClassification – Example on verterbrate classification\\nOriginal dataset – Tan, Steinbach & Kumar, Ch.4 (2006)\\n▶ Classes: amphibian, bird, fish, mammal, reptile\\n▶ Feature space: Body Temperature, Skin Cover, Gives Birth, Aquatic\\nCreature, Aerial Creature, Has Legs, Hibernates\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 7/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 13, 'page_label': '8'}, page_content='Phases of Classification Example datasets Closing\\nClassification Example Dataset 1 – Verterbrates binary\\nModified dataset, based on Tan, Steinbach & Kumar, Ch.4 (2006): two\\nclasses ’Mammal yes/no’, 6 variables + Id variable\\nId Body\\nTemperature\\n(1)\\nSkin\\nCover\\nGives\\nBirth\\n(2)\\nAquatic\\n(3)\\nAerial\\n(4)\\nLegs\\n(5)\\nHiber-\\nnates\\n(6)\\nMammal\\nhuman warm-blooded hair yes no no two no yes\\npython cold-blooded scales no no no zero yes no\\nsalmon cold-blooded scales no yes no zero no no\\nwhale warm-blooded hair yes yes no zero no yes\\nfrog cold-blooded none no semi no four no no\\nkomodo\\ndragon\\ncold-blooded scales no no no four no no\\nbat warm-blooded hair yes no yes four yes yes\\npigeon warm-blooded feathers no no yes two no no\\ncat warm-blooded fur yes no no four no yes\\nleopard\\nshark\\ncold-blooded scales yes yes no zero no no\\nturtle cold-blooded scales no semi no four no no\\npenguin warm-blooded feathers no semi no two no no\\nporcupine warm-blooded quills yes no no four yes yes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 13, 'page_label': '8'}, page_content='cold-blooded scales yes yes no zero no no\\nturtle cold-blooded scales no semi no four no no\\npenguin warm-blooded feathers no semi no two no no\\nporcupine warm-blooded quills yes no no four yes yes\\neel cold-blooded scales no yes no zero no no\\nsalamander cold-blooded none no semi no four yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 8/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 14, 'page_label': '9'}, page_content='Phases of Classification Example datasets Closing\\nClassification – Another verterbrate dataset example\\nVerterbrate classification on\\n▶ Classes: mammal, non-mammal\\n▶ Feature space: Body Temperature, Gives Birth, Four-legged, Hibernates\\nfrom Tan, Steinbach & Kumar, Ch.4 (2006)\\nTraining Set (cf. Table 4.3, with modifications in two instances!)\\nName Body Temperature Gives\\nBirth\\nFour-\\nlegged\\nHiber-\\nnates\\nClass label\\nporcupine warm-blooded yes yes yes mammal\\ncat warm-blooded yes yes no mammal\\nbat warm-blooded yes no yes mammal\\nwhale warm-blooded yes no no mammal\\nsalamander cold-blooded no yes yes non-mammal\\nkomodo dragon cold-blooded no yes no non-mammal\\npython cold-blooded no no yes non-mammal\\nsalmon cold-blooded no no no non-mammal\\neagle warm-blooded no no no non-mammal\\nguppy cold-blooded yes no no non-mammal\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 9/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 15, 'page_label': '10'}, page_content='Phases of Classification Example datasets Closing\\nClassification – Another verterbrate dataset example\\nVerterbrate classification on\\n▶ Classes: mammal, non-mammal\\n▶ Feature space: Body Temperature, Gives Birth, Four-legged, Hibernates\\nfrom Tan, Steinbach & Kumar, Ch.4 (2006)\\nTest Set (cf. Table 4.4 – one instance modified!)\\nName Body temperature Gives\\nBirth\\nfour\\nlegged\\nHiber-\\nnates\\nClass label\\nhuman warm-blooded yes no no mammal\\npigeon warm-blooded no no no non-mammal\\nelephant warm-blooded yes no no mammal\\nleopard shark cold-blooded yes no no non-mammal\\nturtle cold-blooded no no no non-mammal\\npenguin warm-blooded no no no non-mammal\\neel cold-blooded no no no non-mammal\\ndolphin warm-blooded yes no no mammal\\nspiny anteater warm-blooded no yes yes mammal\\ngila monster cold-blooded no yes yes non-mammal\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 10/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 16, 'page_label': '11'}, page_content='Phases of Classification Example datasets Closing\\nClassification Example Dataset 2 – Weather for playing golf\\nPublic domain dataset ”Golf” – Witten & Eibe, Book on Mining with Java\\n▶ Classes: Y es, No\\n▶ Feature space: Outlook, Temperature 1, Humidity, Windy\\nGolf dataset as running example\\nOutlook Temperature Humidity Windy Play\\nSunny Hot High False No\\nSunny Hot High True No\\nOvercast Hot High False Y es\\nRainy Mild High False Y es\\nRainy Cool Normal False Y es\\nRainy Cool Normal True No\\nOvercast Cool Normal True Y es\\nSunny Mild High False No\\nSunny Cool Normal False Y es\\nRainy Mild Normal False Y es\\nSunny Mild Normal True Y es\\nOvercast Mild High True Y es\\nOvercast Hot Normal False Y es\\nRainy Mild High True No\\n1also: ”Temp”\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 11/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 17, 'page_label': '12'}, page_content='Phases of Classification Example datasets Closing\\n. . . and the fictive dataset on patient responses to treatment\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes better no r no yes\\n#2 m M no better no b yes no\\n#3 m M no worse no b no no\\n#4 f VH yes worse no b no yes\\n#5 m L no no effect no l no no\\n#6 m M no better no l no no\\n#7 f VH yes better yes l yes yes\\n#8 f H no better yes r no no\\n#9 f H yes better no l no yes\\n#10 m M yes worse no b no no\\n#11 m M no no effect no l no no\\n#12 f H no no effect no r no no\\n#13 f L yes better no l yes yes\\n#14 m M no worse no b no no\\n#15 m L no no effect no l yes no\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 12/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 18, 'page_label': '13'}, page_content='Phases of Classification Example datasets Closing\\n1 Phases of Classification\\n2 Example datasets\\n3 Closing\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 13/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 19, 'page_label': '14'}, page_content='Phases of Classification Example datasets Closing\\nPhases, Data and . . .\\n▶ How to build a classifier? → Classification algorithms\\n▶ How to assess the quality of a classifier? → Model evaluation\\n▶ After building many classifiers:\\nHow to figure out which one is best? → Model comparison\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 14/15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2023-04-09T00:29:55+02:00', 'author': 'Myra Spiliopoulou', 'title': \"DM I: Block 'Classification' - Unit 'Underpinnings'\", 'subject': '', 'keywords': '', 'moddate': '2023-04-09T00:29:55+02:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_Underpinnings.pdf', 'total_pages': 21, 'page': 20, 'page_label': '15'}, page_content='Phases of Classification Example datasets Closing\\nThank you very much! Questions?\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Underpinnings’ · · · 15/15'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 0, 'page_label': '1'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nData Mining Block “Clustering”\\nMyra Spiliopoulou 1\\n1 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 1, 'page_label': '2'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\n1 Introduction\\n2 K-Means family\\n3 Similarity functions\\n4 Hierarchical clustering\\n5 Density-based clustering\\n6 Evaluation in Clustering\\n2 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 2, 'page_label': '3'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering\\nis a family of mining algorithms,\\ndesigned to help you organize your data into groups of similar objects\\n3 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 3, 'page_label': '4'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering\\nis a family of mining algorithms,\\ndesigned to help you organize your data into groups of similar objects\\nA clustering algorithm groups data points in such a way that the objects\\ninside each cluster are more similar to each other than the objects outside\\nthe cluster.\\n3 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 4, 'page_label': '5'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering\\nis a family of mining algorithms,\\ndesigned to help you organize your data into groups of similar objects\\nA clustering algorithm groups data points in such a way that the objects\\ninside each cluster are more similar to each other than the objects outside\\nthe cluster.\\nFor some clustering algorithms, this corresponds to\\n� minimizing the intra-cluster distance\\n� maximizing the inter-cluster distance\\n3 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 5, 'page_label': '6'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDifferent types of clusters T an, Steinbach & Kumar, Ch.8 (2006)\\n4 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 6, 'page_label': '7'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nGoals of the Block\\n� Make yourself familiar with instruments that you can use to cluster\\ndata\\n� Learn how to choose the right instrument for your data\\n� Learn to recognize bad clusters\\n5 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 7, 'page_label': '8'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n� K-Means\\n� Hierarchical clustering\\n� Density-based clustering: DBSCAN\\n6 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 8, 'page_label': '9'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means\\nK-means is good for a quick-and-dirty attempt toﬁnd groups in the data.\\n7 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 9, 'page_label': '10'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means\\nINPUT: a set of data points D in feature space F; the number of clusters K\\nK-Means algorithm\\n1. Select K initial centroids\\n2. Repeat\\n· Assign each point to the centroid closest to it\\n· Recompute the positions of the K centroids\\nuntil the positions of the centroids do not change anymore.\\n8 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 10, 'page_label': '11'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means\\nINPUT: a set of data points D in feature space F; the number of clusters K\\nK-Means algorithm\\n1. Select K initial centroids\\n2. Repeat\\n· Assign each point to the centroid closest to it\\n· Recompute the positions of the K centroids\\nuntil the positions of the centroids do not change anymore.\\nOptimization criterion for K-Means\\nSSE=\\nK\\n∑\\ni=1\\n∑\\nx∈Ci\\ndist(x,c i)2\\nwhere ci is the centroid of cluster Ci with cij =\\n∑x∈Ci xj\\n|Ci | ,j=1. . .|F|\\n8 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 11, 'page_label': '12'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means\\nINPUT: a set of data points D in feature space F; the number of clusters K\\nK-Means algorithm\\n1. Select K initial centroids\\n2. Repeat\\n· Assign each point to the centroid closest to it\\n· Recompute the positions of the K centroids\\nuntil the positions of the centroids do not change anymore.\\nOptimization criterion for K-Means\\nSSE=\\nK\\n∑\\ni=1\\n∑\\nx∈Ci\\ndist(x,c i)2\\nwhere ci is the centroid of cluster Ci with cij =\\n∑x∈Ci xj\\n|Ci | ,j=1. . .|F|\\nComplexity of K-Means\\nO(K· |F| · |D| ·I) for feature space F , dataset D and number of iterations I\\n8 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 12, 'page_label': '13'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means - Example\\nBuilding K=3 clusters T an, Steinbach & Kumar, Ch.8 (2006)\\n9 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 13, 'page_label': '14'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nExample: same data, different initialization of centroids\\nBuilding K=3 clusters T an, Steinbach & Kumar, Ch.8 (2006)\\n10 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 14, 'page_label': '15'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe K in K-Means\\nHow to determine K ?\\nHow to choose ”good” initial positions for the K centroids?\\n11 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 15, 'page_label': '16'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDisadvantages of K-Means\\n� K-Means favours spherical clusters\\n� K-Means favours clusters of the same size\\n� K-Means favours clusters of the same density\\n� K-Means is very sensitive to outlier objects\\n12 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 16, 'page_label': '17'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means variant: Bisecting K-Means\\nBisecting K-Means algorithm\\nStarting with a single root cluster that contains all objects in dataset D\\nREPEA T\\n1. Apply 2-Means on the cluster\\n2. Choose the most inhomogenenous of the leaf clusters\\n3. GOTO 1\\nUNTIL K leaf clusters have been built.\\n13 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 17, 'page_label': '18'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nK-Means variant: Bisecting K-Means\\nBisecting K-Means algorithm\\nStarting with a single root cluster that contains all objects in dataset D\\nREPEA T\\n1. Apply 2-Means on the cluster\\n2. Choose the most inhomogenenous of the leaf clusters\\n3. GOTO 1\\nUNTIL K leaf clusters have been built.\\nWhat advantage(s) brings Bisecting K-Means over K-Means?\\n13 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 18, 'page_label': '19'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n� Hierarchical clustering\\n� Density-based clustering: DBSCAN\\n14 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 19, 'page_label': '20'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n� Hierarchical clustering\\n� Density-based clustering: DBSCAN\\n� A small excursion on similarity functions and matrices\\n14 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 20, 'page_label': '21'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity functions\\nProperties of a similarity function s()\\nFor any two data points x,y∈D , it holds that:\\n� s(x,y)≤1 and s(x,y) =1↔x=y\\n� s(x,y) =s(y,x)\\n15 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 21, 'page_label': '22'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity functions\\nProperties of a similarity function s()\\nFor any two data points x,y∈D , it holds that:\\n� s(x,y)≤1 and s(x,y) =1↔x=y\\n� s(x,y) =s(y,x)\\nProperties of a distance function d()\\nFor any two data points x,y∈D , it holds that:\\n� d(x,y)≥0 and d(x,y) =0↔x=y\\n� d(x,y) =d(y,x)\\n� For each z∈D:d(x,z)≤d(x,y) +d(y,z)\\n15 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 22, 'page_label': '23'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity functions\\nProperties of a similarity function s()\\nFor any two data points x,y∈D , it holds that:\\n� s(x,y)≤1 and s(x,y) =1↔x=y\\n� s(x,y) =s(y,x)\\nProperties of a distance function d()\\nFor any two data points x,y∈D , it holds that:\\n� d(x,y)≥0 and d(x,y) =0↔x=y\\n� d(x,y) =d(y,x)\\n� For each z∈D:d(x,z)≤d(x,y) +d(y,z)\\nVery often, we use the complement of a distance function as a similarity\\nfunction.\\n15 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 23, 'page_label': '24'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity and Distance functions\\nGiven are two data points x,y∈D over an n-dimensional feature space F.\\n� f1(x,y) =\\n�\\n∑n\\ni=1(xi −y i)2\\n� f2(x,y) = ∑n\\ni=1 |xi −y i|\\n� f3(x,y) = ∑n\\ni=1 xiyi√\\n∑n\\ni=1 x2\\ni\\n√\\n∑n\\ni=1 y2\\ni\\n16 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 24, 'page_label': '25'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 25, 'page_label': '26'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 26, 'page_label': '27'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n� disagree10(x,y) = ∑n\\ni=1 xi(1−y i)\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 27, 'page_label': '28'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n� disagree10(x,y) = ∑n\\ni=1 xi(1−y i)\\n� disagree01(x,y) = ∑n\\ni=1(1−x i)yi\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 28, 'page_label': '29'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n� disagree10(x,y) = ∑n\\ni=1 xi(1−y i)\\n� disagree01(x,y) = ∑n\\ni=1(1−x i)yi\\nRandIndex(x,y) = agree11(x,y) +agree 00(x,y)\\nagree11(x,y) +agree 00(x,y) +disagree 10(x,y) +disagree 01(x,y)\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 29, 'page_label': '30'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nCounting agreements and disagreements\\nGiven are two data points x,y∈D over an n-dimensional feature space F,\\nso that the valuerange of each dimension A∈F is the set {0,1} .\\n� agree11(x,y) = ∑n\\ni=1 xiyi\\n� agree00(x,y) = ∑n\\ni=1(1−x i)(1−y i)\\n� disagree10(x,y) = ∑n\\ni=1 xi(1−y i)\\n� disagree01(x,y) = ∑n\\ni=1(1−x i)yi\\nRandIndex(x,y) = agree11(x,y) +agree 00(x,y)\\nagree11(x,y) +agree 00(x,y) +disagree 10(x,y) +disagree 01(x,y)\\nJaccardCoefﬁcient(x,y) = agree11(x,y)\\nagree11(x,y) +disagree 10(x,y) +disagree 01(x,y)\\n17 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 30, 'page_label': '31'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nSimilarity Matrix or Distance Matrix?\\nExample from T an, Steinbach & Kumar, Ch.8 (2006)\\n18 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 31, 'page_label': '32'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n√\\nSimilarity functions\\n� Hierarchical clustering\\n� Density-based clustering: DBSCAN\\n19 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 32, 'page_label': '33'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nHierarchical clustering\\nThis is a family of methods that processes your data to return atree of\\nclusters.\\nY ou run a horizontal cut on this tree to get a set of clusters.\\n20 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 33, 'page_label': '34'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nHierarchical clustering\\nThis is a family of methods that processes your data to return atree of\\nclusters.\\nY ou run a horizontal cut on this tree to get a set of clusters.\\nY ou use algorithms of this family because they are more robust, and you\\ncan easily switch one against the other, toﬁt the properties of your data\\nbetter.\\n20 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 34, 'page_label': '35'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nHierarchical clustering\\nThis is a family of methods that processes your data to return atree of\\nclusters- a dendrogramm.\\nExample of a dendrogramm T an, Steinbach & Kumar, Ch.8 (2006)\\nY ou run a horizontal cut on this tree to get a set of clusters.\\n21 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 35, 'page_label': '36'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nBottom-up and top-down hierarchical clustering algorithms\\nGiven is a set of data points D.\\nBottom-up: Agglomerative clustering T an, Steinbach & Kumar, Ch.8 (2006)\\nStarting with the bottom layer, where each data point is a cluster of its own:\\n22 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 36, 'page_label': '37'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nBottom-up and top-down hierarchical clustering algorithms\\nGiven is a set of data points D.\\nBottom-up: Agglomerative clustering T an, Steinbach & Kumar, Ch.8 (2006)\\nStarting with the bottom layer, where each data point is a cluster of its own:\\nT op-down: Divisive clustering T an, Steinbach & Kumar, Ch.8 (2006)\\nStarting with a single cluster that contains all data points:\\nrepeat\\nSplit one cluster into two\\nuntilk clusters remain\\nwhere k may be a user-deﬁned number or k =|D| .\\n22 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 37, 'page_label': '38'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms\\nClustering a small dataset T an, Steinbach & Kumar, Ch.8 (2006)\\nWhich data points constitute theﬁrst cluster? 23 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 38, 'page_label': '39'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms\\nAgglomerative clustering algorithms differ in their deﬁnition of ”proximity\\nbetween two clusters”.\\n� MIN - single link\\n� MAX - complete link\\n� Group average\\n� Distance between centroids\\n� Ward’s method - squared error\\nComputing proximity/distance T an, Steinbach & Kumar, Ch.8 (2006)\\n24 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 39, 'page_label': '40'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms:MIN\\nDistance between clusters X and Y\\nfor each x∈X\\nfor each y∈Y\\ncompute dist(x,y)\\nsort the distance values into a list LX,Y\\n25 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 40, 'page_label': '41'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms:MIN\\nDistance between clusters X and Y\\nfor each x∈X\\nfor each y∈Y\\ncompute dist(x,y)\\nsort the distance values into a list LX,Y\\nMIN – single link\\nThe distance between X,Y is the minimum value in LX,Y .\\nd(X,Y) =minL X,Y\\n25 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 41, 'page_label': '42'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: MIN\\nApplying MIN on the dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n26 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 42, 'page_label': '43'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: MAX\\nDistance between clusters X and Y\\nfor each x∈X\\nfor each y∈Y\\ncompute dist(x,y)\\nsort the distance values into a list LX,Y\\nMAX – complete link\\nThe distance between X,Y is the maximum value in LX,Y :\\nd(X,Y) =maxL X,Y\\n27 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 43, 'page_label': '44'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: MAX\\nApplying MAX on the dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n28 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 44, 'page_label': '45'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Group Average\\nDistance between clusters X and Y\\nfor each x∈X\\nfor each y∈Y\\ncompute dist(x,y)\\nsort the distance values into a list LX,Y\\nGroup Average\\nThe distance between X,Y is the average value in LX,Y , i.e.\\nd(X,Y) = ∑x∈X (∑y∈Y dist(x,y))\\n|X| · |Y|\\n29 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 45, 'page_label': '46'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Group Average\\nApplying Group Average on the example dataset T an, Steinbach & Kumar,\\nCh.8 (2006)\\n30 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 46, 'page_label': '47'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Ward’s method\\nWard’s method\\nThe distance between two clusters is the increase in the sum of squared\\nerrors that incurs when merging them.\\nApplying Ward’s method on the example dataset T an, Steinbach & Kumar,\\nCh.8 (2006)\\n31 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 47, 'page_label': '48'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Juxtaposing the methods\\nAdvantages of MIN T an, Steinbach & Kumar, Ch.8 (2006)\\nMIN can discover non-spherical clusters\\nSee also Fig. 8.2(c), contiguity-based clusters.\\nDisadvantages of MIN T an, Steinbach & Kumar, Ch.8 (2006)\\nMIN is sensitive to outliers and noise.\\n32 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 48, 'page_label': '49'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms: Juxtaposing the methods\\nAdvantages of MAX T an, Steinbach & Kumar, Ch.8 (2006)\\nMAX is not inﬂuenced by outliers and noise.\\nDisadvantages of MAX T an, Steinbach & Kumar, Ch.8 (2006)\\nMAX tends to split large clusters and favours spherical clusters.\\n33 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 49, 'page_label': '50'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nAgglomerative clustering algorithms\\nAdvantages:\\n+ Number of clusters needs not be speciﬁed.\\n+ Simple statistics and visualizations help deciding the cut position.\\nDisadvantages\\n− Complexity is O(N3) (N=|D| ) (for some variants: O(N2 logN) ).\\n− The order of merging affects the quality of subsequent iterations.\\nDealing with outliers– quoting from the 2019 edition, p. 564-565:\\nOutliers pose the most serious problems for Ward’s method and\\ncentroid-based hierarchical clustering approaches because they\\nincrease SSE and distort centroids. . . . As hierarchical clustering\\nproceeds for these algorithms, outliers or small groups of outliers tend\\nto form singleton or small clusters that do not merge with any other\\nclusters until much later in the merging process. By discarding\\nsingleton or small clusters that are not merging with other clusters,\\noutliers can be removed.'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 49, 'page_label': '50'}, page_content='clusters until much later in the merging process. By discarding\\nsingleton or small clusters that are not merging with other clusters,\\noutliers can be removed.\\n34 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 50, 'page_label': '51'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n√\\nSimilarity functions\\n√\\nHierarchical clustering\\n� Density-based clustering: DBSCAN\\n35 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 51, 'page_label': '52'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering - DBSCAN\\nThis is a family of methods that deﬁnes a cluster as a set of overlapping\\nneighbourhoods.\\nDBSCAN 1 is the family progenitor.\\n1M. Ester, H.-P . Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering\\nclusters in large spatial databases with noise.In Proc. of the 2nd Int. Conf. on Knowledge\\nDiscovery and Data Mining (KDD’96), pages 226–231, 1996.\\n36 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 52, 'page_label': '53'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering - DBSCAN\\nThis is a family of methods that deﬁnes a cluster as a set of overlapping\\nneighbourhoods.\\nDBSCAN 1 is the family progenitor.\\nY ou use algorithms of this family when you know that your data have some\\ndense areas surrounded by noise.\\n1M. Ester, H.-P . Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering\\nclusters in large spatial databases with noise.In Proc. of the 2nd Int. Conf. on Knowledge\\nDiscovery and Data Mining (KDD’96), pages 226–231, 1996.\\n36 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 53, 'page_label': '54'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering - DBSCAN\\nThis is a family of methods that deﬁnes a cluster as a set of overlapping\\nneighbourhoods.\\nDBSCAN 1 is the family progenitor.\\nY ou use algorithms of this family when you know that your data have some\\ndense areas surrounded by noise.\\nThese algorithms are best for geographical data, but there are further\\napplication areas.\\n1M. Ester, H.-P . Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering\\nclusters in large spatial databases with noise.In Proc. of the 2nd Int. Conf. on Knowledge\\nDiscovery and Data Mining (KDD’96), pages 226–231, 1996.\\n36 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 54, 'page_label': '55'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering with DBSCAN\\nThe core concept of DBSCAN is that of a neighbourhood around a data\\npoint.\\nDensely populated neighbourhoods form the basis for a cluster.\\nA cluster is a maximal set of overlapping, densely populated\\nneighbourhoods.\\n37 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 55, 'page_label': '56'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDensity-based clustering with DBSCAN\\nThe core concept of DBSCAN is that of a neighbourhood around a data\\npoint.\\nDensely populated neighbourhoods form the basis for a cluster.\\nA cluster is a maximal set of overlapping, densely populated\\nneighbourhoods.\\nDBSCAN does not place all data points to clusters. Around each cluster\\nthere are sparsely populated areas, which are not part of any cluster.\\n37 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 56, 'page_label': '57'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nDBSCAN basic concepts T an, Steinbach & Kumar, Ch.8 (2006)\\nA point x is a core point, given eps and minPts, iff there are at least minPts\\npoints within radius eps around x. A point y is a border point if there is a\\ncore point x, such that dist(x,y)≤eps . All other points are noise points.\\n38 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 57, 'page_label': '58'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nDBSCAN parameters\\n� eps: radius of the hypersphere a around a data point, i.e. the\\nneighbourhoodof the data point\\n� minPts: neighbourhood size – can be deﬁned as\\n· number of neighbours of the data point in the center of the\\nhypersphere\\n· number of data points inside the hypersphere\\naThe hypersphere is deﬁned in the feature space F.\\n39 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 58, 'page_label': '59'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofreachabilityin DBSCAN\\nGiven are two data points x,y in the dataset D over feature space F, and\\nﬁxed parameter values for eps,minPts :\\nDirectly density-reachable data point\\ny isdirectly-density reachablefrom x iff\\n� x is a core point AND\\n� y is in the neighbourhood of x\\n40 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 59, 'page_label': '60'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofreachabilityin DBSCAN\\nGiven are two data points x,y in the dataset D over feature space F, and\\nﬁxed parameter values for eps,minPts :\\nDirectly density-reachable data point\\ny isdirectly-density reachablefrom x iff\\n� x is a core point AND\\n� y is in the neighbourhood of x\\nDensity-reachable data point\\ny isdensity reachablefrom x iff there are x1, . . . ,xn ∈D such that:\\n� x=x 1 and y=x n AND\\n� for each i=2, . . . ,n it holds that xi is directly density-reachable from xi−1\\n40 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 60, 'page_label': '61'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofreachabilityin DBSCAN\\nGiven are two data points x,y in the dataset D over feature space F, and\\nﬁxed parameter values for eps,minPts :\\nDirectly density-reachable data point\\ny isdirectly-density reachablefrom x iff\\n� x is a core point AND\\n� y is in the neighbourhood of x\\nDensity-reachable data point\\ny isdensity reachablefrom x iff there are x1, . . . ,xn ∈D such that:\\n� x=x 1 and y=x n AND\\n� for each i=2, . . . ,n it holds that xi is directly density-reachable from xi−1\\nDensity-connected data points\\nx,y are density-connected iff there is a data point z∈D such that:\\n� x is density-reachable from z AND\\n� y is density-reachable from z\\n40 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 61, 'page_label': '62'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofclusterin DBSCAN\\nGiven is a dataset D over feature space F, andﬁxed parameter values for\\neps,minPts .\\nCluster in DBSCAN\\nA cluster C⊆D is a non-empty subset of D that has following properties:\\n� Connectivity:for each x,y∈C it holds that x,y are\\ndensity-connected.\\n� Maximality:for each x,y∈D such that x∈C and x,y are\\ndensity-connected, it holds that y∈C .\\n41 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 62, 'page_label': '63'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThe concept ofclusterin DBSCAN\\nGiven is a dataset D over feature space F, andﬁxed parameter values for\\neps,minPts .\\nCluster in DBSCAN\\nA cluster C⊆D is a non-empty subset of D that has following properties:\\n� Connectivity:for each x,y∈C it holds that x,y are\\ndensity-connected.\\n� Maximality:for each x,y∈D such that x∈C and x,y are\\ndensity-connected, it holds that y∈C .\\nThe noise areas\\nLet C1, . . . ,Ck be all the clusters found by DBSCAN for D.\\nnoise(D) =D\\\\∪ k\\ni=1Ci\\n41 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 63, 'page_label': '64'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nDBSCAN - simpliﬁed T an, Steinbach & Kumar, Ch.8 (2006)\\n42 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 64, 'page_label': '65'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nExample (1 of 3): the dataset\\n43 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 65, 'page_label': '66'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nExample (2 of 3) for eps=1 , minPts=5 : core points and\\nneighbourhoods\\n44 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 66, 'page_label': '67'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nExample (3 of 3) for eps=1 , minPts=5 : clusters and noise areas 2\\n2Beware of a mistake: the blue cluster has two core points, not one.\\n45 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 67, 'page_label': '68'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nAdvantages of DBSCAN\\n� DBSCAN canﬁnd clusters of arbitrary geometrical form\\n� DBSCAN is insensitive to noise\\n46 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 68, 'page_label': '69'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN\\nAdvantages of DBSCAN\\n� DBSCAN canﬁnd clusters of arbitrary geometrical form\\n� DBSCAN is insensitive to noise\\nDisadvantages of DBSCAN\\n� DBSCAN runs into difﬁculties when there are dense areas with\\ndifferent density\\n46 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 69, 'page_label': '70'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN: Disadvantage\\nExample dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n47 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 70, 'page_label': '71'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nDBSCAN: Disadvantage\\nExample dataset T an, Steinbach & Kumar, Ch.8 (2006)\\nT wo runs forMinPts=4 T an, Steinbach & Kumar, Ch.8 (2006)\\neps=9.75\\n eps=9.92 47 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 71, 'page_label': '72'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n√\\nSimilarity functions\\n√\\nHierarchical clustering\\n√\\nDensity-based clustering: DBSCAN\\n48 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 72, 'page_label': '73'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClustering algorithms\\n√\\nK-Means\\n√\\nSimilarity functions\\n√\\nHierarchical clustering\\n√\\nDensity-based clustering: DBSCAN\\nHow toﬁnd out whether the clustering algorithm has built good clusters?\\n48 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 73, 'page_label': '74'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nWhat to evaluate when you do clustering?\\n49 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 74, 'page_label': '75'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nWhat to evaluate when you do clustering?\\n� How good are the clusters?\\n� How good is the algorithm?\\n50 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 75, 'page_label': '76'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation in Clustering\\n� Evaluation of cluster quality 1 − →Internal Indices\\n� Evaluation of cluster quality 2 − →Models of randomness\\n� Evaluation of algorithm performance − →External Indices\\n51 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 76, 'page_label': '77'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\n� Sum of Squared Errors\\n� Cohesion and Separation\\n� Silhouette Coefﬁcient\\n52 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 77, 'page_label': '78'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Sum of Squared Errors\\nQuality on the basis of distance of data points to ”their” centroid\\n· ∀X∈ζ,x∈X:pointSSE(x) =d(x,center(X)) 2\\n53 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 78, 'page_label': '79'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Sum of Squared Errors\\nQuality on the basis of distance of data points to ”their” centroid\\n· ∀X∈ζ,x∈X:pointSSE(x) =d(x,center(X)) 2\\n· ∀X∈ζ:cluSSE(X) = 1\\n|X| ∑x∈X pointSSE(x)\\n53 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 79, 'page_label': '80'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Sum of Squared Errors\\nQuality on the basis of distance of data points to ”their” centroid\\n· ∀X∈ζ,x∈X:pointSSE(x) =d(x,center(X)) 2\\n· ∀X∈ζ:cluSSE(X) = 1\\n|X| ∑x∈X pointSSE(x)\\n· SSE(ζ) = 1\\n|ζ| ∑X∈ζ cluSSE(X) = 1\\n|ζ| ∑X∈ζ\\n�\\n1\\n|X| ∑x∈X d(x,center(X)) 2\\n�\\n53 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 80, 'page_label': '81'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Cohesion and Separation\\nQuality as in-cluster homogeneity and between-clusters gap\\n1. Centroid-based indices: ∀X∈ζ\\n· cohesion(X) =1− 1\\n|X| ∑x∈X d(x,center(X))\\n· separation(X) = 1\\n|ζ|−1 ∑Y∈ζ,Y�=X d(center(X),center(Y))\\nwhere values closer to 1 are better.\\nExample from T an, Steinbach & Kumar, Ch.8 (2006)\\n54 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 81, 'page_label': '82'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nInternal indices of cluster quality\\nFor a clustering ζ over a set D, computed with a distance function d() (the\\ndistances are normalized):\\n� Cohesion and Separation\\nQuality as in-cluster homogeneity and between-clusters gap\\n2. Graph-based indices: ∀X∈ζ\\n· cohesion(X) =1− 1\\n|X|(|X|−1) ∑x,y∈X;x�=y d(x,y)\\n· separation(X) = 1\\n|ζ|−1 ∑Y∈ζ\\\\X\\n1\\n|X|·|Y|\\n�\\n∑x∈X,y∈Y d(x,y)\\n�\\nwhere values closer to 1 are better.\\nExample from T an, Steinbach & Kumar, Ch.8 (2006)\\n55 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 82, 'page_label': '83'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality\\nInternal indices of cluster quality:\\n√\\nSum of Square Errors\\n√\\nCohesion and Separation: center-based and graph-based\\n√\\nSilhouette Coefﬁcient\\nand a visualization aid: Similarity matrix sorted on clusterID\\n57 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 83, 'page_label': '84'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality\\nInternal indices of cluster quality:√\\nSum of Square Errors√\\nCohesion and Separation: center-based and graph-based√\\nSilhouette Coefﬁcient\\nand a visualization aid: Similarity matrix sorted on clusterID\\nA clustered dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n58 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 84, 'page_label': '85'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality and the problem of random data\\nAnother clustered dataset T an, Steinbach & Kumar, Ch.8 (2006)\\n59 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 85, 'page_label': '86'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality and the problem of random data\\nExample continued T an, Steinbach & Kumar, Ch.8 (2006)\\n60 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 86, 'page_label': '87'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality and the problem of random data\\nThe data we cluster may actually have NO clustering structure.\\nA clustering algorithm run on those data will in any case return some\\nclusters.\\n? When is the value of the internal index good enough ?\\n? How to recognize whether the boxes in the similarity matrix are\\nindeed clusters ?\\n61 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 87, 'page_label': '88'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation in Clustering\\n√\\nEvaluation of cluster quality 1 − →Internal Indices\\n� Evaluation of cluster quality 2 − →Models of randomness\\n62 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 88, 'page_label': '89'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality with help ofModels of Randomness\\nModels of Randomness - Approach 1 from T an, Steinbach & Kumar, Ch.8\\n(2006)\\nGiven is a dataset D with feature space F, and a set of clusters ξ learned\\nwith algorithm A .\\nFOR i=1. . .N\\n� generate a random dataset Di, so that |Di|=|D| and Fi =F\\n� derive a model ζi for Di using A with the same parameter settings as\\nused for ζ .\\n� compute the quality of ζi with an internal index q()\\nDO compute the histogramm of the quality values of the N models\\nDO compare q(ξ) with the values on the histogramm\\nDO DISCARD ξ if it is in the wrong area of the plot ???\\n63 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 89, 'page_label': '90'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality with help ofModels of Randomness\\nApproach 1 - Example from T an, Steinbach & Kumar, Ch.8 (2006)\\nWhen is the SSE value of the model ξ good? 64 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 90, 'page_label': '91'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation of cluster quality with help ofModels of Randomness\\nModels of Randomness - Approach 2 Uli Niemann (Master thesis) 2\\nGiven is a dataset D with feature space F, and a set of clusters ξ learned\\nwith algorithm A .\\nA model-of-randomness over D,F , i.e. MofR(D,F) is a set of clusters ζ ,\\nsuch that |ζ|=|ξ| and the assignment of points to clusters in ζ is\\nrandom.\\nFOR i=1. . .N , generate a MofR(D,F) , ζi\\nDO compute the histogramm of the quality values of the N models\\nDO compare q(ξ) with the values on the histogramm\\nDO DISCARD ξ if it is in the wrong area of the plot\\n3\\n3Niemann, U., Spiliopoulou, M., V ¨olzke, H., and K ¨uhn, J.-P . (2014). Subpopulation\\nDiscovery in Epidemiological Data with Subspace Clustering.Foundations of Computing\\nand Decision Sciences(FCDS),39(4):271–300. 65 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 91, 'page_label': '92'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nEvaluation in Clustering\\n√\\nEvaluation of cluster quality 1 − →Internal Indices\\n√\\nEvaluation of cluster quality 2 − →Models of randomness\\n� Evaluation of algorithm performance − →External Indices\\n66 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 92, 'page_label': '93'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nExternal indices\\nT wo groups of external indices:\\nGroup 1: How well did the clustering algorithm guess the true classes of the\\ndata?\\n� Entropy\\n� Purity\\n� Precision & Recall BLOCK: Classiﬁcation\\n� F-measure BLOCK: Classiﬁcation\\nGroup 2: T o what extend do the clusters agree with the classes?√\\nRand Index√\\nJaccard Coefﬁcient\\nwhere higher values are better.\\n67 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 93, 'page_label': '94'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nExternal indices group 1 - class-oriented indices\\nGiven is the dataset D, the true assignment of the data points in D to\\nclasses ϕ={C 1, . . . ,CL} and a set of clusters ξ={X 1, . . . ,XK } over D.\\nEntropy\\nentropy(ξ,ϕ) =\\nK\\n∑\\ni=1\\n|Xi|\\n|D| clusEntropy(Xi,ϕ)\\nwhere clusEntropy(Xi,ϕ) =− ∑L\\nj=1 pij log2(pij) with pij = |Xi∩Cj|\\n|Xi| .\\n68 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 94, 'page_label': '95'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nExternal indices group 1 - class-oriented indices\\nGiven is the dataset D, the true assignment of the data points in D to\\nclasses ϕ={C 1, . . . ,CL} and a set of clusters ξ={X 1, . . . ,XK } over D.\\nEntropy\\nentropy(ξ,ϕ) =\\nK\\n∑\\ni=1\\n|Xi|\\n|D| clusEntropy(Xi,ϕ)\\nwhere clusEntropy(Xi,ϕ) =− ∑L\\nj=1 pij log2(pij) with pij = |Xi∩Cj|\\n|Xi| .\\nPurity\\npurity(ξ,ϕ) =\\nK\\n∑\\ni=1\\n|Xi|\\n|D| clusPurity(Xi,ϕ)\\nwhere clusPurity(Xi,ϕ) =max j=1...L{pij},\\ni.e. each cluster Xi is assigned to the class, to which most of its members\\nbelong.\\n68 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 95, 'page_label': '96'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClosing\\n√\\nBasic clustering algorithms: K-Means, DBSCAN, hierarchical\\nclustering methods\\n√\\nSimilarity/distance functions for clustering\\n√\\nEvaluation of internal cluster quality\\n√\\nEvaluation of algorithm performance towards a ground truth\\n69 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 96, 'page_label': '97'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nClosing\\nClustering encompasses much more:\\n� More basic algorithms like K-Medoids\\n� Algorithms that perceive the data as a graph: Graph clustering\\nalgorithms\\n� Algorithms that build clusters in subsets of the feature space:\\n� Subspace clustering\\n� Projected clustering\\n� Algorithms that exploit knowledge of the expert:\\n� Constraint-based clustering algorithms\\n� Semi-supervised clustering algorithms\\n� Algorithms that assign the data probabilistically to groups\\n� Probabilistic clustering algorithms\\n� Fuzzy clustering algorithms, like C-Means\\n� T opic models (generative models over the data space)\\n70 / 71'),\n",
       " Document(metadata={'producer': 'cairo 1.16.0 (https://cairographics.org)', 'creator': 'PyPDF', 'creationdate': '2024-06-28T12:54:43+02:00', 'source': '..\\\\data\\\\pdf\\\\2_Clustering_withSilhouetteSlide_removed.pdf', 'total_pages': 98, 'page': 97, 'page_label': '98'}, page_content='Introduction K-Means family Similarity functions Hierarchical clustering Density-based clustering Evaluation in Clustering\\nThank you very much!\\nQuestions?\\n71 / 71'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 0, 'page_label': '1'}, page_content='BLOCK Data Engineering - Unit 3 on Feature\\nSelection\\nMyra Spiliopoulou 1\\n1Faculty of Computer Science, Otto-von-Guericke University Magdeburg'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 1, 'page_label': '2'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nLiterature of the unit\\nBOOK: Salvador Garcia, Julian Luengo, Francisco Herrera (2015) Data\\nPreprocessing in Data Mining , SPRINGER International Publishing\\nSwitzerland\\n2 Chapter 3 ’Data Preparation Basic Models’\\n1 Chapter 4 ’Dealing with Missing Values’\\n3 Chapter 7 ’Feature Selection’\\nand some words on\\n3.2.1 Data Integration –’Finding Redundant Attributes’ → [Unit 3]\\n3.5 Data Transformation → [Unit 3]\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 2/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 2, 'page_label': '3'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 3/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 3, 'page_label': '4'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 4/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 4, 'page_label': '5'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nRECALL: Running example on patient response to treatment\\nId I2 I15 I30 I22 I31 I9 I26 Response\\n#1 f VH yes better no r no yes\\n#2 m M no better no b yes no\\n#3 m M no worse no b no no\\n#4 f VH yes worse no b no yes\\n#5 m L no no effect no l no no\\n#6 m M no better no l no no\\n#7 f ?? yes better yes l yes yes\\n#8 f H no better yes r no no\\n#9 f H yes better no l no yes\\n#10 m M yes worse no b no no\\n#11 m M no no effect no l no no\\n#12 f H no no effect no r no no\\n#13 ?? ?? yes ?? no l yes yes\\n#14 m M no worse no b no no\\n#15 m L no no effect no l yes no\\nWe use the data to train models and acquire insights. Therefore we must:√ deal with missing values,√ clean away the errors in the data,√ normalize the data, remove duplicates and\\n→ eliminate redundant variables, because\\nlearning algorithms do not cope well with high-dimensional spaces and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 4, 'page_label': '5'}, page_content='→ eliminate redundant variables, because\\nlearning algorithms do not cope well with high-dimensional spaces and\\nbecause the more variables we seek to record, the more likely it is that we will\\nhave missing values in them.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 5/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 5, 'page_label': '6'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe real counterpart of our tiny dataset [Schleicher et al., 2024]\\n• Original dataset from Charit ´e Universit ¨atsmedizin Berlin: patients with tinnitus\\nduration of at least 3 months, an age of at least 18 years and sufficient knowledge\\nof the German language, in the period January 2011 until October 2015, treated\\n[. . . ], exclusion criteria [. . . ] N=3971\\n• Further exclusion criteria: treatment finished, no intermediate visits, only one\\ntreatment sequence, no longer than 15 days N=1450\\n• Constraint: no missing values in the 9 questionnaires of the study N=1287\\n• Random sample of 500 patients: N=500 (f:240, m: 260)\\nTable 1: lists the questionnaires and the number of items in each one\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 6/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 6, 'page_label': '7'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 7/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 7, 'page_label': '8'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection: What and Why\\nDefinition of FSel [Section 7.1]\\n’Feature selection is a process that chooses the optimal subset of features\\naccording to a certain criterion.’\\nOptimality / Goodness ⇒ Purpose of the feature selection process a\\naFrom Section 4.1 and from citation [48]:Sayes Y ., Inza I, Larranaga P (2007) A review of\\nfeature selection techniques in bioinformatics. Bioinformatics 23(19), 2507-2517\\n⋆ discard redundant features\\n⋆ reduce the cost of data acquisition\\nand when applying FSel in preparation of an already known learning task:\\n◦ discard irrelevant features, and thus also irrelevant data\\n◦ increase the accuracy/quality of the learned models\\n◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 7, 'page_label': '8'}, page_content='◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 8/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 8, 'page_label': '8'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection: What and Why\\nDefinition of FSel [Section 7.1]\\n’Feature selection is a process that chooses the optimal subset of features\\naccording to a certain criterion.’\\nOptimality / Goodness ⇒ Purpose of the feature selection process a\\naFrom Section 4.1 and from citation [48]:Sayes Y ., Inza I, Larranaga P (2007) A review of\\nfeature selection techniques in bioinformatics. Bioinformatics 23(19), 2507-2517\\n⋆ discard redundant features\\n⋆ reduce the cost of data acquisition\\nand when applying FSel in preparation of an already known learning task:\\n◦ discard irrelevant features, and thus also irrelevant data\\n◦ increase the accuracy/quality of the learned models\\n◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 8, 'page_label': '8'}, page_content='◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 8/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 9, 'page_label': '8'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection: What and Why\\nDefinition of FSel [Section 7.1]\\n’Feature selection is a process that chooses the optimal subset of features\\naccording to a certain criterion.’\\nOptimality / Goodness ⇒ Purpose of the feature selection process a\\naFrom Section 4.1 and from citation [48]:Sayes Y ., Inza I, Larranaga P (2007) A review of\\nfeature selection techniques in bioinformatics. Bioinformatics 23(19), 2507-2517\\n⋆ discard redundant features\\n⋆ reduce the cost of data acquisition\\nand when applying FSel in preparation of an already known learning task:\\n◦ discard irrelevant features, and thus also irrelevant data\\n◦ increase the accuracy/quality of the learned models\\n◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 9, 'page_label': '8'}, page_content='◦ reduce the complexity of the model / of the model description\\n◦ improve efficiency of the learning process, e.g. by reducing storage\\nrequirements and computational costs\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 8/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 10, 'page_label': '9'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection Tasks in our Example\\nFrom Table 1 in [Schleicher et al., 2024]:\\nPos Name # Items Outcome\\nof interest\\n1 SOZK 25 -\\n2 TQ 52 YES\\n3 PSQ 30 YES\\n4 SF8 8 –\\n5 SWOP 9 –\\n6 TLQ 8 –\\n7 BSF 30 –\\n8 BI 56 –\\n9 ADSL 20 YES\\n238 YES: 102\\nFSel tasks with known outcomes of\\ninterest:\\n⋆ remove redundant features\\n⇒ reduce cost / burden\\n◦ remove irrelevant features\\n⇒ reduce cost / burden\\n◦ reduce model complexity\\n⇒ make it more actionable for DS\\nexcept that we cannot remove individual\\nfeatures, only complete questionnaires.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 9/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 11, 'page_label': '9'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFeature Selection Tasks in our Example\\nFrom Table 1 in [Schleicher et al., 2024]:\\nPos Name # Items Outcome\\nof interest\\n1 SOZK 25 -\\n2 TQ 52 YES\\n3 PSQ 30 YES\\n4 SF8 8 –\\n5 SWOP 9 –\\n6 TLQ 8 –\\n7 BSF 30 –\\n8 BI 56 –\\n9 ADSL 20 YES\\n238 YES: 102\\nFSel tasks with known outcomes of\\ninterest:\\n⋆ remove redundant features\\n⇒ reduce cost / burden\\n◦ remove irrelevant features\\n⇒ reduce cost / burden\\n◦ reduce model complexity\\n⇒ make it more actionable for DS\\nexcept that we cannot remove individual\\nfeatures, only complete questionnaires.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 9/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 12, 'page_label': '10'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest for the Good Features (Section 7.2.1.1, with modifications)\\nGiven a feature space F and a goodness measure U: Build the subset of the\\ngood features S from scratch or rather by discarding features from F ?\\nSequential forward and backward feature set generation algorithms\\nSFG Sequential forward\\nfeature set generation\\nfunction SFG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextBest(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S satisfies U or F = / 0\\nreturn S\\nend function\\nSBG Sequential backward\\nfeature set generation\\nfunction SBG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextWorst(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S does not satisfy U or F = / 0\\nreturn F ∪ {f } // add the last feature again\\nend function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 12, 'page_label': '10'}, page_content='end function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).\\nHence, each newly selected feature f is best/worst given the previous\\nselections.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 10/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 13, 'page_label': '10'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest for the Good Features (Section 7.2.1.1, with modifications)\\nGiven a feature space F and a goodness measure U: Build the subset of the\\ngood features S from scratch or rather by discarding features from F ?\\nSequential forward and backward feature set generation algorithms\\nSFG Sequential forward\\nfeature set generation\\nfunction SFG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextBest(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S satisfies U or F = / 0\\nreturn S\\nend function\\nSBG Sequential backward\\nfeature set generation\\nfunction SBG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextWorst(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S does not satisfy U or F = / 0\\nreturn F ∪ {f } // add the last feature again\\nend function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 13, 'page_label': '10'}, page_content='end function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).\\nHence, each newly selected feature f is best/worst given the previous\\nselections.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 10/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 14, 'page_label': '10'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest for the Good Features (Section 7.2.1.1, with modifications)\\nGiven a feature space F and a goodness measure U: Build the subset of the\\ngood features S from scratch or rather by discarding features from F ?\\nSequential forward and backward feature set generation algorithms\\nSFG Sequential forward\\nfeature set generation\\nfunction SFG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextBest(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S satisfies U or F = / 0\\nreturn S\\nend function\\nSBG Sequential backward\\nfeature set generation\\nfunction SBG(F, U)\\ninitialize S = / 0\\nrepeat\\nf = FindNextWorst(F, U)\\nS = S ∪ {f }\\nF = F \\\\ {f }\\nuntil S does not satisfy U or F = / 0\\nreturn F ∪ {f } // add the last feature again\\nend function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 14, 'page_label': '10'}, page_content='end function\\nwhere the FindNextBest() and FindNextWorst() functions operate on the\\nfeature set as it is modified in each iteration (rather than the original one).\\nHence, each newly selected feature f is best/worst given the previous\\nselections.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 10/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 15, 'page_label': '11'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest cntd (Algorithms 3 & 4, with modifications)\\nCombining SFG and SBG into: Bidirectional feature set generation algorithm\\nBG Bidirectional feature set generation\\nfunction BG(F, U)\\ninitialize Sg = / 0; Sb = / 0; Fg = F ; Fb = F\\nrepeat\\nfg = FindNextBest(Fg, U) ; Sg = Sg ∪ {fg} ; Fg = Fg \\\\ {fg}\\nfb = FindNextWorst(Fb, U) ; Sb = Sb ∪ {fb} ; Fb = Fb \\\\ {fb}\\nuntil (a)Sf satisfies U or Ff = / 0 or (b) Sb does not satisfy U or Fb = / 0\\nif (a) holds then return Sf else return Fb ∪ {fb} endif\\nend function\\nGenerating subsets of F at random:\\nRG Random feature set generation\\nfunction RG(F, U)\\ninitialize S = Sbest / 0; cbest = |F|\\nrepeat\\nS = RandGen(F) // pick features from F at random\\nif |S| ≤ cbest and S satisfies U then Sbest = S ; cbest = |S| endif\\nuntil some stopping criterion is satisfied\\nreturn Sbest\\nend function\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 11/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 16, 'page_label': '11'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThe Quest cntd (Algorithms 3 & 4, with modifications)\\nCombining SFG and SBG into: Bidirectional feature set generation algorithm\\nBG Bidirectional feature set generation\\nfunction BG(F, U)\\ninitialize Sg = / 0; Sb = / 0; Fg = F ; Fb = F\\nrepeat\\nfg = FindNextBest(Fg, U) ; Sg = Sg ∪ {fg} ; Fg = Fg \\\\ {fg}\\nfb = FindNextWorst(Fb, U) ; Sb = Sb ∪ {fb} ; Fb = Fb \\\\ {fb}\\nuntil (a)Sf satisfies U or Ff = / 0 or (b) Sb does not satisfy U or Fb = / 0\\nif (a) holds then return Sf else return Fb ∪ {fb} endif\\nend function\\nGenerating subsets of F at random:\\nRG Random feature set generation\\nfunction RG(F, U)\\ninitialize S = Sbest / 0; cbest = |F|\\nrepeat\\nS = RandGen(F) // pick features from F at random\\nif |S| ≤ cbest and S satisfies U then Sbest = S ; cbest = |S| endif\\nuntil some stopping criterion is satisfied\\nreturn Sbest\\nend functionMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 11/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 17, 'page_label': '12'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nHow long to search? (Section 7.2.1.2, with modifications)\\n▶ Exhaustive search:\\nGenerate all non-empty subsets of F, i.e. P(F) \\\\ {/ 0}, and choose the\\nbest one\\n· ok if F is small\\n· nok if F is large ⇒ set a threshold m as the minimum number of features to be\\nselected or removed\\n▶ Heuristic search:\\nTraverse the search space of solutions by adhering to some heuristic,\\nwhich also incorporates a termination criterion\\n▶ Non-deterministic search:\\nGenerate subsets of F at random (cf. Algorithm RG) without an explicit\\ntermination criterion ⇒ at each time point, RG() returns the best subset\\nfound thus far 1\\nTo which category does each of SFG, SBG and BG belong?\\n1In ’stream mining’, best effort algorithms of this kind are calledanytime algorithms.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 12/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 18, 'page_label': '12'}, page_content='Running\\nexample Feature Selection Process Goodness\\nCriteria Filter\\ns & Wrappers Closing\\nHo\\nw long to search? (Section 7.2.1.2, with modifications)\\n▶ Exhaustiv\\ne search:\\nGenerate all non-empty subsets of F, i.e. P(F) \\\\ {/ 0}, and choose the\\nbest one\\n· ok if F is small\\n· nok if F is large ⇒ set a threshold m as the minimum number of features to be\\nselected or removed\\n▶ Heuristic search:\\nTraverse the search space of solutions by adhering to some heuristic,\\nwhich also incorporates a termination criterion\\n▶ Non-deterministic search:\\nGenerate subsets of F at random (cf. Algorithm RG) without an explicit\\ntermination criterion ⇒ at each time point, RG() returns the best subset\\nfound thus far 1\\nT\\no which category does each of SFG, SBG and BG belong?\\n1In\\n’stream mining’, best effort algorithms of this kind are calledanytime algorithms.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 12/28\\nHeuristic search'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 19, 'page_label': '13'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen what feature selection means, and what reasons\\nare there to perform a feature selection process.\\nWe have seen some ways of building a ’good’ subset of the original set of\\nfeatures, assuming a criterion of ’goodness’, such as redundancy (negative\\ncriterion) or good predictive power towards a target (positive criterion)\\nY our turn:Y ou must be able to explain the four feature set construction\\nalgorithms, name their termination criteria (if any), and say whether each of\\nthem is exhaustive, heuristic or non-deterministic – and explain why.\\nWhat comes next: ’Goodness’ quantified\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 13/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 20, 'page_label': '13'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen what feature selection means, and what reasons\\nare there to perform a feature selection process.\\nWe have seen some ways of building a ’good’ subset of the original set of\\nfeatures, assuming a criterion of ’goodness’, such as redundancy (negative\\ncriterion) or good predictive power towards a target (positive criterion)\\nY our turn:Y ou must be able to explain the four feature set construction\\nalgorithms, name their termination criteria (if any), and say whether each of\\nthem is exhaustive, heuristic or non-deterministic – and explain why.\\nWhat comes next: ’Goodness’ quantified\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 13/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 21, 'page_label': '13'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen what feature selection means, and what reasons\\nare there to perform a feature selection process.\\nWe have seen some ways of building a ’good’ subset of the original set of\\nfeatures, assuming a criterion of ’goodness’, such as redundancy (negative\\ncriterion) or good predictive power towards a target (positive criterion)\\nY our turn:Y ou must be able to explain the four feature set construction\\nalgorithms, name their termination criteria (if any), and say whether each of\\nthem is exhaustive, heuristic or non-deterministic – and explain why.\\nWhat comes next: ’Goodness’ quantified\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 13/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 22, 'page_label': '14'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 14/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 23, 'page_label': '15'}, page_content='Running\\nexample Feature Selection Process Goodness\\nCriteria Filter\\ns & Wrappers Closing\\nGoodness\\nas ’redundancy’: Correlations between features (1a)\\nχ 2 f\\nor two categorical attributes (Section 3.2.1.1)\\nLet A, B be\\ntwo nominal (categorical) attributes with c, respectively r distinct\\nvalues, in a dataset with m instances. We compute\\nχ 2 =\\nc\\n∑\\ni=1\\nr\\n∑\\nj=1\\n(oij − eij)2\\neij\\nwhere oij is\\nthe observed frequency of the joint event (Ai, Bj), and\\neij = count(A=ai)×count(B=bj)\\nm is\\nthe expected frequency.\\nWe test against the hypothesis that A, B are independent (H 0)\\nwith\\n(c − 1) × (r − 1) degrees of freedom. a\\naT\\no perform a statistical test like χ 2 we use a table or a software that can provide the values\\nfor the likelihood of the independence hypothesis.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 15/28\\nhow often the feature are independent, and how often its is observed together\\nif both are dependent on each other we might discard any one of them.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 24, 'page_label': '16'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness as ’redundancy’: Correlations between features (1b)\\nPearson product moment coeff for two numerical attributes (Section 3.2.1.2)\\nA, B with means A, B, standard deviations σA, σB:\\nrA,B = ∑m\\ni=1(ai − A)(bi − B)\\nmσAσB\\n∈ [−1, +1]\\nwhere (ai, bi) the values of A, B at the ith instance in the dataset, i = 1 . . .m.\\nPearson correlation coeff (Section 7.2.2.3)\\nmeasures the degree of linear correlation between two variables X, Y with\\nmeasurements {xi} and {yi} and means x, y:\\nρ(X, Y) = ∑i()xi − x)(yi − y)p\\n( ∑i(xi − x)2 ∑i(yi − y)2)\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 16/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 25, 'page_label': '16'}, page_content='Running\\nexample Feature Selection Process Goodness\\nCriteria Filter\\ns & Wrappers Closing\\nGoodness\\nas ’redundancy’: Correlations between features (1b)\\nP\\nearson product moment coeff for two numerical attributes (Section 3.2.1.2)\\nA, B with\\nmeans A, B,\\nstandard deviations σA, σB:\\nrA,B = ∑m\\ni=1(ai − A)\\n(bi − B)\\nmσAσB\\n∈ [−1, +1]\\nwhere (ai, bi) the\\nvalues of A, B at the ith instance in the dataset, i = 1 . . .m.\\nP\\nearson correlation coeff (Section 7.2.2.3)\\nmeasures\\nthe degree of linear correlation between two variables X, Y with\\nmeasurements {xi} and {yi} and means x, y:\\nρ(X, Y)\\n= ∑i()xi − x)\\n(yi − y)p\\n( ∑i(xi − x)2 ∑i(yi − y)2)\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 16/28\\nvalue closer to one will decribe it the two feature values are strongly corellated or no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 26, 'page_label': '17'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness as ’redundancy’: Correlations between features (1c)\\nCovariance between two numerical attributes (Section 3.2.1.2)\\nCov(A, B) = E((A − A)(B − B)) = 1\\nm\\nm\\n∑\\ni=1\\n(ai − A)(bi − B)\\n· If A, B are independent, then E(A · B) = E(A) · E(B), and thus\\nCov(A, B) = 0\\n· If A, B vary similarly, then whenever A > A we also expect that\\nB > B, and thus the covariance will be positive.\\n· If A, B vary in opposite directions, then whenever A > A we\\nexpect that B < B, and thus the covariance will be negative.\\nPearson product moment coeff and the covariance matrix\\nrA,B = ∑m\\ni=1(ai−A)(bi−B)\\nm · 1\\nσAσB\\n= Cov(A,B)\\nσAσB\\n(Eq. 3.5)\\n’\\n 2\\n2Covariance trends.svg.png from Cmglee, CC BY -SA 4.0\\nhttps://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 17/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 27, 'page_label': '17'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness as ’redundancy’: Correlations between features (1c)\\nCovariance between two numerical attributes (Section 3.2.1.2)\\nCov(A, B) = E((A − A)(B − B)) = 1\\nm\\nm\\n∑\\ni=1\\n(ai − A)(bi − B)\\n· If A, B are independent, then E(A · B) = E(A) · E(B), and thus\\nCov(A, B) = 0\\n· If A, B vary similarly, then whenever A > A we also expect that\\nB > B, and thus the covariance will be positive.\\n· If A, B vary in opposite directions, then whenever A > A we\\nexpect that B < B, and thus the covariance will be negative.\\nPearson product moment coeff and the covariance matrix\\nrA,B = ∑m\\ni=1(ai−A)(bi−B)\\nm · 1\\nσAσB\\n= Cov(A,B)\\nσAσB\\n(Eq. 3.5)\\n’\\n 2\\n2Covariance trends.svg.png from Cmglee, CC BY -SA 4.0\\nhttps://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 17/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 28, 'page_label': '17'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness as ’redundancy’: Correlations between features (1c)\\nCovariance between two numerical attributes (Section 3.2.1.2)\\nCov(A, B) = E((A − A)(B − B)) = 1\\nm\\nm\\n∑\\ni=1\\n(ai − A)(bi − B)\\n· If A, B are independent, then E(A · B) = E(A) · E(B), and thus\\nCov(A, B) = 0\\n· If A, B vary similarly, then whenever A > A we also expect that\\nB > B, and thus the covariance will be positive.\\n· If A, B vary in opposite directions, then whenever A > A we\\nexpect that B < B, and thus the covariance will be negative.\\nPearson product moment coeff and the covariance matrix\\nrA,B = ∑m\\ni=1(ai−A)(bi−B)\\nm · 1\\nσAσB\\n= Cov(A,B)\\nσAσB\\n(Eq. 3.5)\\n’\\n 2\\n2Covariance trends.svg.png from Cmglee, CC BY -SA 4.0\\nhttps://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 17/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 29, 'page_label': '18'}, page_content='Running\\nexample Feature Selection Process Goodness\\nCriteria Filter\\ns & Wrappers Closing\\nGoodness\\nas ’redundancy’: Correlations between features (1d)\\nSpear\\nman correlation coeff\\nis\\nthe Pearson correlation coeff for the ranks of the\\nvariables, i.e.\\nrs = ρ(R(X), R(Y)) = Cov(R(X), R(Y))\\nσR(X)σR(Y)\\nwhere X, Y are\\nnumerical variables and R(X), R(Y) are their\\nranks.\\nSpear\\nman correlation coeff can be used for ordinal vari-\\nables, and, iff all ranks are distinct integers, then it holds that\\n[Fieller et al., 1957] : rs = 1 − 6 ∑m\\ni=1(xi−yi)2\\nm3−m\\n3\\n3Upper\\nfig: upload.wikimedia.org/wikipedia/commons/4/4e/Spearman_fig1.svg\\nLower fig: upload.wikimedia.org/wikipedia/commons/6/67/Spearman_fig3.svg\\nSkbkekas, CC BY -SA 3.0https://creativecommons.org/licenses/by-sa/3.0, via\\nWikimedia Commons\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 18/28\\nfor categorical data- usage of chi square is preffered\\nbelow one depends on numerical data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 30, 'page_label': '19'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nRECALL [Unit 2] Example of a correlation analysis [Niemann et al., 2020] 4\\nCorrelation analysis (with Spearman correlation coefficient) on a dataset of 4,117\\ntinnitus patients who had been treated at the Tinnitus Center of Charit´e\\nUniversitaetsmedizin Berlin between January 2011 and October 2015.\\nFigure 3\\nFeature-feature correlation & feature correlation\\nwith respect to TQ tinnitus-related distress score\\nin T0 and T1.\\n(A) Correlation heatmap for all pairs of features\\n(T0). Features are ordered by agglomerative\\nhierarchical clustering with complete linkage. (B)\\nCorrelation of each feature with TQ\\ntinnitus-related distress score, in T0 (x-axis) and\\nin T1 (y-axis). The diamond symbol represents a\\nquestionnaire’s median. (C) Top-20 features with\\nhighest correlation to TQ tinnitus-related distress\\nscore (T0). (D) Top-20 features with highest\\ncorrelation to TQ tinnitus-related distress score'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 30, 'page_label': '19'}, page_content='questionnaire’s median. (C) Top-20 features with\\nhighest correlation to TQ tinnitus-related distress\\nscore (T0). (D) Top-20 features with highest\\ncorrelation to TQ tinnitus-related distress score\\n(T1). (E) Top-10 features whose correlational\\neffects with TQ tinnitus-related distress score\\ndiffer in T0 vs. T1. Correlation values before and\\nafter treatment are shown as light blue and dark\\nblue bars, respectively. Differences in correlation\\nare represented as black bars centered in\\nbetween.\\n4Article at DOI: 10.1371/journal.pone.0228037, figure at DOI: 10.1371/journal.pone.0228037Myra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 19/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 31, 'page_label': '20'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness towards a target variable (2)\\nQuantifying goodness as uncertainty U\\nGiven a set of k classes/labels C with prior class probability P(ci) for each i = 1, . . . ,k\\nand a feature space F:\\n◦ Information measures: What is the information gain achieved when we consider a\\nfeature A ∈ F (i) in comparison to P(ck) or (ii) in comparison to considering another\\nfeature B ∈ F ? (Section 7.2.2.1)\\n◦ Accuracy-like measures: How does accuracy change when (i) we consider A ∈ F\\nadditionally to previous selected features, (ii) we skip A ∈ F ? (Section 7.2.2.5)\\nExample: information gain as uncertainty reduction (Section 7.2.2.1)\\nThe ’Information Gain’ achieved by considering a featureA is the difference between\\nthe prior uncertainty ∑k\\ni=1 U(P(ci)) and the expected posterior uncertainty using A:\\nIG(A) =\\nk\\n∑\\ni=1\\nU(P(ci)) − E\\n \\nk\\n∑\\ni=1\\nU(P(ci|A))\\n!'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 31, 'page_label': '20'}, page_content='the prior uncertainty ∑k\\ni=1 U(P(ci)) and the expected posterior uncertainty using A:\\nIG(A) =\\nk\\n∑\\ni=1\\nU(P(ci)) − E\\n \\nk\\n∑\\ni=1\\nU(P(ci|A))\\n!\\nand as U() we use Shannon’s entropy (or a derived function), so that:\\n∑k\\ni=1 U(P(ci)) = − ∑k\\ni=1 P(ci) log2 P(ci)\\nand accordingly for the expected posterior uncertainty given A.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 20/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 32, 'page_label': '20'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nGoodness towards a target variable (2)\\nQuantifying goodness as uncertainty U\\nGiven a set of k classes/labels C with prior class probability P(ci) for each i = 1, . . . ,k\\nand a feature space F:\\n◦ Information measures: What is the information gain achieved when we consider a\\nfeature A ∈ F (i) in comparison to P(ck) or (ii) in comparison to considering another\\nfeature B ∈ F ? (Section 7.2.2.1)\\n◦ Accuracy-like measures: How does accuracy change when (i) we consider A ∈ F\\nadditionally to previous selected features, (ii) we skip A ∈ F ? (Section 7.2.2.5)\\nExample: information gain as uncertainty reduction (Section 7.2.2.1)\\nThe ’Information Gain’ achieved by considering a featureA is the difference between\\nthe prior uncertainty ∑k\\ni=1 U(P(ci)) and the expected posterior uncertainty using A:\\nIG(A) =\\nk\\n∑\\ni=1\\nU(P(ci)) − E\\n \\nk\\n∑\\ni=1\\nU(P(ci|A))\\n!'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 32, 'page_label': '20'}, page_content='the prior uncertainty ∑k\\ni=1 U(P(ci)) and the expected posterior uncertainty using A:\\nIG(A) =\\nk\\n∑\\ni=1\\nU(P(ci)) − E\\n \\nk\\n∑\\ni=1\\nU(P(ci|A))\\n!\\nand as U() we use Shannon’s entropy (or a derived function), so that:\\n∑k\\ni=1 U(P(ci)) = − ∑k\\ni=1 P(ci) log2 P(ci)\\nand accordingly for the expected posterior uncertainty given A.Myra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 20/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 33, 'page_label': '21'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen ways of quantifying goodness of features.\\nWe focused on correlations between two features, and we have seen some of\\nthe many many functions that can be used to compute correlations. We have\\nalso re-visited a visualization instrument for showing all pairs of correlation\\ncoefficient values in a heatmap matrix.\\nWe have also seen examples of functions that compute the goodness of a\\nfeature for class separation.\\nY our turn:Y ou must be able to compute correlation coefficients for example\\ncases, and to explain correlation results. Y ou must be also able to compute\\ninformation gain with Shannon’s entropy.\\nWhat comes next: Closing the workflow of feature selection\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 21/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 34, 'page_label': '21'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen ways of quantifying goodness of features.\\nWe focused on correlations between two features, and we have seen some of\\nthe many many functions that can be used to compute correlations. We have\\nalso re-visited a visualization instrument for showing all pairs of correlation\\ncoefficient values in a heatmap matrix.\\nWe have also seen examples of functions that compute the goodness of a\\nfeature for class separation.\\nY our turn:Y ou must be able to compute correlation coefficients for example\\ncases, and to explain correlation results. Y ou must be also able to compute\\ninformation gain with Shannon’s entropy.\\nWhat comes next: Closing the workflow of feature selection\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 21/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 35, 'page_label': '21'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThus far: We have seen ways of quantifying goodness of features.\\nWe focused on correlations between two features, and we have seen some of\\nthe many many functions that can be used to compute correlations. We have\\nalso re-visited a visualization instrument for showing all pairs of correlation\\ncoefficient values in a heatmap matrix.\\nWe have also seen examples of functions that compute the goodness of a\\nfeature for class separation.\\nY our turn:Y ou must be able to compute correlation coefficients for example\\ncases, and to explain correlation results. Y ou must be also able to compute\\ninformation gain with Shannon’s entropy.\\nWhat comes next: Closing the workflow of feature selection\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 21/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 36, 'page_label': '22'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 22/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 37, 'page_label': '23'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nWhat is missing from the feature selection workflow?\\nWe already have√ functions that help us decide whether a feature is good to keep or rather to discard√ procedures to traverse the search space, i.e. all combinations of features, and\\ncreate feature subsets√ stopping criteria\\nbut how good is our constructed subset of features ? for a learning algorithm ?\\n↙\\nTraining and testing\\nGiven a sample of the data D that is representative\\nof the population under study:\\n· We split D into a training set Dtrain and a test set\\nDtest so that D = Dtrain ∪ Dtest and\\nDtrain ∩ Dtest = / 0.\\n· We use Dtrain to train the learning algorithm and\\ninduce a model M\\n· We use Dtest to test the behaviour of M on\\npreviously unseen data\\n→\\n? How to test the impact of\\nthe feature selection\\nprocess on the model M\\n? And WHEN?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 23/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 38, 'page_label': '23'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nWhat is missing from the feature selection workflow?\\nWe already have√ functions that help us decide whether a feature is good to keep or rather to discard√ procedures to traverse the search space, i.e. all combinations of features, and\\ncreate feature subsets√ stopping criteria\\nbut how good is our constructed subset of features ? for a learning algorithm ?\\n↙\\nTraining and testing\\nGiven a sample of the data D that is representative\\nof the population under study:\\n· We split D into a training set Dtrain and a test set\\nDtest so that D = Dtrain ∪ Dtest and\\nDtrain ∩ Dtest = / 0.\\n· We use Dtrain to train the learning algorithm and\\ninduce a model M\\n· We use Dtest to test the behaviour of M on\\npreviously unseen data\\n→\\n? How to test the impact of\\nthe feature selection\\nprocess on the model M\\n? And WHEN?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 23/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 39, 'page_label': '23'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nWhat is missing from the feature selection workflow?\\nWe already have√ functions that help us decide whether a feature is good to keep or rather to discard√ procedures to traverse the search space, i.e. all combinations of features, and\\ncreate feature subsets√ stopping criteria\\nbut how good is our constructed subset of features ? for a learning algorithm ?\\n↙\\nTraining and testing\\nGiven a sample of the data D that is representative\\nof the population under study:\\n· We split D into a training set Dtrain and a test set\\nDtest so that D = Dtrain ∪ Dtest and\\nDtrain ∩ Dtest = / 0.\\n· We use Dtrain to train the learning algorithm and\\ninduce a model M\\n· We use Dtest to test the behaviour of M on\\npreviously unseen data\\n→\\n? How to test the impact of\\nthe feature selection\\nprocess on the model M\\n? And WHEN?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 23/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 40, 'page_label': '23'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nWhat is missing from the feature selection workflow?\\nWe already have√ functions that help us decide whether a feature is good to keep or rather to discard√ procedures to traverse the search space, i.e. all combinations of features, and\\ncreate feature subsets√ stopping criteria\\nbut how good is our constructed subset of features ? for a learning algorithm ?\\n↙\\nTraining and testing\\nGiven a sample of the data D that is representative\\nof the population under study:\\n· We split D into a training set Dtrain and a test set\\nDtest so that D = Dtrain ∪ Dtest and\\nDtrain ∩ Dtest = / 0.\\n· We use Dtrain to train the learning algorithm and\\ninduce a model M\\n· We use Dtest to test the behaviour of M on\\npreviously unseen data\\n→\\n? How to test the impact of\\nthe feature selection\\nprocess on the model M\\n? And WHEN?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 23/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 41, 'page_label': '24'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFilters (Section 7.2.3.1) & Wrappers (Section 7.2.3.2)\\nFilters\\nfilter the good from the no-good features before learning.\\nStage 1: Feature subset selection with a goodness criterion and a feature set\\ngeneration algorithm – it delivers its best subset of features\\nStage 2: The learning algorithm uses the data of this subset of features only, first for\\ntraining, then for testing\\nSubcategory Rankers: Stage 1 delivers all features, each one ranked on goodness;\\nat stage 2, the algorithm applies a threshold to pick the top-ranked features\\nWrappers\\nengage a classifier to decide whether a feature should be kept or discarded,\\ndepending on its impact on classification quality.\\nStage 1: Feature subset selection, where the goodness function is in the blackbox of\\nthe classification algorithm – it delivers the best subset of features, from the\\nviewpoint of the classifier\\nStage 2: as for Filters, but'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 41, 'page_label': '24'}, page_content='the classification algorithm – it delivers the best subset of features, from the\\nviewpoint of the classifier\\nStage 2: as for Filters, but\\n! a model must be induced on the best subset of features\\n! and tested on data that have not been seen before\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 24/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 42, 'page_label': '24'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nFilters (Section 7.2.3.1) & Wrappers (Section 7.2.3.2)\\nFilters\\nfilter the good from the no-good features before learning.\\nStage 1: Feature subset selection with a goodness criterion and a feature set\\ngeneration algorithm – it delivers its best subset of features\\nStage 2: The learning algorithm uses the data of this subset of features only, first for\\ntraining, then for testing\\nSubcategory Rankers: Stage 1 delivers all features, each one ranked on goodness;\\nat stage 2, the algorithm applies a threshold to pick the top-ranked features\\nWrappers\\nengage a classifier to decide whether a feature should be kept or discarded,\\ndepending on its impact on classification quality.\\nStage 1: Feature subset selection, where the goodness function is in the blackbox of\\nthe classification algorithm – it delivers the best subset of features, from the\\nviewpoint of the classifier\\nStage 2: as for Filters, but'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 42, 'page_label': '24'}, page_content='the classification algorithm – it delivers the best subset of features, from the\\nviewpoint of the classifier\\nStage 2: as for Filters, but\\n! a model must be induced on the best subset of features\\n! and tested on data that have not been seen before\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 24/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 43, 'page_label': '25'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\n1 Running example\\n2 Feature Selection Process\\n3 Goodness Criteria\\n4 Filters & Wrappers\\n5 Closing\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 25/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 44, 'page_label': '26'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nSummary and Outlook\\nWe have seen\\na workflow for the feature selection process:√ Ways of building up a feature subset by adding or discarding features√ Functions that quantify the goodness of features√ Two types of link to the learning algorithm: filter the feature space before\\nlearning, or wrap the learning algorithm into the feature selection workflow\\nInstruments that we skipped:\\n· Methods for feature construction in a projected space, e.g. ’Principal\\nComponent Analysis’\\n· Methods quantifying the discriminative power of a feature towards a target,\\ne.g. SHAP\\nWhat comes next: Shifting from static data to time series\\n4 What is a time series\\n4 What to learn on one time series, and what to learn on many\\n4 Imputing missing values − →filling gaps\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 26/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 45, 'page_label': '27'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nThank you very much!\\nQuestions?\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 27/28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.22', 'creator': 'LaTeX with Beamer class', 'creationdate': '2024-01-08T04:05:01+01:00', 'author': 'Myra Spiliopoulou', 'keywords': '', 'moddate': '2025-06-27T10:21:46+02:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev', 'subject': '', 'title': 'BLOCK Data Engineering - Unit 3 on Feature Selection', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\DataEng_3_FeatureSelection_HANDOUT.pdf', 'total_pages': 47, 'page': 46, 'page_label': '28'}, page_content='Running example Feature Selection Process Goodness Criteria Filters & Wrappers Closing\\nBibliography I\\nFieller, E. C., Hartley, H. O., and Pearson, E. S. (1957). Tests for rank\\ncorrelation coefficients. i. Biometrika, 44(3/4):470–481.\\nNiemann, U., Boecking, B., Brueggemann, P ., Mebus, W., Mazurek, B.,\\nand Spiliopoulou, M. (2020). Tinnitus-related distress after multimodal\\ntreatment can be characterized using a key subset of baseline variables.\\nPLOS ONE, 15(1):1–18.\\nSchleicher, M., Br¨uggemann, P ., B¨ocking, B., Niemann, U., Mazurek, B.,\\nand Spiliopoulou, M. (2024). Parsimonious predictors for medical\\ndecision support: Minimizing the set of questionnaires used for tinnitus\\noutcome prediction. Expert Systems with Applications , 239:122336.\\nMyra Spiliopoulou · · · BLOCK Data Engineering - Unit 3 on Feature Selection · · · 28/28')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bec9a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b1b0063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for text of length 239...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7e2cfdc01343c993652df765517983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding of shape (239, 384).\n",
      "Added 239 documents to vector store.\n",
      "New number of vectors in collection: 239\n"
     ]
    }
   ],
   "source": [
    "text = [doc.page_content for doc in chunks]\n",
    "\n",
    "\n",
    "embedding=embedding_manager.generate_embedding(text)\n",
    "embedding.shape\n",
    "\n",
    "vector_store.add_documents(chunks,embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df7866",
   "metadata": {},
   "source": [
    "## Retrieval from Vectore store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307068d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self,vector_store:VectorStore,embedding_manager:EmbeddingManager):\n",
    "        self.vector_store=vector_store\n",
    "        self.embedding_manager=embedding_manager\n",
    "        \n",
    "    def retrieve(self,query:str,top_k:int=5,score_threshold:float=0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from the vector store based on the query.\n",
    "        \n",
    "        Parameters:\n",
    "        - query: The input query string.\n",
    "        - top_k: Number of top similar documents to retrieve.\n",
    "        - score_threshold: Minimum similarity score to consider a document relevant.\n",
    "        \n",
    "        Returns:\n",
    "        - List of dictionaries containing 'document' and 'score'.\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score Threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate embedding for the query from user\n",
    "        query_embedding = self.embedding_manager.generate_embedding([query])[0]\n",
    "        \n",
    "        # Perform similarity search in the vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "        \n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents=results['documents'][0]\n",
    "                metadatas=results['metadatas'][0]\n",
    "                distances=results['distances'][0]\n",
    "                ids=results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id,document, metadata, distance) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "                    similarity_score=1 - distance\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            \"id\": doc_id,\n",
    "                            \"content\": document,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"similarity_score\": similarity_score,\n",
    "                            \"distance\": distance,\n",
    "                            \"rank\": i + 1\n",
    "                        })\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents above the score threshold.\")\n",
    "            else:\n",
    "                print(\"No documents retrieved from vector store.\")\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            raise e\n",
    "rag_retriever=RAGRetriever(vector_store,embedding_manager)   \n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "045a5b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1cb1209bf10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf2a6505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Naive Bayes?'\n",
      "Top K: 5, Score Threshold: 0.0\n",
      "Generating embedding for text of length 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eff1a2299a84ad9b5abb5e4a3643187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding of shape (1, 384).\n",
      "Retrieved 5 documents above the score threshold.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'f3829e6d-85de-4004-9f39-3cf5808160ba',\n",
       "  'content': 'DM I: Block ’Classification’\\nUnit ’Naive Bayes’\\nMyra Spiliopoulou',\n",
       "  'metadata': {'creator': 'LaTeX with Beamer class',\n",
       "   'keywords': '',\n",
       "   'subject': '',\n",
       "   'page_label': '1',\n",
       "   'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\",\n",
       "   'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf',\n",
       "   'author': 'Myra Spiliopoulou',\n",
       "   'page': 0,\n",
       "   'moddate': '2023-10-07T20:24:48+02:00',\n",
       "   'trapped': '/False',\n",
       "   'producer': 'pdfTeX-1.40.22',\n",
       "   'total_pages': 23,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev',\n",
       "   'creationdate': '2023-10-07T20:24:48+02:00'},\n",
       "  'similarity_score': 0.3949319124221802,\n",
       "  'distance': 0.6050680875778198,\n",
       "  'rank': 1},\n",
       " {'id': '096c27d3-fe93-4e28-8219-7bf7df125b42',\n",
       "  'content': 'The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes from Witten & Eibe\\nLaw of Bayes\\nThe probability of an event H given evidence E is: p(H|E) = p(E|H)p(H)\\np(E)\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nAppeared in:\\n“Essay towards solving a problem in the doctrine of chances” (1763)\\nby Thomas Bayes (born: 1702, London; died: 1761, Tunbridge Wells, Kent)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 5/17',\n",
       "  'metadata': {'creator': 'LaTeX with Beamer class',\n",
       "   'moddate': '2023-10-07T20:24:48+02:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf',\n",
       "   'page': 4,\n",
       "   'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\",\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev',\n",
       "   'total_pages': 23,\n",
       "   'creationdate': '2023-10-07T20:24:48+02:00',\n",
       "   'author': 'Myra Spiliopoulou',\n",
       "   'trapped': '/False',\n",
       "   'producer': 'pdfTeX-1.40.22',\n",
       "   'page_label': '5',\n",
       "   'subject': '',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.1616324782371521,\n",
       "  'distance': 0.8383675217628479,\n",
       "  'rank': 2},\n",
       " {'id': '584fef9b-c816-4dac-81b8-722509ad2231',\n",
       "  'content': 'The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes from Witten & Eibe\\nLaw of Bayes\\nThe probability of an event H given evidence E is: p(H|E) = p(E|H)p(H)\\np(E)\\nLaw of Bayes, using the Independence Assumption\\nThe probability of an event H given evidence E is:\\np(H|E) = p(E|H)p(H)\\np(E) = ∏n\\ni=1 p(Ei|H)p(H)\\np(E)\\ni.e. it is assumed that p(E|H) = ∏n\\ni=1 p(Ei|H) (naive).\\nAppeared in:\\n“Essay towards solving a problem in the doctrine of chances” (1763)\\nby Thomas Bayes (born: 1702, London; died: 1761, Tunbridge Wells, Kent)\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 5/17',\n",
       "  'metadata': {'page': 5,\n",
       "   'subject': '',\n",
       "   'page_label': '5',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev',\n",
       "   'producer': 'pdfTeX-1.40.22',\n",
       "   'moddate': '2023-10-07T20:24:48+02:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf',\n",
       "   'trapped': '/False',\n",
       "   'creationdate': '2023-10-07T20:24:48+02:00',\n",
       "   'author': 'Myra Spiliopoulou',\n",
       "   'keywords': '',\n",
       "   'total_pages': 23,\n",
       "   'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\",\n",
       "   'creator': 'LaTeX with Beamer class'},\n",
       "  'similarity_score': 0.1616324782371521,\n",
       "  'distance': 0.8383675217628479,\n",
       "  'rank': 3},\n",
       " {'id': '4e02f467-dfa3-409d-8e55-64a88674517f',\n",
       "  'content': 'The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes\\nLearning phase of Naive Bayes\\nFor each (attribute, value)-pair (a, z) for an attribute a ∈ A:\\n▶ For each label y ∈ L: compute p((a, z)|y)\\nApplication phase with Naive Bayes\\nFor an instance x with unknown label:\\n▶ For each label y ∈ L: compute p(y|x) using the computations of the\\nlearning phase.\\n▶ Assign to x the label with the highest probability, i.e.\\nlabel(x) = arg maxy∈L p(y|x).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 7/17',\n",
       "  'metadata': {'keywords': '',\n",
       "   'author': 'Myra Spiliopoulou',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev',\n",
       "   'page': 9,\n",
       "   'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf',\n",
       "   'page_label': '7',\n",
       "   'total_pages': 23,\n",
       "   'creator': 'LaTeX with Beamer class',\n",
       "   'creationdate': '2023-10-07T20:24:48+02:00',\n",
       "   'subject': '',\n",
       "   'moddate': '2023-10-07T20:24:48+02:00',\n",
       "   'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\",\n",
       "   'producer': 'pdfTeX-1.40.22',\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.15910279750823975,\n",
       "  'distance': 0.8408972024917603,\n",
       "  'rank': 4},\n",
       " {'id': 'f0e45940-c026-40f3-98f2-a0c26c307b56',\n",
       "  'content': 'The Law of Bayes for Model Learning The Zero-Frequency Problem NB on numerical attributes Closing\\nClassification with Naive Bayes\\nLearning phase of Naive Bayes\\nFor each (attribute, value)-pair (a, z) for an attribute a ∈ A:\\n▶ For each label y ∈ L: compute p((a, z)|y)\\nApplication phase with Naive Bayes\\nFor an instance x with unknown label:\\n▶ For each label y ∈ L: compute p(y|x) using the computations of the\\nlearning phase.\\n▶ Assign to x the label with the highest probability, i.e.\\nlabel(x) = arg maxy∈L p(y|x).\\nMyra Spiliopoulou · · · DM I: Block ’Classification’ Unit ’Naive Bayes’ · · · 7/17',\n",
       "  'metadata': {'total_pages': 23,\n",
       "   'title': \"DM I: Block 'Classification' - Unit 'Naive Bayes'\",\n",
       "   'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'moddate': '2023-10-07T20:24:48+02:00',\n",
       "   'creationdate': '2023-10-07T20:24:48+02:00',\n",
       "   'author': 'Myra Spiliopoulou',\n",
       "   'keywords': '',\n",
       "   'creator': 'LaTeX with Beamer class',\n",
       "   'page_label': '7',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2022/dev/Debian) kpathsea version 6.3.4/dev',\n",
       "   'producer': 'pdfTeX-1.40.22',\n",
       "   'page': 10,\n",
       "   'source': '..\\\\data\\\\pdf\\\\1_Classification_Unit_NB.pdf'},\n",
       "  'similarity_score': 0.15910279750823975,\n",
       "  'distance': 0.8408972024917603,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is Naive Bayes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba2c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe6c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
